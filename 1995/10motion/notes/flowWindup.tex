%
%	There is too much windup here.  Start with a phenomenon,
%	please, or the stimulus representation.
%
%
\section{Optical Flow Estimation}
\paragraph{Overview of Approaches}
There are two historical approaches
optic flow field estimation.
One can has its roots in a series of classic papers
by W. Reichardt and the contemporaneous papers by
Barlow and Levick.
These scientists
analyzed biological motion estimation
in the visual system of various animals.
These investigations led to a variety of
computations based on the design and analysis
of the output of linear space-time filters.
The algorithms are based on linear filters
because so much of the early encoding in the visual
pathways (see Chapter \ref{chapter:retina}) is amenable
to linear analysis.
We will study modern treatments
of this method based on the work
of Adelson and Bergen (see also 
Reichardt, Watson and Ahumada, van Santen and
Sperling)

A related approach, called {\em motion gradient estimation},
is based on approximations to 
the expected change in image intensities when
the camera moves.
This approach begins from a very different
premise, but
as Simoncelli and Heeger point out the calculations
embodied in linear filtering and motion gradient
methods are very similar.

There is another apparent distinction in motion
estimation algorithms.
Some authors estimate local motions by
first classifying local image intensity patterns
into identifiable features and then tracking the
displacement of these features.
These algorithms include a classification step
after the linear filtering.
Identifying corresponding object features
in a pair of images is an unsolved problem;
but image-tracking algorithms for optic flow are practical
because when we sample the image closely in time.
Then, the displacements of
image features are small and the algorithms are manageable
and often robust.
For example, Tomassi and Kanade () derive optic flow using this method
and they obtain good estimates of
measure camera motion and object depth.

When the feature classification rules
are strict, the method is quite non-linear and
seems different from basic linear filtering.
But, the differences are not as great as they
may first appear.
First, it is possible to design
linear filters whose output measures
one's confidence in the local retinal image measurement
as well as the direction.
This sort of continuous classification brings the methods
together.
Second, the feature tracking methods themselves involve
various types of linear filtering prior to the strict
classification judgment.
At this level, they share many computational properties.

One final distinction between motion estimation algorithms
concerns the type of optical flow field one measures.
Often, the many local motion estimates in the retinal image
are noisy, providing very little useful information.
In an important early set of papers,
Ullman (197X; xxx) studied algorithms based on
just a few motion estimates,
whose values are quite salient and secure.
These investigations led to a class of algorithms to discover
how many motion vectors does one need, across how many
images, to decide whether the motions
can be interpreted as a single, rigid, three-dimensional object.
Since these methods are based on correspondences between
points in different images, they too are based on optic
flow measurements.
They differ mainly in that they require
space-time sampling of the optic flow fields that
are sparsely sampled.
Presumably, these points are obtained from fundamental
optic flow calculations by singling out the strongest
and most secure correspondences in the optic flow field.

