%
%	From the chapter on Color Appearance
%
\newcommand{\recSens}[1]{\mbox{$R_{#1}$}}
\newcommand{\recResp}{\mbox{$\bf r$}}
\newcommand{\recRespi}[1]{\mbox{$r_{#1}$}}
\newcommand{\sensorMat}{\mbox{$\bf S$}}

\newcommand{\colsig}[1]{\mbox{$c(#1)$}}
\newcommand{\colsigi}[1]{\mbox{$c_{#1}$}}

\newcommand{\ill}[1]{\mbox{$e(#1)$}}
\newcommand{\illhat}[1]{\mbox{$\hat{e}(#1)$}}
\newcommand{\illi}[2]{\mbox{$e_{#1}(#2)$}}
\newcommand{\illvec}{\mbox{$\bf e$}}

\newcommand{\illveci}[1]{\mbox{$\bf e_{#1}$}}
\newcommand{\illBasis}{\mbox{$\bf B_e$}}
\newcommand{\illBasisi}[2]{\mbox{$E_{#1}(#2)$}}
\newcommand{\illCoef}{\mbox{${\bf \omega}$}}
\newcommand{\illCoefi}[1]{\mbox{$\omega_{#1}$}}
\newcommand{\illMat}[1]{\mbox{$\Lambda_{#1}$}}
\newcommand{\illMatinv}[1]{\mbox{$\Lambda^{-1}_{#1}$}}

\newcommand{\surf}[1]{\mbox{$s(#1)$}}
\newcommand{\surfvec}{\mbox{$\bf s$}}
\newcommand{\surfveci}[1]{\mbox{$s_{#1}$}}
\newcommand{\surCoef}{\mbox{${\bf \sigma}$}}
\newcommand{\surCoefi}[1]{\mbox{$\sigma_{#1}$}}
\newcommand{\surBasis}{\mbox{$\bf B_s$}}
\newcommand{\surBasisi}[2]{\mbox{$S_{#1}(#2)$}}

\chapter{Topics in Color}
\label{chapter:Color}

\section{Theory and Computation}
In a paper published in the 1959 Proceedings
of the National Academy of Sciences (USA)
Edwin Land dismissed the importance of color-matching.
Land, the inventor of instant developing film and
founder of the Polaroid Corporation, wrote
\begin{quote}
We have come to the conclusion that the classical
laws of color mixing conceal great basic laws of color vision
[Proc. Nat. Acad. Sci, 1959].
\end{quote}

These sharp words, an arrow aimed at the heart of color science,
provoked a rejoinder from two leading scientists,
D. B. Judd and G. Walls.
Their critical appraisals of Land's contribution,
published over the next few years,
created a rift between the great inventor and the
scientific community that never fully healed~\footnote{
One has only to read Semir Zeki's fascinating book to see that the passion
has been passed on to a new generation.}.

What was it that Land, a brilliant man, found so
objectionable about color-matching?
It seems to me that
Land's textbook studies led him to believe that
the curves we obtain from
the color-matching experiment should
also serve as a measurement of color appearance.
When he set his textbooks down
and began to perform experiments with color,
he was sorely disappointed.
He found that the color-matching measurements are not 
a satisfactory answer to many important questions about
color appearance,
since color appearance
depends on the responses spread across
the cone mosaics, not the cone absorptions at a point.
While not novel, this point is absolutely correct and 
his criticism continues to serve
as a powerful reminder that there is more to color than matching.
As to his claim in those papers that there are two, not three,
types of photoreceptors, well, we all have off days.

To a certain extent, color {\em is} related
to the {\em relative} absorption rates of the cones.
Within an image, bright objects usually create more
cone absorptions than dark objects;
red objects create more $\Red$ cone absorptions
and blue objects more $\Blue$ cone absorptions.
But Land's demonstrations, and many others,
prove that color appearance
cannot be equated with the absolute level of cone absorptions.

I think you can be persuaded
of this by considering a simple thought experiment.
Suppose you read a newspaper indoors.
The white part of the newspaper reflects about 90 percent
of the light towards your eye, while the black ink reflects
only about 2 percent.
Hence, if the ambient illumination inside a reading
room is 100 units,
the white paper reflects 90 units and the black ink 2 units.

When you take the newspaper outside, the illumination level can
be 100 times greater, or 10,000 units.
Outside the black ink reflects 200 units towards your eye, which
far exceeds the level of the white paper when you were indoors.
Yet, the ink continues to look black.
The relationship between appearance, whether it is
black/white or different shades of color is not predicted
simply by calculating the cone absorption rates~\footnote{
This example is taken from Hering (1964).}.

This thought experiment reminds us that,
mainly, color is a property of objects.
Objects retain their
color whether seen in shade or sun;
when the color of an object changes, 
we think something about the object has changed.
From our thought experiment, it follows that the
color of an object imaged at a point on the retina
is inferred from the overall pattern of cone absorptions.
The absolute level of
absorptions at a point will not serve as an index
to color appearance.

\subsection*{Spectral Image Formation}

In the introduction to this part of the book,
I describe how Helmholtz encouraged us to think of our percepts
as explanations of the pattern of cone excitations.
He encouraged us to frame these explanations in terms of
the physical properties of the objects.
On this view, the color we perceive is a mental
explanation of why an object causes relatively more
absorptions in one cone type than another.
Recently this idea about color has been labeled
{\em computational color vision}.

Amongst the physical properties of an object,
it is the object's {\em surface reflectance} function
that distinguishes how strongly
the object will influence each of the cone absorption rates.
The surface reflectance functions describes
the tendency of the object to reflect light at different wavelengths.
Objects that reflect light mainly in the long-wavelength 
part of the spectrum usually
appear red; objects that reflect mainly
short-wavelength light usually appear blue.

A problem that the central nervous system must confront, then,
is how to interpret the cone absorption rates in terms
of the surface reflectance functions.
We will begin this chapter by asking a general
computational question:
what can the central nervous system possibly do to 
infer the surface reflectance function from the
mosaic of cone absorptions?
In the next section, we will use this framework to think
about psychological and biological experiments
concerning color appearance.

The light incident at our corneas depends in part on the
properties of the objects that reflect the light and
in part on the wavelength composition of the ambient illumination.
We must understand each of these components, and how they
fit together, to see what information we might extract
from the retinal image about surface and illuminant functions.

Figure \ref{f8:surfR} shows the reflectance function
of a few colored papers.
These curves describe the fraction of light that is reflected
from the object as a function of wavelength.
Hence, the curve ranges between zero and one~\footnote{
Often, there is
more than one surface reflectance curve associated with an object.
For example,
shiny objects reflect different amounts of light
in different directions depending on the viewing geometry.
Some types of materials fluoresce, which is to say they absorb
light at one wavelength and emit light at another (longer) wavelength.
I will say a bit more about this topic later in the chapter.}.
\begin{figure}
\centerline{
 \psfig{figure=../08col/fig/surfR.ps ,clip= ,height=3.5in}
}
\caption[Surface Reflectance Functions]{
{\em The surface reflectance function} is the proportion of light
scattered from a surface at each wavelength.
The panels show the surface reflectance functions of 
various colored papers along with the color name associated
with the paper.
Surface reflectance correlates with the color appearance;
as Newton wrote
``colors in the object are nothing but a disposition
to reflect this or that sort of ray more copiously than the rest.''
}
\label{f8:surfR}
\end{figure}

We do not sense the surface reflectance directly.
Rather, the light that reaches our eye originates at an illumination
source and is scattered towards our eye the surface.
The scattering of the light from surface favors certain
wavelengths over others.
The imaging process is described mathematically
as the product of two spectral functions.
First, we express the illuminant in terms of the relative
energy at different wavelengths, as in
the top left panel of Figure~\ref{f8:colorSig}.
which shows the spectral power distribution
of an illuminant that is like the disk of the sun.
The bottom panel on the left shows the spectral
power distribution from the blue sky.
\begin{figure}
\centerline{
 \psfig{figure=../08col/fig/colorSig.ps,clip= ,height=3.5in}
}
\caption[The Color Signal Under Two Illuminants]{
{\em The light reflected from objects} changes
when the illuminant changes.
The separate panels show the spectral power distributions
of light reflected from the red, green and yellow
papers in Figure~\ref{f8:surfR}
when the illuminant is the disk of the sun (a)
or blue skylight (b).
The insets in the graphs of reflected light
show the relative cone absorption rates caused by the reflected light.
}
\label{f8:colorSig}
\end{figure}

Compare the top and bottom rows of Figure~\ref{f8:colorSig}.
In the top row I have plotted
the light reflected from each of the papers
when they are illuminated by the disk of the sun.
In the second row I have calculated the reflected light
from the same three surfaces when they are illuminated by
blue skylight.
The spectral composition of the
light that arrives at our eye, the {\em color signal},
is very different under these two illuminants.

From our knowledge of the spectral absorption of the photopigments
(see Chapter~\ref{chapter:wavelength}) we can calculate the photopigment
absorption rates from each of these color signals.
The estimated absorption rates
of the three photopigments are plotted as insets within the figures.
When the illuminant changes,
the cone absorption rates from each surface changes dramatically.

This simple calculation defines
a central problem in understanding color appearance.
Color is a property of objects,
but the light scattered, and thus the cone absorptions,
depends on the illumination.
If color must describe a property of
an object, the nervous system
must {\em interpret} the mosaic
of photopigment absorptions and estimate
something about the surface reflectance function.
This is an estimation problem.

\subsection*{Color Appearance:  An Estimation Problem}
We think of the color percept as an estimate of the
surface reflectance function of the object.
How might we use the three photopigment
absorption rates to infer
the a surface reflectance function?

It is useful to begin this analysis with
a good dose of humility.
With only three types of cones, we obtain a
very modest measurement of the color signal.
The color signal itself depends on two unknown spectral functions.
Using a small number of cones,
it is impossible to recover
surface reflectances without error.

It is easy way to see why this is so.
The color signal is equal to the product of the illuminant
spectral power distribution $\ill{\lambda}$
and the surface reflectance function, $\surf{\lambda}$,
\[
\colsig{\lambda} = \surf{\lambda} \ill{\lambda} .
\]
Suppose we replace the illuminant with a new illuminant,
$f(\lambda) \ill{\lambda}$ and all of the surfaces with new
functions $\surf{\lambda} / f(\lambda)$.
This change has no effect on the color signal,
\[
\colsig{\lambda} = \surf{\lambda} / f(\lambda) f(\lambda) \ill{\lambda}
= \surf{\lambda} \ill{\lambda}
\]
and thus no effect on the photopigment absorption rates.
Hence, the visual system can't discriminate between these
conditions.

In the introduction to this part of the book,
I quoted Helmholtz' suggestion:
the visual system imagines those objects being present that
could give rise to the retinal image.
We now find that the difficulty we have
in following this advice is not
that we can't find a solution,
but  rather that there are too many solutions;
many different objects could all have given rise
to the retinal image.

Which solution should we select?
The general strategy we should adopt is straightforward:
Pick the most likely one.

Perhaps the most important point that we have learned from
color constancy calculations over the last ten years
is this:  the set of surface and illuminant functions
we encounter is not as diverse as we might have feared.
Since some surface reflectance functions and illuminants are much
more likely than others, even with only three types of cones,
it is possible to make educated guesses
that do more good than harm.

The surface reflectance estimation calculations
we will review are all based on this principle.
They differ only in the set of assumptions they make concerning
what the observer knows and what we mean by most likely.
I review them now because
some of the tools are useful and interesting, and some
of the summaries of the data are very helpful in practical
calculations and experiments.

\subsection*{Linear Models}
To estimate which lights and surfaces are more probable,
we need to do two things.
First, we need to measure the spectral data from lights and surfaces.
Second, we need a way to represent the likelihood
of observing different surface and illuminant functions.

Over the last ten years or so,
{\em linear models} of surface and illuminant
functions have been used widely to represent
our best guess about the most likely surface and illuminant functions.
A linear model of a set of spectral
functions, such as surface reflectances,
is a method of efficiently approximating the measurements.
There are several ways to
build linear models, including {\em principal components analysis},
{\em centroid analysis}, or {\em one mode analysis}.
These methods have much in common, but
they differ slightly in their
linear model formulation and error measures.

Let's examine the construction of linear models by
reviewing a classic paper by
Judd, Macadam and Wyszecki (1964).
These authors built a linear model of an important
source of illumination, daylight spectral power distributions.

They began their study by collecting sample measurements of
the spectral power distribution of daylights at
different times of day and under different weather conditions
and on different continents.
To measure the spectral power distribution of daylight
we place an object with a known reflectance outside.
It is common to use blocks of pressed magnesium oxide
as a standard object because such blocks reflect light
of all wavelengths nearly equally.
Moreover, the material is essentially a pure
diffuser: a quantum of light incident on the surface
from any angle is reflected back with equal
probability in all other directions above the surface.

Judd et al. collected more than six hundred examples
of daylight spectral power distributions.
Since they were interested in the relative spectral
composition, not the absolute level, they normalized
all of their data to equal 100 at 560nm.
Their main interest was in the wavelength regime
visible to the human eye,
so they made measurements spaced every 
roughly every 10 nm from 400nm to 700nm.
Hence, they could represent each daylight measurement
by a set of thirty-one numbers.
Some examples are plotted in Figure \ref{f8:daylights}.
\begin{figure}
\centerline{
 \psfig{figure=../08col/fig/daylights.ps,clip= ,height=3.5in}
}
\caption[Examples of Daylights]{
{\em The relative spectral power distribution of typical daylights}
vary with the weather conditions.
Hence, the spectral power distribution of
light reflected from objects in our environment varies as well.
The curves drawn here are typical daylights measured
by Judd, Macadam and Wyszecki (1964).
The curves are normalized to coincide at 560nm.
\comment{
Basically a Judd et al. figure.  Get permission.
}
}
\label{f8:daylights}
\end{figure}

The daylight relative spectral power distributions
differ depending on the time
of day and the weather conditions.
But, the functions also share some common structure.
Judd et al. captured the regularities in the data
by building a linear model of the observed spectral
power distributions.
They designed their linear model, a {\em principal components model},
using the following logic.

First, they decided that they wanted
to approximate their observations in the squared-error sense.
Suppose $\ill{\lambda}$ is a measurement, and 
$\illhat{\lambda}$ is the linear model
estimate.
They decided to select the approximation in order
to minimize the {\em squared error}
\[
\sum_{\lambda} ( \ill{\lambda} - \illhat{\lambda} )^2 .
\]

When we consider the collection of observations as a whole,
the function with the smallest squared error
and the data is the mean.
The principal components method treats the
mean curve as a special case.
The mean observation from Judd et al.'s data set, 
$ \illi{0}{\lambda} $,
is the bold curve in Figure \ref{f8:linearModDay}.

Now, we need only to approximate the differences from the mean
for each measurement.
We build the linear model to explain
these differences as follows.
First, we select a set of {\em basis functions},
which are curves just like the measurements.
We approximate the difference from
the mean as the weighted sum of the basis functions.

For example, suppose $\Delta \illi{j}{\lambda}$ is the difference
between the $j^{th}$ measurement and the mean.
We select a set of, say $N$, basis functions, $\illBasisi{i}{\lambda}$,
and we approximate the differences from the mean as
\begin{equation}
\label{e8:linMod}
\Delta \illi{j}{\lambda}
  \approx \sum_{i=1}^{i=N} \illCoefi{i} \illBasisi{i}{\lambda} .
\end{equation}
The values $\illCoefi{i}$ are called 
the linear model {\em weights}, or {\em coefficients}.
They are chosen to make the squared error between an 
{\em individual} measurement and its approximation as small as possible.
The basis functions
are chosen to make the sum of the squared errors between
the {\em collection} of measurements
and their approximations as small as possible.
The number of basis functions, $N$, is called the {\em dimensionality}
of the linear model.
As the dimension increases,
the precision of the linear model approximation improves.
The dimensionality one chooses for an application
depends on the required precision~\footnote{
The basis functions that minimize the squared error can be
found in several ways.
If the data are in the columns of a matrix,
one can apply the singular value decomposition 
to the data matrix and use the left singular vectors.
Equivalently, one
can find the eigenvectors of the covariance matrix of the data.
These calculations are included in most statistical packages.
}.

Judd et al. found that an excellent
approximation to the data using the mean
and two basis functions.
The linear model representation
expresses the data efficiently.
We must specify the mean and the two basis functions.
Then, we use only two numbers to represent each of
the individual measurements.
These empirical results were confirmed by
other investigators (Dixon, Sastri),
and the results have been adopted by the international
color standards organization to create a model of daylights.
The mean and two basis terms from the international standard
are plotted in Figure \ref{f8:linearModDay}.
\begin{figure}
\centerline{
 \psfig{figure=../08col/fig/linearModDay.ps,clip= ,width=4.0in,height=3.5in}
}
\caption[Linear Model Curves for Daylight]{
{\em A linear model for daylight spectral power distributions.}
The three curves are the mean spectral power distribution
of daylight and two basis curves.
By adding together weighted sums of the mean
and two basis functions,
one can generate examples of typical
relative spectral power distributions of daylight.
The original data set was normalized to coincide
at 560nm, so the two basis curves are zero at that wavelength.
(After Judd et al., 1964).
}
\label{f8:linearModDay}
\end{figure}
\nocite{JuddMacWys,Sastri,Dixon}

The linear model described by Judd et al. 
was derived by normalizing all of the measurements
to a common value at 560nm.
As a result, the two basis functions are both
zero at 560nm and therefore the sum of the mean
and the two basis functions is always equal to
the constant value at 560nm.

Daylights vary in their intensity as well.
One may wish to extend Judd et al.'s linear model
by using a three-dimensional linear model,
where the three basis functions are the mean
and the two functions derived by Judd et al.
Although it is possible to improve on this model
slightly, as a practical matter it does quite nicely
in approximating daylights.
In this case the linear model approximation is
\begin{equation}
\label{e8:dayMod}
\ill{\lambda} = \sum_{i=1}^{3} \illCoefi{i} \illBasisi{i}{\lambda} .
\end{equation}
We can express the linear model in Equation \ref{e8:dayMod}
as a matrix equation,
$\illvec = \illBasis \illCoef$
in which $\illvec$ is a vector
representing the illumination,
$\illBasis$ is a matrix whose three columns are the
illuminant linear model basis functions, $\illBasisi{i}{\lambda}$,
and $\illCoef$ is a three dimensional vector
containing the linear model coefficients $\illCoefi{i}$.

\subsection*{Simple Illuminant Estimation}
We can see why linear models are efficient
by writing Equation \ref{e8:linMod} in matrix tableau.

\begin{equation}
\left (
 \begin{array}{ccc}
    & . & \\
    & . & \\
    & . & \\
   & e(\lambda) & \\
    & . & \\
    & . & \\
    & . & \\
 \end{array} \right ) =
\left (
 \begin{array}{ccc}
    .   & . & . \\
    .   & . & . \\
    .   & . & . \\
   \illBasisi{1}{\lambda} & \illBasisi{2}{\lambda} & \illBasisi{3}{\lambda} \\
    .   & . & . \\
    .   & . & . \\
    .   & . & . \\
 \end{array}
\right )
\left (
 \begin{array}{ccc}
   & \illCoefi{1} & \\
   & \illCoefi{2} & \\
   & \illCoefi{3} & \\
 \end{array}
\right ) \nonumber
\end{equation}

A single spectral power distribution, shown on the left,
consists of measurements at many different wavelengths.
The linear model summarizes all of these measurements as the weighted
sum of the basis functions, contained in the columns of the matrix
on the right.
These basis functions are fixed and assumed to be known.
The linear model is efficient because
it approximates each additional measurement
using only a few weights, $\illCoef$.

Efficiency is useful; but, if efficiency were our only
objective we could find
more efficient algorithms.
The linear models are also important because they
lead to very simple estimation algorithms.

As an example, consider how we might
use a device with three color sensors, like the eye,
to estimate the spectral power distribution of daylight.
Such a device is vastly simpler than the spectroradiometer
Judd et al. needed to make many measurements of the light.

Suppose we have a device with three color sensors,
whose spectral responsivities are, say, $\recSens{i}(\lambda), i = 1 \ldots 3$.
The three sensor responses will be
\begin{eqnarray}
\label{e8:recIll}
\recRespi{1} & = & \sum_\lambda \recSens{1}(\lambda) \ill{\lambda} \nonumber \\
\recRespi{2} & = & \sum_\lambda \recSens{2}(\lambda) \ill{\lambda} \nonumber \\
\recRespi{3} & = & \sum_\lambda \recSens{3}(\lambda) \ill{\lambda}  . 
\end{eqnarray}
We can group these three linear equations into a single
matrix equation
\begin{equation}
\label{e8:dayEst1}
\recResp = \sensorMat \illvec
\end{equation}
where the column vector
$\recResp$ contains the sensor responses, 
the rows of the matrix $\sensorMat$ are the sensor spectral responsivities,
and $\illvec$ is the illuminant spectral power distribution.

Before the Judd et al. study, it might have seemed that
these three sensor responses were insufficient to estimate the
illumination.
But, from their data we have learned that we can approximate
$\illvec$ with a  three-dimensional linear model, 
$\illvec \approx \illBasis \illCoef$.
This reduces the equation to
\begin{equation}
\label{e8:dayEst2}
\recResp \approx (\sensorMat \illBasis) \illCoef .
\end{equation}
The matrix $\sensorMat \illBasis$ is $3 \times 3$,
and its entries are all known.
The sensor responses, $\recResp$, are also known.
The only unknown is $\illCoef$.
Hence, we can estimate, $\illCoef$,
and use these weights to calculate
the spectral power distribution, $\illBasis \illCoef$.

This calculation illustrates two aspects
of the role of linear models in color calculations.
First, linear models represent a priori knowledge about the likely
set of inputs.
Using this information permits us to convert underdetermined
linear equations (Equation \ref{e8:dayEst1}) into
equations we can solve (Equation \ref{e8:dayEst2}).
They are a blunt tool for representing probabilities.
Using this information,
it becomes possible to use measurements
from only three color sensors
to estimate the 
full relative spectral power distribution of daylight illumination.

Second, linear models work smoothly with the imaging equations.
Since the imaging equations are linear,
the estimation methods remain linear and simple.

\subsection*{Surface Reflectance Models}
From the point of view of evolution, daylights are an
important class of signals.
No clear choice of important surface reflectance functions exist.
I was reminded by
this once by the brilliant color scientist, G. Wyszecki.
When I was just beginning my study of these issues,
I asked him
why he had not undertaken a study of surfaces similar to
daylight study.
He shrugged at me and answered, ``How do you sample the universe?''

Wyszecki was right, of course.
The daylight measurement study could begin and
end in a single paper.
There is no specific set of surfaces that
of equal importance to the daylights,
so we have no way to perform a similar
analysis on surfaces.

There are two related questions we can make some progress on.
First, we can ask what the properties are of certain
collections of surfaces that are of specific
interest to us, say for practical applications.
Second, we can ask what the visual system, with only
three types of cones, can infer about surfaces.

Over the years
linear models for special sets of materials
can be used in many applications.
Printer and scanner manufacturers may be interested in
the reflectance functions of inks.
Computer graphics programmers may be interested in the reflectance
factors of geological materials, or tea pots.
Color scientists have repeatedly measured the reflectance functions of
standard color samples used in industry, such
as the Munsell chips.
These cases can be of great practical value.
(Citations go here).

\subsection*{Measurement of Surface Reflectance}
In a certain sense, the notion of a surface reflectance
function is a ruse.
If you look about the room you are in, you will probably
see some surfaces that are shiny or glossy.
As you move around these surfaces, changing the geometrical
relationship between yourself, the lighting, and the surface,
the light reflected to your eye changes considerably.
Hence, the tendency of the surface to reflect light towards
your eye does not depend only on the surface.
The light scattered to your eye
depends on the viewing geometry as well as the surface.

This dependence on geometry occurs because
several different physical processes
can contribute simultaneously to 
the light scattered from a surface to the eye.
Figure \ref{f8:refProcess} sketches a
model illustrating two of these processes: {\em body } and
{\em interface} reflections.
Figure \ref{f8:refProcess} shows the incident light
falling on an {\em inhomogeneous} or {\em dielectric} surface~\footnote{
Dielectrics are non-conducting materials.}.
The material consists
of a clear substrate
with embedded colorant particles.

One way light is scattered from the surface
is by a mirror-like
reflection at the interface, 
This scattered light is quite restricted in angle,
much as a mirror reflects incident rays.

A second scattering process occurs
when the rays enter the material.
These rays are refelcted
randomly between the colorant particles.
A fraction of the incident light is absorbed by the material,
heating it up, and part of the light emerges.
When rays emerge after this process,
their direction is usually quite broad~\footnote{
S. Shafer introduced this reflectance model and
called it {\em dichromatic reflection model}.
He and his students have developed several
applications based on the observation
that light reflected from an object can be
represented as the weighted sum of interface
and body reflection.
$G_1 E( \lambda ) B ( \lambda ) + G_2 E ( \lambda )$
The first term is the product of the 
spectral power distribution of the incident illumination,
$E( \lambda )$ and the body reflectance, $B(\lambda)$.
The second term assumes is the interface reflection is
a constant function of wavelength.
The relative weights of these two terms, $G_i$,
depend on the viewing geometry.
This model separates the terms describing
the geometrical and spectral factors.
While only an approximation,
it provides important simplification
}.

When a surface has no interface reflection, but only
body reflections, the light
is usually scattered in all directions uniformly.
Such a surface is called a {\em matte}
or {\em Lambertian} reflector.
Interface reflection is commonly
called {\em specular} reflection and is the reason
why some objects appear {\em glossy}.
%8.5 high 9.5 wide
\begin{figure}
\centerline{
  \psfig{figure=../08col/fig/refProcess.ps,clip= ,height=4.25in}
}
\caption[Physical Mechanisms of Reflectance]{
{\em A model of the surface reflectance of inhomogeneous materials.}
(a) In the dichromatic reflection model,
light scattered from a surface arises through two mechanisms.
Some incident light is reflected
at the interface (interface reflection),
in a mirror-like fashion within a narrow set of directions.
Other light enters the material,
interacts with the embedded particles,
and is scattered in many directions (body reflection).
(b) The complete pattern of
reflected light consists of both body reflection,
reflected in many directions uniformly,
and interface reflection, concentrated in
in one principal direction.
This concentration makes specular highlights more intense.
}
\label{f8:refProcess}
\end{figure}

The geometrical difference between the body
and interface reflections gives rise to a second
property of reflection that you have probably noticed.
From the point of view of an observer, however,
specular highlights on a surface appear much brighter
than diffuse reflection.
Specular highlights can appear much brighter than
the diffuse reflection because
nearly all of the specular scattering
is confined to a small angle;
the body reflection is
divided among many directions.
Thus, while interface reflections provide a strong
signal they are not reliable.
As the object and observer
change their geometric relationship the specular
highlight moves along the surface
of the object, or it may disappear altogether.

For many types of materials,
interface reflection is not selective for wavelength.
The spectral power distribution of the light
scattered at the interface
is the same as the spectral power distribution of the
incident light.
This is the reason specular highlights 
take on the color of the illumination source.

Body reflection does not return all wavelengths uniformly.
The particles in the medium absorb light selectively,
and it is this property of the material that
distinguishes objects in terms of their color appearance.

\subsection*{Measurements of Matte Surfaces}
To discover the regularities in
surface functions, then, 
we should measure the body reflection terms.
From the studies that have taken place over the
last several years,
it has become increasingly clear that in the visible
wavelength region
the surface reflectance functions tend to be quite smooth,
and thus exhibit a great deal of regularity.
Hence, linear models serve to describe the reflectance
functions quite well.

For example, Cohen (1970),
Maloney (1986) and Parkinnen (1990) studied the reflectance
properties of a variety of surfaces including
special samples and some natural objects.
For each of the sets studied by
these authors the data
can be modeled nearly prefectly using a linear model
with less than six dimensions.
Excellent approximations, though
not quite perfect, can be obtained by three-dimensional
approximations.

As an example, I can describe
a linear model to approximate
a small collection surface reflectance functions
for a color target, the {\em Macbeth ColorChecker}, which
is used widely in industrial applications.
This target consists of 24 square patches
laid out in a 4 by 6 array.
The surfaces in this target were selected
to have reflectance functions similar to
a range of naturally occurring surfaces.
They include reflectances similar to human
skin, flora, and other materials.
\nocite{Mccamy}

\begin{figure}
\centerline{
 \psfig{figure=../08col/fig/macbethApprox.ps ,clip= ,height=3.0in}
}
\caption[Macbeth approximations]{
{\em Surface reflectances can be approximated using linear models.}
The panels in each row of this figure show the surface reflectance
functions of six colored surfaces (dashed line) and the approximation
to these functions using a linear model (solid lines).  The rows show
approximations using linear models with three (a), two (b) and one (c)
dimension respectively.  
}
\label{f8:macbethApprox}
\end{figure}
First, I measured the surface reflectance functions of
these patches with a spectral radiometer in my labortory.
The original data set, then, consisted of measurements
from 370nm to 730nm in 1 nm steps for each of the 24 patches.

Using conventional methods,
I calculated a three-dimensional linear model
to fit all of these surface reflectance functions.
The linear model basis functions, $\surBasisi{i}{\lambda}$, were
selected to minimize the squared error~\footnote{
I calculated the singular value decomposition of the matrix
whose columns consist of the surface reflectance vectors.
I used the left singular vectors as the basis functions.
}
\begin{equation}
\label{e8:surModel}
\left (\surf{\lambda} - \sum_{i=1}^{N} \surCoefi{i} \surBasisi{i}{\lambda} \right ) ^2 .
\end{equation}

The values $\surCoefi{i}$ are called the {\em surface coefficients},
and we will represent them as 
a vector, $\surCoef = (\surCoefi{1}, \ldots , \surCoefi{N})$.
There are fewer surface coefficients than data measurements.
If we create a matrix whose columns are are the basis
functions, $\surBasis$, then we can express
the linear model approximation as $\surBasis \surCoef$.

The dashed lines in Figure~\ref{f8:macbethApprox} show the
reflectance functions of 6 of the twenty-four surfaces.
The smooth curves within each
row of the figure contain the approximations
using linear models with different dimensionality.
The bottom row shows a one-dimensional linear model;
in this case the approximations
are scaled copies of one another.
As we increase the dimensionality of the linear model
the approximations become very similar to the originals.
The 3-dimensional model the approximations
are quite close to the true functions.

The low-dimensional linear model approximates
these surface reflectance functions because the 
functions vary smoothly as a function of wavelength.
The linear model captures the regularity
by consiting of a few, slowly varying
basis functions (see Figure~\ref{f8:macbethBasis}).
The first basis function captures the light-dark
variation of the surfaces.
The second basis function captures a red-green variation,
and the third a blue-yellow variation.

\begin{figure}
\centerline{
 \psfig{figure=../08col/fig/macbethBasis.ps ,clip= ,height=3.0in}
}
\caption[Macbeth Basis Functions]{
{\em Linear model basis functions}
used to approximate the Macbeth ColorChecker surface reflectance
functions are shown.
The basis functions vary slowly as a
function of wavelength, as do the real surface reflectances.
The first term is all positive and explains the most
variance in the surface reflectance functions.
The basis functions are ordered in terms
of their importance.
}
\label{f8:macbethBasis}
\end{figure}

\begin{figure}
\centerline{
 \psfig{figure=../08col/fig/macbethRender.ps ,clip= ,height=3.5in}
}
\caption[Macbeth Color Checker Approximations]{
{\em Color renderings of the Macbeth ColorChecker}
using a linear model approximation
to the surface reflectance functions under a blue sky daylight.
The dimension of the linear model
used in the approximation is shown above each image.
The one-dimensional approximation the surfaces 
appear neutral, varying only in lightness.
With three linear model dimensions and beyond,
the rendering is visually accurate for this illuminant.
}
\label{f8:macbethColor}
\end{figure}

Although the approximations are
quite good, there are still differences between the data
and the three dimensional linear model.
We might ask whether these differences are visually salient.
This question is answered in Figure \ref{f8:macbethColor},
which is a visual rendering of these surface approximations.
The one-dimensional model looks like shades of gray, and the
higher dimensional models approach the rendering from the
complete description of the data.
The three dimensional model does a rather good job
of approximating the surfaces under this illuminant.

\subsection*{Sensor-based error measures. }
I have described linear models
that minimize the squared error between
the approximation and the original spectral function.
As the last analysis showed, however, when we
choose linear models to minimize the spectral error
we are not always certain whether we have done a good
job in minimizing the visual error.
In some applications, the spectral error
may not be the objective
function that we care most about minimizing.
When the final consumer of the image is a human,
we may only need to capture
that part of the reflectance function
that is seen by the sensors.

If we are modeling reflectance functions for
a computer graphics application, for example,
there is no point in modeling the reflectance function at 300nm
since the human visual system cannot sense light in that part
of the spectrum anyway.
For these applications, we should be careful to 
model accurately those parts of the function that are most
significant for vision.
In these cases, one should select
a linear model of the surface reflectance by minimizing
a different error measure, one that takes into
account the selectivity of the eye.

Marimont and Wandell (1992) describe how
to create linear models that are appropriate for the eye.
They consider how to define linear models that
minimize the root mean squared error in the photopigment
absorption rates, rather than the root mean squared
error of the spectral curves.
Their method is called {\em one-mode analysis}.
For many applications,
the error measure minimized by one-mode analysis
is superior to root mean squared error of the
spectral curves.

\subsection*{Surface and Illuminant Estimation}
There is much regularity in daylight
and surface functions;
so, it makes sense to evaluate
how well we estimate spectral functions from sensor responses.
Estimation algorithms rely on two essential components.

First, we need a method of representing our knowledge
about the likely surface and illuminant functions.
I will use
linear models or sophisticated variants of
these linear models to encode our a priori knowledge.
These methods, or sophisticated variants of them,
are widely used~\footnote{
Some of the more sophisticated models
try to take into account the fact that real surfaces and illuminant
functions must be non-negative;
some authors are trying to find ways to
incorporate Bayesian estimation directly (see Chapter~\ref{chapter:space}).
}.

A second important assumption,
common to most estimation methods,
concerns the illumination.
Nearly all estimation methods assume that the illumination
varies either slowly or not at all across the image.
This assumption is important
because it means that the illumination
adds very few new parameters to estimate.
Take an example:
Suppose we have an image with p points;
there are 3 photopigment absorption rates at each image point;
the surface reflectance model
is 3 dimensional; the illuminant model is 3 dimensional.
With these assumptions there are 3p sensor responses
to use as data.
There are also 3p surface reflectance models.

Since the relationship between surface and sensor response
is linear, we could solve for the surface reflectances
were the illuminant known.
The additional unknown illuminant parameters
make the problem a challenge.
If the illuminant can vary from point to point, there
will be 3p unknown parameters and the mismatch between
known and uknown parameters will be very great.
But, if the illuminant is constant across the
image, we only have 3 additional parameters.
By making some modest assumptions about the image, we
can find ways to infer these three parameters and then
proceed to estimate the surface parameters.

It follows that the role of the
estimation algorithms is to formulate a method
of reducing the mismatch between
the measurements and the unknowns.
We can divide existing estimation algorithms into two groups.

The majority of estimation algorithms 
infer the illumination parameters by making
one additional assumption about the image contents.
For example, suppose we know the reflectance function of
one surface.
Then, we can use the sensor responses
from that surface to measure the illuminant parameters.
Knowing the reflectance function
of one surface in the image compensates
for the three unknown illuminant parameters.

There are several implementations of this principle.
The most important is the
assumption that the average of all
the surfaces in the image is gray,
which is called the {\em gray-world} assumption
(Buchsbaum, 1980; Land, 1988).
Other algorithms are based on the assumption
that the brightest surface
in the image is a uniform, perfect reflector (McCann, 198*).
A interesting variant on both of these
assumptions is the idea
that we can identify specular or glossy surfaces in the image.
Since specularities reflect the illumination directly,
often without altering the illuminant's spectral power distribution,
the sensor responses to
glossy surfaces provide information about the
illuminant (Lee; D'Zmura and Lennie; Tominaga and Wandell).

A second group of estimation algorithms
refuses to yield any ground concerning image contents.
Instead, these algorithms compensate for the mismatch in
measurements and parameters
by acquiring more data.
For example, Maloney and I showed that if one adds a fourth sensor
(at three spatial locations),
one can also estimate
the surface and illuminants.
D'Zmura and Iverson thoroughly 
explored an interesting variant of this idea.
They asked what information is available
if we observe the same surface under several illuminants.
Changing the illumination on a surface is conceptually
equivalent to seeing the surfaces with additional sensors
(e.g., see Wandell, 198X).
Pooling information about the same object seen under
different illuminants, is much like acquiring additional
information from extra sensors.

\subsection*{Implications for Color Appearance}
We have performed this computational analysis
of surface and illuminant estimation
to provide a framework for thinking about color appearance.
It is time to summarize what we have learned,
and to frame some specific implications for color appearance.

We began with the knowledge that there is a linear
relationship between the photopigment absorptions
and incident light (Equation~\ref{e8:recIll}).
Next, we represented the
light incident at the eye
in terms of the illuminant and surface functions.
The photopigment absorptions can be described, then,
by the three equations,
\begin{eqnarray}
\label{e8:recSur}
r_1 & = & \sum_{\lambda} R_1(\lambda) E(\lambda) S(\lambda) \nonumber \\
r_2 & = & \sum_{\lambda} R_2(\lambda) E(\lambda) S(\lambda) \nonumber \\
r_3 & = & \sum_{\lambda} R_3(\lambda) E(\lambda) S(\lambda) .
\end{eqnarray}

We then asked
how one might use the photopigment absorptions to
estimate the surface and illuminant
functions, $E(\lambda)$ and $S(\lambda)$.
Since there are only three cones,
it is apparent that our estimates must rely
on a priori knowledge about the surface and illuminant functions.
We reviewed measurements of these spectral functions,
and we summarized their properties using linear models.

We can use the linear models to express our best
guess about the
relationship between the photopigment absorptions
and the surfaces and illuminants.
If the surfaces and illuminants all fall within the
linear models, then following
some algebraic manipulation
we can derive a fundamental equation relating the
surface coefficients to the cone absorptions.
\begin{equation}
\label{e8:est1}
\recResp \approx \illMat{e} \surCoef .
\end{equation}
where $\recResp$ are the cone absorptions, $\surCoef$
are the linear model coefficients for the surface,
and $\illMat{e}$ is the {\em lighting} matrix.
The $ij^{th}$ entry of the lighting matrix is
\begin{equation}
\label{e8:lmatrix}
\sum_k \illCoefi{k} 
 \sum_{\lambda} \illBasisi{k}{\lambda} 
                \recSens{i}(\lambda) 
                \surBasisi{j}{\lambda} .
\end{equation}
Equation~\ref{e8:est1} represents two important points:
\be
\item The photopigment absorptions are linearly
related to the linear model surface coefficients.
\item The linear transformation relating these
quantities depends on the illuminant.
\ee

These two observations are key
to deciding whether the cone absorptions
observed under two illuminants are from the same surface.
The argument is as follows.

From Equation \ref{e8:est1} we see that
the photopigment absorptions to {\em the same surface} under
two different illuminants are
\begin{eqnarray}
\recResp & \approx & \illMat{e} \surCoef, \mbox{ and } \nonumber \\
\recResp' & \approx &  \illMat{e'} \surCoef \nonumber .
\end{eqnarray}

Because we think of color appearance as
a perceptual representation of the surface coefficients,
we expect color appearance to represent the nervous
system's estimation, $\illMatinv{e} \recResp$, rather than
the cone absorptions, $\recResp$.
It follows that objects seen
under two illuminants will look the same when
\begin{equation}
\label{e8:est2}
\illMatinv{e} \recResp \approx \illMatinv{e'} \recResp ' .
\end{equation}

This line of reasoning suggests that there should
be a linear relationship between the cone absorptions
of objects that look the same under different illuminants.
Rearranging Equation \ref{e8:est2} we have
\begin{equation}
\label{e8:est3}
\recResp \approx \left ( \illMatinv{e} \illMatinv{e'} \right ) \recResp ' .
\end{equation}

The precision of Equations~\ref{e8:est1}, \ref{e8:est2}, and \ref{e8:est3}
will depend
on how well the linear models describe
the surface and illuminant functions.
If the surface and illuminant functions are not regular,
and the cannot be well-described with low dimensional approximations,
then the approximations in these equations will be very poor.
If the surface and illuminant functions are regular,
then these approximations can be very accurate.
We can only interpret cone absorptions across
different illuminants accurately if the surface and illuminant
functions are regular.
In this case, Equations~\ref{e8:est1},\ref{e8:est2}, and \ref{e8:est2}
describe the cone absorptions under the
different illuminants accurately.

It is instructive to compute an example of the
linear transformation in Equation \ref{e8:est3}.
We can use
the Judd et al. illuminant basis functions to
approximate the illuminants (Figure~\ref{f8:linearModDay});
the Macbeth ColorChecker surface basis functions
to approximate the surface (Figure~\ref{f8:macbethBasis});
and the Stockman and MacLeod cone absorption functions
(see the appendix to Chapter \ref{chapter:wavelength}).
These quantities are sufficient to calculate the entries
of the lighting matrix in Equation \ref{e8:est1}.

Now, we can calculate the relationship
between the cone absorptions
when the illumination changes
between the two curves shown in Figure~\ref{f8:illExample}.
One of these illuminants is the mean daylight, which
has a lot of blue sky.
The other has relatively more long-wavelength energy
from the disk of the sun.
I will refer to them as blue sky and disk of the sun.
\begin{figure}
\centerline{
  \psfig{figure=../08col/fig/illExample.ps ,clip= ,height=3.5in}
}
\caption[Illuminant Example]{
Spectral power distributions of the two illuminants used 
in the surface reflectance estimation computation.
One illuminant has relatively more blue skylight and the other has
relatively more light from the disk of the sun.
}
\label{f8:illExample}
\end{figure}

First, I computed the matrices $\illMat{e}$ and $\illMat{e'}$
for each illuminant using the definition in expression~\ref{e8:lmatrix}.
Then I computed the linear transformation that
maps the cone absorptions under the disk of the sun to
the cone absorptions under the blue sky (Equation \ref{e8:est2}).
This matrix is
\begin{eqnarray}
\label{e8:e8:est3}
\left (
 \begin{array}{ccc}
    0.8119 &   0.2271  &  0.0550 \\
   -0.0803 &   1.1344  &  0.1282 \\
    0.0429 &  -0.0755  &  1.8091 \\
 \end{array}
\right ) .
\end{eqnarray}
We can use this linear transformation
to predict what the cone absorption rates from
a surface seen under disk of the sun will 
be when move the surface under a blue sky.
For example, under the disk of the sun
the relative cone absorptions from a white
surface are $(1.13,0.91,0.38)$
while under the blue sky the will become $(1.15, 1.0,.67)$.
These calculations show that 
the cone absorptions of the Macbeth ColorChecker surfaces,
seen under these two illuminants,
are approximately related by a linear transformation.

\subsection*{Von Kries Coefficient Law:  Theory}
One might have hoped that the matrix relating
cone absorptions under different illuminants would be simpler,
namely diagonal.
Were the matrix in expression~\ref{e8:est3} diagonal,
one could simply scale the $\Red$ cone absorptions
without regard to the other cone classes.
Since, the matrix is not diagonal,
to compute the change the $\Red$ cone absorptions
each calculation
must take into account the values of all
three cone absorption rates.

Notice, however, that the diagonal terms in the
correction are quite a bit larger than the off-diagonal
terms.
Thus while we know that the proper correction is general,
we might ask how poorly do we do simply by scaling
the signal from each of the receptor classes alone.
I have estimated the best squared error
approximation using a simpler, diagonal matrix, to be
\begin{eqnarray}
\left (
 \begin{array}{ccc}
    0.9992 &       0  &      0  \\
         0 &  1.0744  &      0  \\
         0 &        0 &  1.7590 \\
 \end{array}
\right ) .
\end{eqnarray}
Interestingly, the diagonal values are quite close the 
ratio of cone absorptions of the white surface under the two illuminants.
The ratios of the cone absorptions from the white surface
when the illuminant changes are $(1.01,1.09,1.77)$.

In this example, as is in many others,
the best diagonal correction is not much worse
than the best linear transformation.
The mean error of the diagonal correction
is roughly 5 percent,
while the full linear model calculation
has a mean error of 1 percent.
And, the entries of the best diagonal are not too different
from the ratios of the cone absorptions to a white surface.

Compensating for the illumination
change by a purely diagonal scaling of the cone absorptions
is called {\em von Kries Coefficient Law}.
The method is not as precise as the general
linear correction, but it frequently
provides a good approximation.
And, as we shall see in the next section,
this simple calculation describes certain aspects
of human color appearance measurements as well.

