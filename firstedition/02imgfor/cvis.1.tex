
\chapter{Image Formation}
\label{chapter:imageformation}
\pagestyle{headings}  

The cornea and lens are at the interface between
the physical world of light
and the neural encoding of the visual pathways.
The cornea and lens bring light
into focus at the light sensitive receptors in our retina
and initiate a series of visual
events that result in our visual experience.

The initial encoding of light at the retina is
but the first in a series of visual transformations:
The stimulus incident at the cornea
is transformed into an image at the retina.
The retinal image is transformed into a neural response by the
light sensitive elements of the eye, the photoreceptors.
The photoreceptor responses are transformed to
a neural response on the optic nerve.
The optic nerve representation is transformed into a
cortical representation, and so forth.
We can describe most of our understanding
of these transformations, and thus most of our understanding
of the early encoding of light by the visual pathways
by using {\em linear systems theory}.
Because all of our visual experience is limited by the 
image formation within our eye,
we begin by describing this transformation of the light signal
and we will use this analysis as an introduction to linear methods.

\section{Optical Components of the Eye}
\begin{figure}
\centerline {
\psfig{figure=../02imgfor/fig/eyeball.ps,clip=,height=3.0in}
}
\caption[EyeBall From Salzman]{
{\em The imaging components of the eye.}
The cornea and lens focus the image onto the retina.
Light enters through
the pupil which is bordered by the iris.
The fovea is a region of the retina that is specialized
for high visual acuity and color perception.
The retinal output fibers leave at a point in the
retina called the blindspot.
The bundle of output fibers is called the optic nerve.
% The anatomy and histology of the human eyeball in the
% normal state; Salzman, 1912, U. of Chicago press
% But, copied from Rodieck's book, along with the citation.
% (After Salzman,1912).
}
\label{f1:eyeball}
\end{figure}
Figure \ref{f1:eyeball}
contains an overview of the imaging components of the eye.
Light from a source arrives at the cornea
and is focused by the cornea and lens
onto the {\em photoreceptors},
a collection of light sensitive neurons.
The photoreceptors are part of a thin
layer of neural tissue, called the {\em retina}.
The photoreceptor signals are communicated through the 
several layers of retinal neurons 
to the neurons whose output fibers makes up the {\em optic nerve}.
The optic nerve fibers exit through a hole in the
retina called the {\em optic disk}.
The optical imaging of light incident at the cornea
into an image at the retinal photoreceptors
is the first visual transformation.
Since all of our visual experiences are influenced by
this transformation, we begin the study of vision
by analyzing the properties of image formation.

\begin{figure}
\centerline {
\psfig{figure=../02imgfor/fig/monitor.to.retina.ps,clip=,height=3.0in}
}
\caption[Monitor to Retina]{
{\em Retinal image formation} illustrated with a single-line
input image.
(a) A one-dimensional monitor image consists of a set of 
lines at different intensities.
The image is brought to focus on the retina
by the cornea and lens.
(b) We can represent the intensity of a
one-dimensional image using a simple graph 
that shows the light as a function of horizontal screen position.
Only a single value is plotted since the one-dimensional
image is constant along the vertical dimension.
(c) The retinal image is a blurred version of the one-dimensional
input image.
The retinal image is also one-dimensional and
is also represented by a single curve.
}
\label{f1:monitor.to.retina}
\end{figure}
When we study transformations, we must specify
their inputs and outputs.
As an example, we will consider how simple
{\em one-dimensional} intensity
patterns displayed on a video display monitor
are imaged onto the retina (Figure~\ref{f1:monitor.to.retina}a).
In this case the input is
the light signal incident at the cornea.
One-dimensional patterns have a constant intensity along
the, say, horizontal dimension and varies along the 
perpendicular (vertical) dimension.
We will call the pattern of light intensity
we measure at the monitor screen the {\em monitor image}.
We can measure the intensity of the one-dimensional
image by placing a light-sensitive device called a {\em photodetector}
at different positions on the screen.
The vertical graph in Figure~\ref{f1:monitor.to.retina}b shows
a measurement of the intensity of the monitor image at all screen locations.

The output of the optical transformation is the image formed
at the retina.
When the input image is one-dimensional, the retinal image
will be one-dimensional, too.
Hence, we can represent it using a curve as in
Figure~\ref{f1:monitor.to.retina}c.
We will discuss the optical components of the visual
system in more detail later in this chapter,
but from simply looking at a picture of the
eye in Figure~\ref{f1:eyeball} we can see that
the monitor image passes through a lot of biological material
before arriving at the retina.
Because the optics of the eye are not perfect, the retinal
image is not an exact copy of the monitor image:
The retinal image is a blurred copy of the input image.

The image in Figure~\ref{f1:monitor.to.retina}b shows one example
of an infinite array of possible input images.
Since there is no hope of measuring the response to
every possible input,
to characterize optical blurring completely
we must build a model
that specifies how any input image
is transformed into a retinal image.
We will use {\em linear systems} methods
to develop a method of predicting
the retinal image from any input image.

\section{Reflections From the Eye}
To study the optics of a human eye
you will need an experimental eye, so
you might invite a friend to dinner.
In addition, you will
need a light source, such as a candle,
as a stimulus to present to your friend's eye.
If you look directly into your friend's
eye, you will see a mysterious darkness that has beguiled
poets and befuddled visual scientists.
The reason for the darkness can be understood
by considering the problem of ophthalmoscope design
illustrated in Figure \ref{f1:ophthalmoscope}a.
\begin{figure}
\centerline {
\psfig{figure=../02imgfor/fig/ophthalmoscope.ps,clip=,height=2.0in,width=5.0in}
}
\caption[Ophtalmoscope Principles]{
{\em Principles of the ophthalmoscope.}
An ophthalmoscope is used to see an image reflected
from the interior of the eye.
(a) When we look directly into the eye,
we cast a shadow making it impossible to see
light reflected from the interior of the eye.
(b) The ophthalmoscope permits us to
see light reflected from the interior of the eye.
Helmholtz invented the first ophthalmoscope.
(After Cornsweet, 1970).
% Cornsweet's book figure 3.22 and 3.24 combined
%	Do we have permission?  Should/can we redraw to avoid payments?
}
\label{f1:ophthalmoscope}
\end{figure}

If the light source is behind you, so that your
head is between the light source and the eye you are studying,
then your head will cast
a shadow that interferes with the light from the point source
arriving at your friend's eye.
As a result, when you look in to measure the 
retinal image you see nothing beyond what is in your heart.
If you move to the side of the light path,
the image at the back of your friend's eye will be reflected
towards the light source, following a reversible path.
Since you are now on the side, out of the path of the light source, 
no light will be sent towards your eye\footnote{
The great nineteenth century scientist, H. von Helmholtz,
built the first ophthalmoscope.
The ophthalmoscope design shown in Figure~\ref{f1:ophthalmoscope}
is unconventional, though it does include the basic principle:
We need to arrange a light path
so that the examiner's eye does not cast a shadow.
A bright source light is required since
the back of the human eye is not very reflective.
An more conventional
design is in Appendix III of {\em Visual Perception} by T. Cornsweet.
}

%height= 5 width = 10.5
\begin{figure}
\centerline {
\psfig{figure=../02imgfor/fig/cg.apparatus.ps,clip=,width=5.5in,height=2.5in}
}
\caption[Double Pass Instrument]{ 
{\em A modified opthalmoscope}
measures the human retinal image.
Light from a bright source passes through a slit and into the eye.
A fraction of the light is reflected from the retina
and is imaged.
The intensity of the reflected light is measured at different
spatial positions by varying the location of the analyzing slit.
(After Campbell and Gubisch, 1967).
% Figure 2, Optical quality of the human eye J. Physiol., p. 560
}
\label{f1:cg.apparatus}
\end{figure}
Flamant (1955) first measured the retinal 
image using a modified ophthalmoscope.
She modified the instrument by placing a light sensitive recording,
a {\em photodetector}, at the position normally reserved for the ophthalmologist's eye.
In this way, she measured the intensity pattern of the light
reflected from the back of the observer's eye.
Campbell and Gubisch (1967) used Flamant's method
to build their apparatus, which
is sketched in Figure \ref{f1:cg.apparatus}.
Campbell and Gubisch measured
the reflection of a single bright line,
that served the input stimulus in their experiment.
As shown in the Figure, a beam-splitter placed 
between the input light and the
observer's eye divides the input stimulus
into two parts.
The beam-splitter causes some of the light to be turned away
from the observer and lost;
this stray light is absorbed by a light baffle.
The rest of the light continues toward the observer.
When the light travels in this direction, the beam-splitter is an annoyance,
serving only to lose some of the light;
it will accomplish its function on the return trip.

The light that enters the observer's eye is brought to 
a good focus on the retina by a lens.
A small fraction of the light incident on the retina
is reflected and passes -- a second time -- through the optics of the eye.
On the return path of the light,
the beam-splitter now plays its functional role.
The reflected image would normally return to a focus
at the light source.
But the beam-splitter
divides the returning beam so that a portion
of it is brought to focus in a measurement plane 
to one side of the apparatus.
Using a very fine slit in the
measurement plane, with a photodetector behind it,
Campbell and Gubisch measured the reflected light
and used the measurements of the reflected light
to infer the shape of the image on the retinal surface.

\begin{figure}
\centerline{
\psfig{figure=../02imgfor/fig/retinalCrossSection.ps,clip=,height=3.0in}
}
\caption[Retinal Cross Section]{ 
{\em The retina} contains the light sensitive photoreceptors
where light is focussed.
This cross-section of a monkey retina outside the fovea
shows there are several layers of neurons
in the optical path between the lens and the photoreceptors.
As we will see later, in the central fovea
these neurons are displaced to leaving a clear
optical path from the lens to the photoreceptors
(Source: Boycott and Dowling, 1969).
% Phil. Trans. of the Roy. Soc. v. 255 p. 109-184 Fig. ???
}
\label{f1:retinalCrossSection}
\end{figure}
What part of the eye reflects the image?
In Figure \ref{f1:retinalCrossSection}
we see a cross-section of the peripheral retina.
In normal vision, the image
is focused on the retina
at the level of the photoreceptors.
The light measured by Campbell and Gubisch probably
contains components from several different planes
at the back of the eye.
Thus, their measurements probably underestimate the quality
of the image at the level of the photoreceptors.
\nocite{BoycottDowlingSeeROdieckReference}

\begin{figure}
\centerline {
\psfig{figure=../02imgfor/fig/cg.data.ps,clip=,height=3.0in}
}
\caption[Campbell and Gubisch Data]{ 
\label{f1:cg.data}
{\em Experimental measurements} of light
that has been reflected from a human eye looking at a fine line.
The reflected light has been blurred by double passage through
the optics of the eye.
(Source: Campbell and Gubisch, 1967).
%Optical quality Figure 5
}
\end{figure}
Figure \ref{f1:cg.data} shows
several examples of Campbell and Gubisch's measurements
of the light reflected from the eye when the observer is
looking at a very fine line.
The different curves show measurements
for different pupil sizes.
When the pupil was wide open (top, 6.6mm diameter) the reflected
light is
blurred more strongly than when the pupil is closed (middle, 2.0mm).
Notice that the measurements made with a large pupil opening
are less noisy;
when the pupil is wide open more light passes into the eye
and more light is reflected, improving the quality of the measurements.

The light measured in Figure~\ref{f1:cg.data} passed through the
optical elements of the eye twice, while
the retinal image passes through the optics only once.
It follows that the spread in these curves is wider than the
spread we would observe had we measured at the retina.
How can we use these {\em doublepass}
measurements to estimate the blur at the retina?
To solve this problem,
we must understand the general features of their experiment.
It is time for some theory.

\section{Linear Systems Methods}
A good theoretical account of a
transformation, such as the mapping from monitor image to retinal image,
should have two important features.
First, the theoretical account should
suggest to us {\em which} measurements we should
make to characterize the transformation fully.
Second, the theoretical account should
tell us {\em how}  to use these measurements
to predict the retinal image distribution for
all other monitor images.

In this section we will develop a set of general
tools, referred to as {\em linear systems methods}.
These tools will permit us to solve the
problem of estimating the optical transformation from the
monitor to the retinal image.
The tools are sufficiently general, however, that we will
be able to use them repeatedly throughout this book.

There is no single theory that
applies to all measurement situations.
But, linear systems theory does apply
to many important experiments.
Best of all,
we have a simple experimental test that permits us to
decide whether linear systems theory is
appropriate to our measurements.
To see whether linear systems theory is appropriate,
we must check to see that our data satisfy the two properties of
{\em homogeneity} and {\em superposition}.

\subsection*{Homogeneity}

\begin{figure}
\centerline {
\psfig{figure=../02imgfor/fig/homogeneity.ps,clip=,height=3.0in}
}
\caption[Homogeneity]{
{\em The principle of homogeneity} illustrated.
An input stimulus and corresponding
retinal image are shown in each part of the figure.
The three input stimuli
are the same except for a scale factor.
Homogeneity is satisfied when the corresponding
retinal images are scaled by the same factor.
Part (a) shows an input image at unit intensity,
while (b) and (c) show the image scaled by 0.5 and 2.0 respectively.
}
\label{f1:homogeneity}
\end{figure}

A test of {\em homogeneity}
is illustrated in Figure~\ref{f1:homogeneity}.
The left-hand panels show a series of monitor images,
and the right-hand panels show the corresponding measurements
of reflected light\footnote{
We will use vectors and matrices in our
calculations to eliminate burdensome notation
Matrices will be denoted by boldface, upper case Roman letters, $\bf M$.
Column vectors will be denoted using lower case boldface Roman letters,
${\bf v}$.
The transpose operation will be denoted by a superscript T, ${\bf v}^T$.
Scalar values will be in normal typeface, and they will
usually be denoted using Roman characters ($a$) except when
tradition demands the  use of Greek symbols ($\alpha$).
The $i^{th}$ entry of a vector, ${\bf v}$, is a scalar and will
be denoted as $v_i$.
The $i^{th}$ column of a matrix, $\bf M$, is a vector
that we denote as ${\bf m}_i$.
The scalar entry in the $i^{th}$ row and
$j^{th}$ column of the matrix ${\bf M}$ will be denoted $m_{ij}$.
}.
Suppose we represent the intensities of the
lines in the one-dimensional monitor image
using the vector $\pixvec$ (upper left)
and we represent the retinal image measurements
by the vector $\resvec$.
Now, suppose we scale the input signal by a factor $a$,
so that the new input is $a \pixvec$.
We say that the system satisfies homogeneity
if the output signal is also scaled by the same factor of $a$,
and thus the new output is $a \resvec$.
For example, if we halve the input intensity,
then the reflected light measured at their photodetector
should be one-half the intensity (middle panel).
If we double the light intensity, the response should double (bottom panel).
Campbell and Gubisch's measurements
of light reflected from the human eye satisfy homogeneity.

\subsection*{Superposition}

\begin{figure}
\centerline {
\psfig{figure=../02imgfor/fig/superposition.ps,clip=,height=3.0in}
}
\caption[Superposition]{
{\em The principle of superposition} illustrated.
Each of the three parts of the picture shows an
input stimulus and the corresponding retinal image.
The stimulus in part (a) is a single-line
image and in part (b) the stimulus is a second line displaced from
the first.
The stimulus in part (c) is the sum of the first two lines.
Superposition holds if the retinal image in part (c) is
the sum of the retinal images in parts (a) and (b).
}
\label{f1:superposition}
\end{figure}
{\em Superposition}, used as both an experimental
procedure and a theoretical tool,
is probably the single most important idea in this book.
You will see it again and again in many forms.
We describe it here for the first time.

Suppose we measure the response to two different input stimuli.
For example, suppose we find
that input pattern $\pixvec$ (top left) generates the
response $\resvec$ (top right),
and input pattern $\pixvec '$ (middle left) generates
response $\resvec '$ (middle right).
Now we measure the response
to a new input stimulus equal to the sum of $\pixvec$ and $\pixvec '$.
If the response to the new stimulus
is the sum of the responses measured singly, $\resvec  + \resvec ' $,
then the system is a {\em linear system}.
By measuring
the responses stimuli individually and then the
response to the sum of the stimuli, we test superposition.
When the responses to sum of the stimuli equals the sum of
the individual responses,
then we say the system satisfies superposition.
Campbell and Gubisch's measurements of light reflected from
the eye satisfy this principle.

We can summarize homogeneity and superposition
succinctly using two equations.
Write the linear optical transformation that maps the input image to the
light intensity at each of the receptors as
\begin{equation}
\resvec = L ( \pixvec ) ~~~.
\end{equation}

Homogeneity and superposition are defined by the pair of equations
\footnote{Notice that 
the superposition leads us to expect
homogeneity for integer scalars since
\[
L( 2 \pixvec) = 
 L ( \pixvec + \pixvec ) = 
 L ( 2 \pixvec ) = 
 2 L ( \pixvec ) \nonumber
\]
and in general if we sum $n$ copies of $\pixvec$
\[
L ( n \pixvec ) = n L ( \pixvec )
\]
We write the homogeneity separately from superposition
to avoid the tedium of treating the case of irrational numbers
in certain proofs.}
\begin{eqnarray}
L ( a \pixvec ) = & a L ( \pixvec ) &  (~Homogeneity~) \\
L ( \pixvec + \pixvec ' ) = & L ( \pixvec ) + L ( \pixvec ' ) & (~Superposition~)
\end{eqnarray}

\subsection*{Implications of Homogeneity and Superposition}
\begin{figure}
\centerline{
\psfig{figure=../02imgfor/fig/app.hom.sup.ps,clip=,height=4.5in}
}
\caption[Homogeneity and Superposition]{
{\em Application of homogeneity and superposition.}
(a)
A one-dimensional monitor image is the weighted sum of a set of lines.
An example of a one-dimensional image is shown on the left
and the individual monitor lines comprising
the monitor image are shown separately on the right.
(b) Each line in the component monitor image 
contributes to the retinal image.
The retinal images created by the individual lines
are shown below the individual monitors.
The sum of the retinal images is shown on the left.
(c) The retinal image generated by
the $i^{th}$ monitor line at unit intensity is 
represented by the vector $\resvec_{i}$.
The intensity of the $i^{th}$
monitor line is $\pixveci{i}$.
By homogeneity, the retinal image of the $i^{th}$ monitor line
is $\pixveci{i} \resvec_{i}$.
By superposition, the retinal image of the collection
of monitor lines is sum 
of the individual retinal images, $\sum \pixveci{i} \resvec_{i}$.
}
\label{f1:app.hom.sup}
\end{figure}
Figure \ref{f1:app.hom.sup} illustrates how we will use
linear systems methods to characterize
the relationship between the input signal from a monitor,
light reflected from the eye\footnote{We analyze a one-dimensional monitor
images to simplify the notation. The principles remain the same,
but the notation becomes cumbersome, when we consider two-dimensional images.}.
First, we make an initial set of measurements
of the light reflected from the eye for each single-line monitor
image, with the line set to unit intensity.
If we know the images from single-line images,
and we know the system is linear,
then we can calculate the light reflected from the eye
from any monitor image:
Any one-dimensional image is the sum of a collection of lines.

Consider an arbitrary one-dimensional image, as illustrated at the
top of Figure~\ref{f1:app.hom.sup}.
We can conceive of this image as the sum of a set of single-line monitor
images, each at its own intensity, $\pixveci{i}$.
We have measured the reflected light from each single-line image
alone, call this $\resvec_i$ for the $i^{th}$ line.
By homogeneity it follows that the reflected light from $i^{th}$ line
will be a scaled version of this response, namely $\pixveci{i} \resvec_i$..
Next, we combine the light reflected from the single-line images.
By superposition, we know that the light reflected from the
original monitor image, $\resvec$, is the sum of the light reflected from
the single-line images,
\begin{equation}
\label{e1:MatrixMultiplication0}
\resvec = \sum_i^{N} \pixveci{i} \resvec_i .
\end{equation}

Equation~\ref{e1:MatrixMultiplication0} defines
a transformation
that maps the input stimulus, $\pixvec$, into the measurement, $\resvec$.
Because of the properties of homogeneity and superposition,
the transformation is the weighted sum of a fixed collection of vectors:
When the monitor image varies, only
the weights in the formula, $\pixveci{i}$, vary but
the vectors $\resvec_i$, the reflections from single-line stimuli,
remain the same.
Hence, the reflected light will always be the weighted sum of these
reflections.

To represent the weighted sum of a set of
vectors, we use the mathematical notation of {\em matrix multiplication}.
As shown in Figure \ref{f1:MatMult},
multiplying a matrix times a vector
computes the weighted sum of the matrix columns;
the entries of the vector define the weights.
Matrix multiplication and linear systems methods are closely linked.
In fact, the set of all possible matrices define the set of 
all possible linear transformations of the input vectors.

\begin{figure}
\centerline {
\psfig{figure=../02imgfor/fig/mat.mult.ps,clip=,height=3.0in}
}
\caption[Matrix Multiplication]{
{\em Matrix multiplication} is a convenient notation
for linear systems methods.
For example, the weighted sum of a set of vectors, as in part (c) of
Figure~\ref{f1:app.hom.sup}, can be represented using
matrix multiplication.
The matrix product equals the sum of
the columns of $\Resmat$ weighted by the entries of $\pixvec$.
When the matrix describes the responses
of a linear system, we call it a {\em system matrix.}
}
\label{f1:MatMult}
\end{figure}
Matrix multiplication has a shorthand notation
to replace the explicit sum of vectors in
Equation~\ref{e1:MatrixMultiplication0}.
In the example here, we define a
matrix, $\Resmat$, whose columns are
the responses to individual monitor lines at unit intensity,
$\resvec_i$.
The matrix $\Resmat$ is called the {\em system matrix}.
Matrix multiplication of the input vector, $\pixvec$, times the 
system matrix $\Resmat$,
transforms the input vector into the output vector.
Matrix multiplication is written using the notation
\begin{equation}
\resvec = \Resmat \pixvec .
\end{equation}
Matrix multiplication follows naturally from
the properties of homogeneity and superposition.
Hence, if a system satisfies homogeneity and superposition,
we can describe the system response by creating
a {\em system matrix} that transforms the input to the output.

\paragraph{A numerical example of a system matrix. }
\comment{
sys = [ 0.1  0  0 ;
   0.2  0.1  0 ;
   0.5  0.2  0.2;
   0.3  0.5  0.5 ;
   0    0.1  0.3 ;
   0    0    0 ];
p = [0.5 1 0.2];
sys* p'
}
Let's use a specific numerical example to illustrate the principle
of matrix multiplication.
Suppose we measure a monitor that displays only three lines.
We can describe the monitor image using
a column vector with three entries, $\pixvec = (p_1, p_2, p_3)^T$.

The lines of unit intensity are
$(1,0,0)^T$, $(0,1,0)^T$ and $(0,0,1)^T$.
We measure the response to these input vectors
to build the {\em system matrix}.
Suppose the measurements for these three lines are
$(0.1,0.2,0.5,0.3,0,0)^T$, $(0,0.1,0.2,0.5,0.1,0)^T$, and
$(0,0,0.2,0.5,0.3,0)^T$ respectively.
We place these responses into the columns of the system matrix:
\begin{equation}
\Resmat = 
\left (
 \begin{array}{ccc}
   0.1 & 0 & 0 \\
   0.2 & 0.1 & 0 \\
   0.5 & 0.2 & 0.2 \\
   0.3 & 0.5 & 0.5 \\
   0   & 0.1 & 0.3 \\
   0   & 0   & 0 
  \end{array}
\right )
\end{equation}
We can predict the response to any monitor
image using the system matrix.
For example, if the
monitor image is $\pixvec = (0.5,1.0,.2)^T$
we multiply the input vector and the system matrix to obtain
the response, on the left side of Equation~\ref{e1:example}.
\begin{equation}
\label{e1:example}
 \left (
  \begin{array}{c}
    0.05 \\
    0.20 \\
    0.49 \\
    0.75 \\
    0.16 \\
    0
  \end{array}
 \right )
= 
 \left (
  \begin{array}{ccc}
   0.1 & 0 & 0 \\
   0.2 & 0.1 & 0 \\
   0.5 & 0.2 & 0.2 \\
   0.3 & 0.5 & 0.5 \\
   0   & 0.1 & 0.3 \\
   0   & 0   & 0 
  \end{array}
 \right )
 \left (
  \begin{array}{c}
   0.5 \\
   1.0 \\
   0.2
  \end{array}
 \right )
\end{equation}

\subsection*{Why linear methods are useful}
Linear systems methods are a good starting
point for answering an essential scientific question:
How can we generalize from the results of measurements using
a few stimuli to predict the results we will obtain when
we measure using novel stimuli?
Linear systems methods tell us to examine homogeneity and superposition.
If these empirical properties hold in our experiment, then we
will be able to measure responses to a few stimuli
and predict responses to many other stimuli.

This is very important advice.
Quantitative scientific theories are attempts
to {\em characterize} and then {\em explain} systems
with many possible input stimuli.
Linear systems methods tell us how to organize experiments
to characterize our system:
measure the responses to a few individual stimuli, and then
measure the responses to mixtures of these stimuli.
If superposition holds, then we can obtain a good characterization
of the system we are studying.
If superposition fails,
your work will not be wasted since
you will need to explain the results of superposition experiments
to obtain a complete characterization of the measurements.

To explain a system, we need to understand the general
organizational principles concerning the system parts and how
the system works in relationship to other systems.
Achieving such an explanation is a creative act that
goes beyond simple characterization of the input and output
relationships.
But, any explanation must begin with a good characterization
of the processing the system performs.

\section{Shift-Invariant Linear Transformations}
\label{sec1:ShiftInvariance}
\subsection*{Shift-Invariant Systems:  Definition}
Since homogeneity and superposition are well satisfied
by Campbell and Gubisch's experimental data,
we can predict the result of any input stimulus
by measuring the system matrix that describes the mapping from
the input signal to the measurements at the photodetector.
But the experimental data are measurements of light that has passed
through the optical elements of the eye {\em twice},
and we want to know the transformation when we pass
through the optics {\em once}.
To correct for the effects of double passage,
we will take advantage of a
special property of optics of the eye, {\em shift-invariance}.
Shift-invariant linear systems are an important
class of linear systems, and
they have several properties that make them
simpler than general linear systems.
The following section briefly describes these properties
and how we take advantage of them.
The mathematics underlying these properties is not hard;
I sketch proofs of these properties in the Appendix.

Suppose we start to measure the system matrix for the
Campbell and Gubisch experiment by
measuring responses to different
lines near the center of the monitor.
Because the quality of the optics of our eye is fairly
uniform near the fovea,
we will find that our measurements,
and by implication the retinal images,
are nearly the same for all single-line monitor images.
The only way they will differ is that as
the position of the input translates,
the position of the output will translate by a corresponding amount.
The shape of the output, however, will not change.
An example of two measurements we might find
when we measure using two
lines on the monitor is illustrated 
in the top two rows of Figure \ref{f1:superposition}.
As we shift the input line, the measured output shifts.
This shift is a good feature for a lens to have,
because as an object's position changes,
the recorded image should remain the same (except for a shift).
When we shift the input and the form of the
output is invariant,
we call the system {\em shift-invariant}.

\subsection*{Shift-Invariant Systems:  Properties}
\paragraph{We can define the system matrix of a shift-invariant
system from the response to a single stimulus.}
Ordinarily, we need to build the system matrix
by combining the responses to many individual lines.
The system matrix of a linear shift-invariant system  
is simple to estimate since these responses are all the same
except for a shift.
Hence, if we measure a single column of the matrix, we can
fill in the rest of the matrix.
For a shift-invariant system, there is only one response
to a line.
This response is called the {\em linespread} of the system.
We can use the linespread function to fill in the entire
system matrix.

\paragraph{The response to a harmonic function at frequency $f$
is a harmonic function at the same frequency. }
Sinusoids and cosinusoids are called {\em harmonics}
or {\em harmonic functions}.
When the input to shift-invariant system is a
harmonic at frequency $f$,
the output will be a harmonic at the same frequency.
The output may be scaled in amplitude and shifted in position,
but it still will be a harmonic at the input frequency.

For example, when the input stimulus is defined at $N$
points and at these points its values are sinusoidal, $\SfNi$.
Then, the response of a shift-invariant system
will be a scaled and shifted sinusoid, $s_f \sin ( \pfiN + \phi_f )$.
There is some uncertainty concerning the output
because there two unknown values,
the scale factor, $s_f$, and phase shift, $\phi_f$.
But, for each sinusoidal input we know a lot about the output;
the output will be a sinusoid of the same frequency as the input.

We can express this same result another useful way.
Expanding the sinusoidal output using the summation rule we have
\begin{equation} 
\label{e1:r1}
s_f \sin (\pfiN + \phi_f ) = a_f \cos (\pfiN) + b_f \sin (\pfiN) .
\end{equation}
where
\begin{eqnarray}
a_f &=& s_f \sin ( \phi_f) \nonumber \\
b_f &=& s_f \cos ( \phi_f ).
\end{eqnarray}
In other words, when the input is a sinusoid at frequency $f$,
the output is the weighted
sum of a sinusoid and a cosinusoid, both at the
same frequency as the input.
In this representation, the two unknown values are the
weights of the sinusoid and the cosinusoid.

For many optical systems, such as the human eye, the relationship between
harmonic inputs and the output is even simpler.
When the input is a harmonic function at frequency $f$,
the output is a scaled copy of the function
and there is no shift in spatial phase.
For example, when the input is $\SfNi$ the output will be $s_f \SfNi$,
and only the scale factor, which depends on frequency, is unknown.
