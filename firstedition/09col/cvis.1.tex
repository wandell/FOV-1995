\chapter{Color}
\label{chapter:Color}

\section{Color Constancy:  Theory}
Edwin Land  was one of the great inventors and entrepreneurs in US history;
he created instant developing film and founded the Polaroid Corporation.
The first instant developing film made black and white
reproductions, and after a few years Land decided to create a color
version of the film.  In order to learn about color appearance, Land
returned to his laboratory to experiment with color.  He
was so surprised by his observations that he decided to write a paper
summarizing his observations.  In a paper published in the Proceedings of the
National Academy of Sciences (USA), Land startled many people by
arguing that there are only two, not three, types of cones.  He
further went on to dismiss the significance of the color-matching
experiments.  He wrote: ``We have come to the conclusion that the
classical laws of color mixing conceal great basic laws of color
vision [Proc. Nat. Acad. Sci, 1959].''  Land's sharp words, an arrow
aimed at the heart of color science, provoked heated rejoinders from
two leading scientists, Judd (1960) and Walls (1960).

What was it that Land, a brilliant man, found so objectionable about
color-matching?  It seems to me that Land's reading of the literature
led him to believe that the curves we measure in the color-matching
experiment can be used to predict color appearance.  When he set the
textbooks down and began to experiment with color, he was sorely
disappointed.  He found that the color-matching measurements do not
answer many important questions about color appearance.

Land's observation is consistent with our review of color-matching in
Chapter~\ref{chapter:wavelength}.  The results from the color-matching
experiment can be used to predict when two lights will look the same,
but they cannot be used to tell us what the two lights look like.  As
Figure~\ref{f3:albers} illustrates, the experimental results in the
color-matching experiment can be explained by the matches between the
cone photopigment absorptions at a point, while color appearance
forces us to think about the pattern of photopigment absorptions
spread across the cone mosaics.  Figure~\ref{f8:contrast} illustrates
this point again.  The two squares in the image reflect the same
amount of light to your eye.  Yet, because the squares are embedded in
different surroundings, we interpret the squares very differently,
seeing one as light and the other as dark.
\begin{figure}
\centerline{
  \psfig{figure=../09col/fig/contrast.ps,clip= ,width=5.5in}
}
\caption[Contrast illusion]{
{\em Color appearance in a region depends on the spatial pattern of
cone absorptions, not just the absorptions the absorptions within the
region.}  The two square regions are physically identical and thus
create the same local rate of photopigment absorption.  Yet, they
appear to have different lightness because of the difference in their
relative absorptions compared to nearby areas.}
\label{f8:contrast}
\end{figure}

Land's paper contained a set of interesting qualitative demonstrations
that illustrate these same points.  While the limitations of the
color-matching experiment were new to Land and the reviewers of his
paper, they were not new to most color scientists.  For example, Judd
(1940) had worked for years trying to understand these effects.  Later
in this chapter I will review work at Kodak and in academic
laboratories, contemporaneous with that of Land, that was designed to
elucidate the mechanisms of color appearance.  This episode in the
history of color science remains important, however, because it
reminds us that the phenomena of color appearance are very significant
and very compelling, enough so to motivate Edwin Land to challenge
whether the color establishment had answered the right questions.  As
to Land's additional and extraordinary claim in those papers, that
there are two not three types of photoreceptors, well, we all have off
days\footnote{ Of course, even on his off days, Land was worth a
billion dollars.}.

If the absolute rates of photopigment absorptions don't explain color
appearance, what does? The illusions in Figures~\ref{f3:albers} and
\ref{f8:contrast} both suggest that color appearance is related to the
{\em relative} cone absorption rates.  Within an image, bright objects
generate more cone absorptions than dark objects; red objects create
more $\Red$ cone absorptions and blue objects more $\Blue$ cone
absorptions.  Hence, one square in Figure~\ref{f8:contrast} appears
light because it is associated with more cone absorptions than its
neighboring region, while the other appears dark because it is
associated with fewer cone absorptions.  The relative absorption rate
is very closely connected to the idea of the stimulus contrast that
has been so important in this book.  Color appearance depends more on
the local contrast of the cone absorptions than on the absolute level
of cone absorptions.

The dependence on relative, rather than absolute, absorption rates is
a general phenomenon, not something that is restricted to a few
textbook illusions.  Consider a simple thought experiment that
illustrates the generality of the phenomenon.  Suppose you read this
book indoors.  The white part of the page reflects about 90 percent of
the light towards your eye, while the black ink reflects only about 2
percent.  Hence, if the ambient illumination inside a reading room is
100 units, the white paper reflects 90 units and the black ink 2
units.  When you take the book outside, the illumination level can be
100 times greater, or 10,000 units.  Outside the black ink reflects
200 units towards your eye, which far exceeds the level of the white
paper when you were indoors.  Yet, the ink continues to look black.
As we walk about the environment, then, we must constantly be
inferring the lightness and color of objects by comparing the spatial
pattern of cone absorptions~\footnote{This example was used by Hering
(1964).}.

This thought experiment also illustrates us that the color we perceive
informs us mainly about objects.  The neural computation of color is
structured so that objects retain their color appearance whether we
encounter them in shade or sun.  When the color appearance of an
object changes, we think that the object itself has changed.  The
defining property of an object is not the absolute amount of light it
reflects, but rather how much light it reflects relative to other
objects.  From our thought experiment, it follows that the color of an
object imaged at a point on the retina should be inferred from the
relative level of cone absorptions caused by an object.  To compute
the relative level of cone absorptions, we must take into account the
spatial pattern of cone absorptions, not just the cone absorptions at
a single point.

On this view, color appearance is a mental explanation of why an
object causes relatively more absorptions in one cone type than
another object.  The physical attribute of an object that describes
how well the object reflects light at different wavelengths is called
the object's {\em surface reflectance}.  Generally, objects that
reflect light mainly in the long-wavelength part of the spectrum
usually appear red; objects that reflect mainly short-wavelength light
usually appear blue.  Yet, as we shall explore in the next few pages,
interpreting the cone absorption rates in terms of the surface
reflectance functions is not trivial.  How the nervous system makes
this interpretation is an essential question in color appearance. A
natural place to begin our analysis of color appearance, then, is with
the question: how can the central nervous system can infer an object's
surface reflectance function from the mosaic of cone absorptions?

\subsection*{Spectral Image Formation}
To understand the process of inferring surface reflectance from the
light incident at our eyes, we must understand a little about how
images are formed.  The light incident at our corneas and absorbed by
our cones depends in part on the properties of the objects that
reflect the light and in part on the wavelength composition of the
ambient illumination.  We must understand each of these components,
and how they fit together, to see what information we might extract
from the retinal image about the surface reflectance function.  A very
simple description of the imaging process is shown in
Figure~\ref{f8:imaging}a.
\begin{figure}
\centerline{
 \psfig{figure=../09col/fig/imaging.ps ,clip= ,width=5.5in}
}
\caption[The color imaging process]{
{\em A description of spectral image formation.}  (a) Light from a
source arrives at a surface and is reflected towards an observer.  The
light at the observer is absorbed by the cones and ultimately leads to
a perception of color.  (b) The functions associated with the imaging
process include the spectral power distribution of the light source,
the surface reflectance function of the object, the result of
multiplying these two functions to create the color signal incident at
the eye, and the cone absorptions caused by the incident signal.  }
\label{f8:imaging}
\end{figure}

Ordinarily, image formation begins with a light source.  We can
describe the spectral properties of the light source in terms of the
relative amount of energy emitted at each wavelength, namely the {\em
spectral power distribution} of the light source (see
Chapter~\ref{chapter:wavelength}).  The light from the source is
either absorbed by the surface or reflected.  The fraction of the
light reflected by the surface defines the {\em surface reflectance}
function.  As a first approximation, we can calculate the light
reflected towards the eye by multiplying the spectral power
distribution and the surface reflectance function together
(Figure~\ref{f8:imaging}b).  We will call this light the {\em color
signal} because it serves as the signal that ultimately leads to the
experience of color.  The color signal leads to different amounts of
absorptions in the three cone classes, and the interpretation of these
cone absorptions by the nervous system is the basis of our color
perception. 

\begin{figure}
\centerline{
 \psfig{figure=../09col/fig/surfR.ps ,clip= ,width=5.5in}
}
\caption[Surface Reflectance Functions]{
{\em The surface reflectance function} measures the proportion of
light scattered from a surface at each wavelength.  The panels show
the surface reflectance functions of various colored papers along with
the color name associated with the paper.  Surface reflectance
correlates with the color appearance; as Newton wrote ``colors in the
object are nothing but a disposition to reflect this or that sort of
ray more copiously than the rest.''  }
\label{f8:surfR}
\end{figure}
In Chapter~\ref{chapter:wavelength} I reviewed some properties of
illuminants and sensors.  But, this is the first time we have
considered the surface reflectance function; it is worth spending time
thinking about some of the properties of how surfaces reflect light.
Figure~\ref{f8:surfR} shows the reflectance function of four matte
papers, that is papers with a smooth even surface free from shine or
highlights.  Because these curves describe the fraction of light
reflected, they range between zero and one.  While it is common to
refer to an object as having a surface reflectance function, as I have
just done, in a certain sense, the notion of a surface reflectance
function is a ruse.  If you look about the room you are in, you will
probably see some surfaces that are shiny or glossy.  As you move
around these surfaces, changing the geometrical relationship between
yourself, the lighting, and the surface, the light reflected to your
eye changes considerably.  Hence, the tendency of the surface to
reflect light towards your eye does not depend only on the surface;
the light scattered to your eye also depends on the viewing geometry,
too\footnote{Also, some types of materials fluoresce, which is to say
they absorb light at one wavelength and emit light at another (longer)
wavelength.  This is also a linear process, but too complex to
consider in this discussion.}.

The surface reflectance dependence on viewing geometry because the
reflected light arises from several different physical processes that
occur when light is incident at the surface.  Each of these processes
contributes simultaneously to the light scattered from a surface, and
each has its own unique properties. The full model describing
reflectance appears to be be complex; but, Shafer (1985) has created a
simple approximation of the reflection process, called the {\em
dichromatic reflection model}, that captures several important
features of surface reflectance.  Figure~\ref{f8:refProcess}a sketches
the model, which applies to a broad collection of materials called
{\em dielectric} surfaces~\footnote{ Dielectrics are non-conducting
materials.}  (Klinker et al., 1988; Shafer, 1985; Nayar, 1993, Wolff,
1994).

According to the dichromatic reflection model, dielectric material
consists of a clear substrate with embedded colorant particles.  One
way light is scattered from the surface is by a mirror-like reflection
at the interface of the surface.  This process is called {\em
interface} reflections.  A second scattering process takes place when
the rays enter the material.  These rays are reflected randomly
between the colorant particles.  A fraction of the incident light is
absorbed by the material, heating it up, and part of the light
emerges.  This process is called {\em body} reflection.

The spatial distributions of light scattered by these two mechanisms
are quite different (Figure~\ref{f8:refProcess}b).  Light scattered by
interface reflection is quite restricted in angle, much as a mirror
reflects incident rays.  Conversely, the light scattered by body
reflection emerges equally in all directions.  When a surface has no
interface reflections, but only body reflections, it is called a {\em
matte} or {\em Lambertian} surface.  Interface reflection is commonly
called {\em specular} reflection and is the reason why some objects
appear {\em glossy}.
\begin{figure}
\centerline{
  \psfig{figure=../09col/fig/refProcess.ps,clip= ,width=5.5in}
}
\caption[Physical Mechanisms of Reflectance]{
{\em The dichromatic reflection model of surface reflectance in
inhomogeneous materials. } (a) Light is scattered from a surface by
two different mechanisms.  Some incident light is reflected at the
interface (interface reflection).  Other light enters the material,
interacts with the embedded particles, and then emerges as reflected
light (body reflection).  (b) Rays of light reflected by interface
reflections is likely to be concentrated in one direction.  Rays of
light reflected by body reflection are reflected with nearly equal
likelihood in many different directions.  Because interface
reflections are concentrated in certain directions, light reflected by
this process can be much more intense than light reflected by body
reflection. }
\label{f8:refProcess}
\end{figure}

The different geometrical distribution in how body and interface
reflections are reflected is the reason why specular highlights on a
surface appear much brighter than diffuse reflection.  Nearly all of
the specular scattering is confined to a small angle; the body
reflection is divided among many directions.  The interface
reflections provide a strong signal, but because they can only be seen
from certain angles they are not a reliable source of information.  As
the object and observer change their geometric relationship the
specular highlight moves along the surface of the object, or it may
disappear altogether.

For many types of materials, interface reflection is not selective for
wavelength.  The spectral power distribution of the light scattered at
the interface is the same as the spectral power distribution of the
incident light.  This is the reason specular highlights take on the
color of the illumination source.  Body reflection, on the other hand,
does not return all wavelengths uniformly.  The particles in the
medium absorb light selectively, and it is this property of the
material that distinguishes objects in terms of their color
appearance.  Ordinarily, when people refer to the surface reflectance
of an object, they mean to refer to the body reflection of the object.

\begin{figure}
\centerline{
 \psfig{figure=../09col/fig/colorSig.ps,clip= ,width=5.5in}
}
\caption[The Color Signal Under Two Illuminants]{
{\em The light reflected from objects changes as the illuminant
changes.}  (a) The shaded panel on the left shows the spectral power
distributions of a light source similar to a tungsten bulb.  The
three graphs on the right show the light reflected from the red, green
and yellow papers in Figure~\ref{f8:surfR} when illuminated by this
source. The bar plots above the graphs show the three cone absorption
rates caused by the color signal.  (b) When the light source is
similar to the blue sky, as in the shaded panel on the left, the light
reflected from the same papers is quite different.  These change
considerably, too, and are thus an unreliable cue to the surface
reflectance of the object.  }
\label{f8:colorSig}
\end{figure}

We can describe the reflection of light by a matte surface with a
simple mathematical formula.  Suppose that the illuminant spectral
power distribution is $\ill{\lambda}$.  We suppose that the body
reflectance is $\surf{\lambda}$.  Then the color signal, that is the
light arriving at the eye, is
\[
\colsig{\lambda} = \surf{\lambda} \ill{\lambda} .
\]
Figure~\ref{f8:colorSig} shows several examples of the light reflected
from matte surfaces.  The shaded graph in Figure~\ref{f8:colorSig}a is
the spectral power distribution of an illuminant similar to a tungsten
bulb.  The other panels show the spectral power distribution of
light that would be reflected from the red, green and yellow papers in
Figure~\ref{f8:surfR}.  The shaded graph in Figure~\ref{f8:colorSig}b
shows the spectral power distribution similar to blue sky illumination
and the light reflected from the same three papers.  Plainly, the
spectral composition of the reflected light changes when the
illuminant changes.

We can calculate the cone absorption rates caused by each of these
color signals (see Chapter~\ref{chapter:wavelength}).  These rates are
shown in the bar plots inset within the individual graphs of reflected
light.  By comparing the insets in the top and bottom, you can see
that the cone absorption rates from each surface changes dramatically
when the illumination changes.  This observation defines a central
problem in understanding color appearance.  Color is a property of
objects; but, the reflected light, and thus the cone absorptions,
varies with the illumination.  If color must describe a property of an
object, the nervous system must {\em interpret} the mosaic of
photopigment absorptions and estimate something about the surface
reflectance function.  This is an estimation problem.  How can the
nervous system use the information in the cone absorptions to infer
the surface reflectance function?

\subsection*{Surface reflectance estimation}
There are some very strong limitations on what we can achieve when we
set out to estimate surface reflectance from cone absorptions. First,
notice that the color signal depends on two spectral functions that
are continuous functions of wavelength: the spectral power
distribution of the ambient illumination and the surface reflectance
function.  The light incident at the eye is the product of these two
functions.  So, any illuminant and surface combination that produces
this same light will be indistinguishable to the eye.

One easy mathematical way to see why this is so is to consider the
color signal.  Recall that the color signal is equal to the product of
the illuminant spectral power distribution $\ill{\lambda}$ and the
surface reflectance function, $\surf{\lambda}$,
\[
\colsig{\lambda} = \surf{\lambda} \ill{\lambda} .
\]
Suppose we replace the illuminant with a new illuminant, $f(\lambda)
\ill{\lambda}$ and all of the surfaces with new functions
$\surf{\lambda} / f(\lambda)$.  This change has no effect on the color
signal,
\[
\colsig{\lambda} = \surf{\lambda} / f(\lambda) f(\lambda) \ill{\lambda}
= \surf{\lambda} \ill{\lambda}
\]
and thus no effect on the photopigment absorption rates.  Hence, there
is no way the visual system can discriminate between these two
illuminant and surface pairs.

Now, consider a second limitation to the estimation problem.  The
visual system does not measure the spectral power distribution
directly.  Rather, the visual system only encodes the absorption rates
of the three different cones.  Hence, the nervous system cannot be
certain which of the many metameric spectral power distributions is
responsible for causing the observed cone absorption rates (see
Chapter~\ref{chapter:wavelength} for a definition of metameric).  This
creates even more uncertainty for the estimation problem.

In the introduction to this part of the book, I quoted Helmholtz'
suggestion: the visual system imagines those objects being present
that could give rise to the retinal image.  We now find that the
difficulty we have in following this advice is not that are no
solutions, but rather that there are too many.  We encode so little
about the color signal that many different objects could all have
given rise to the retinal image.

Which of the many possible solutions should we select?  The general
strategy we should adopt is straightforward: Pick the most likely one.
Will this be a helpful estimate, or are there so many likely signals
that encoding the most likely one is hardly better than guessing with
no information?

Perhaps the most important point that we have learned from color
constancy calculations over the last ten years is this: the set of
surface and illuminant functions we encounter is not so diverse as to
make estimation from the cone catches useless.  Some surface
reflectance functions and some illuminants are much more likely than
others, even with only three types of cones, it is possible to make
educated guesses that do more good than harm.

The surface reflectance estimation algorithms we will review are all
based on this principle.  They differ only in the set of assumptions
they make concerning what the observer knows and what we mean by most
likely.  I review them now because some of the tools are useful and
interesting, and some of the summaries of the data are very helpful in
practical calculations and experiments.

\subsection*{Linear Models}
To estimate which lights and surfaces are more probable, we need to do
two things.  First, we need to measure the spectral data from lights
and surfaces.  Second, we need a way to represent the likelihood of
observing different surface and illuminant functions.

Since the early part of the 1980s, {\em linear models} of surface and
illuminant functions have been used widely to represent our best guess
about the most likely surface and illuminant functions.  A linear
model of a set of spectral functions, such as surface reflectances, is
a method of efficiently approximating the measurements.  There are
several ways to build linear models, including {\em principal
components analysis}, {\em centroid analysis}, or {\em one mode
analysis}.  These methods have much in common, but they differ
slightly in their linear model formulation and error measures (Cohen,
1970; Judd et al., 1964; Maloney, 1986; Marimont and Wandell, 1993).

As an example of how to build a linear model, we will review a classic
paper by Judd, Macadam and Wyszecki (1964).  These authors built a
linear model of an important source of illumination, daylight spectral
power distributions.  They collected more than six hundred
measurements of the spectral power distribution of daylights at
different times of day and under different weather conditions and on
different continents.  To measure the spectral power distribution of
daylight we place an object with a known reflectance outside.  It is
common to use blocks of pressed magnesium oxide as a standard object
because such blocks reflect light of all wavelengths nearly equally.
Moreover, the material is essentially a pure diffuser: a quantum of
light incident on the surface from any angle is reflected back with
equal probability in all other directions above the surface.

Since they were interested in the relative spectral composition, not
the absolute level, Judd et al. normalized their measurements so that
they were all equal to the value 100 at 560nm.  Their main interest
was in the wavelength regime visible to the human eye, so they made
measurements roughly from 400nm to 700nm.  Their measurements were
spaced every 10 nm.  Hence, they could represent each daylight
measurement by a set of thirty-one numbers.  Three example daylight
spectral power distributions, normalized to coincide at 560nm, are
shown in Figure \ref{f8:daylights}.
\begin{figure}
\centerline{
 \psfig{figure=../09col/fig/daylights.ps,clip= ,width=5.5in}
}
\caption[Examples of Daylights]{
{\em The relative spectral power distribution of three typical
daylights.}  The curves drawn here are typical daylights measured by
Judd, Macadam and Wyszecki (1964).  The curves are normalized to
coincide at 560nm (Source:  Judd et al., 1964).
\comment{
Basically a Judd et al. figure.  Get permission?
}
}
\label{f8:daylights}
\end{figure}

The data plotted in Figure~\ref{f8:daylights} show that measured
daylight relative spectral power distributions can differ depending on
the time of day and the weather conditions.  But, after examining many
daylight functions, Judd et al. found that the curves do not vary
wildly and unpredictably; the data are fairly regular.  Judd et
al. captured the regularities in the data by building a linear model
of the observed spectral power distributions.  They designed their
linear model, a {\em principal components model}, using the following
logic.

First, they decided that they wanted to approximate their observations
in the {\em squared-error sense}.  That is, suppose $\ill{\lambda}$ is
a measurement, and $\illhat{\lambda}$ is the linear model estimate of
the measurement.  Then, they decided to select the approximation in
order to minimize the {\em squared error}
\[
\sum_{\lambda} ( \ill{\lambda} - \illhat{\lambda} )^2 .
\]
When we consider the collection of observations as a whole, the
function that approximates the entire data set with the smallest
squared error is the mean.  The mean observation from Judd et al.'s
data set, $\illi{0}{\lambda}$, is the bold curve in Figure
\ref{f8:linearModDay}.

Once we know the mean, we need only to approximate the difference
between the mean and each individual measurement.  We build the linear
model to explain these differences as follows.  First, we select a
fixed set of {\em basis functions}.  Basis functions, like the mean,
are descriptions of the measurements.  We approximate a measurement's
difference from the mean as the weighted sum of the basis functions.
For example, suppose $\Delta \illi{j}{\lambda}$ is the difference
between the $j^{th}$ daylight measurement and the mean daylight.
Further, suppose we select a set of $N$ basis functions,
$\illBasisi{i}{\lambda}$.  Then, we approximate the differences from
the mean as the weighted sum of these basis functions, namely
\begin{equation}
\label{e8:linMod}
\Delta \illi{j}{\lambda}
  \approx \sum_{i=1}^{i=N} \illCoefi{i} \illBasisi{i}{\lambda} .
\end{equation}
The basis functions are chosen to make the sum of the squared errors
between the {\em collection} of measurements and their approximations
as small as possible.  The number of basis functions, $N$, is called
the {\em dimension} of the linear model.  The values
$\illCoefi{i}$ are called the linear model {\em weights}, or {\em
coefficients}.  They are chosen to make the squared error between an
{\em individual} measurement and its approximation as small as
possible.  They serve to describe the properties of the specific
measurement.

As the dimension of the linear model increases, the precision of the
linear model approximation improves.  The dimension one chooses for an
application depends on the required precision~\footnote{The basis
functions that minimize the squared error can be found in several
ways, most of which are explained in widely available statistical
packages.  If the data are in the columns of a matrix, one can apply
the singular value decomposition to the data matrix and use the left
singular vectors.  Equivalently, one can find the eigenvectors of the
covariance matrix of the data.}.

Judd et al. found an excellent approximation of the daylight
measurements using the mean and two basis functions.  The linear model
representation expresses the measurements efficiently.  The mean and
two basis functions are fixed, side conditions of the model.  Their
linear model approximation of each measurement uses only two weights.
The empirical results have been confirmed by other investigators and
the results have been adopted by the international color standards
organization to create a model of daylights (Dixon, 1978; Sastri,
1965).  The mean and two basis terms from the international standard
are plotted in Figure \ref{f8:linearModDay}.
\begin{figure}
\centerline{
 \psfig{figure=../09col/fig/linearModDay.ps,clip= ,width=5.5in}
}
\caption[Linear Model Curves for Daylight] {
{\em A linear model for daylight spectral power distributions.}  The
curve labeled mean is the mean spectral power distribution of a set of
daylights whose spectral power distributions were normalized to a
value of 100 at 560nm.  The curves labeled 1st and 2nd show the two
basis curves used to define a linear model of daylights.  By adding
together the mean and weighted sums of the two basis functions, one
can generate examples of typical relative spectral power distributions
of daylight. (Source: Judd et al., 1964).
}
\label{f8:linearModDay}
\end{figure}

Because daylights vary in their absolute spectral power distributions,
not just their relative distributions, we should
extend Judd et al.'s linear model to a three-dimensional
linear model that includes absolute intensity.
A three dimensional linear model
we might use consists of the mean and the two
derived curves.  In this case the three-dimensional linear model
approximation becomes\footnote{It is possible to improve on this model
slightly, but as a practical matter these three curves do quite well
as basis functions.} 
\begin{equation}
\label{e8:dayMod}
\ill{\lambda} = \sum_{i=1}^{3} \illCoefi{i} \illBasisi{i}{\lambda} .
\end{equation}
We can express the linear model in Equation \ref{e8:dayMod} as a
matrix equation, $\illvec = \illBasis \illCoef$ in which $\illvec$ is
a vector representing the illuminant spectral power distribution, the
three columns of $\illBasis$ contain the basis functions
$\illBasisi{i}{\lambda}$, and $\illCoef$ is a three dimensional vector
containing the linear model coefficients $\illCoefi{i}$.

We can see why linear models are efficient by writing Equation
\ref{e8:linMod} as a matrix tableau.
\begin{equation}
\left (
 \begin{array}{ccc}
    & . & \\
    & . & \\
    & . & \\
   & e(\lambda) & \\
    & . & \\
    & . & \\
    & . & \\
 \end{array} \right ) =
\left (
 \begin{array}{ccc}
    .   & . & . \\
    .   & . & . \\
    .   & . & . \\
   \illBasisi{1}{\lambda} & \illBasisi{2}{\lambda} & \illBasisi{3}{\lambda} \\
    .   & . & . \\
    .   & . & . \\
    .   & . & . \\
 \end{array}
\right )
\left (
 \begin{array}{ccc}
   & \illCoefi{1} & \\
   & \illCoefi{2} & \\
   & \illCoefi{3} & \\
 \end{array}
\right ) \nonumber
\end{equation}
The single spectral power distribution, the vector on the left,
consists of measurements at many different wavelengths.  The linear
model summarizes each measurement as the weighted sum of the basis
functions, which are the same for all measurements, and a few weights,
$\illCoef$, which are unique to each measurement.  The linear model is
efficient because we represent each additional measurement using
only three weights, $\illCoef$, rather than the full spectral power
distribution. 

\subsection*{Simple Illuminant Estimation}
Efficiency is useful; but, if efficiency were our only objective we
could find more efficient algorithms.  The linear models are also
important because they lead to very simple estimation algorithms.  As
an example, consider how we might use a device with three color
sensors, like the eye, to estimate the spectral power distribution of
daylight.  Such a device is vastly simpler than the spectroradiometer
Judd et al. needed to make many measurements of the light.

Suppose we have a device with three color sensors, whose spectral
responsivities are, say, $\recSens{i}(\lambda), i = 1 \ldots 3$.  The
three sensor responses will be
\begin{eqnarray}
\label{e8:recIll}
\recRespi{1} & = & \sum_\lambda \recSens{1}(\lambda) \ill{\lambda} \nonumber \\
\recRespi{2} & = & \sum_\lambda \recSens{2}(\lambda) \ill{\lambda} \nonumber \\
\recRespi{3} & = & \sum_\lambda \recSens{3}(\lambda) \ill{\lambda}  . 
\end{eqnarray}
We can group these three linear equations into a single matrix
equation
\begin{equation}
\label{e8:dayEst1}
\recResp = \sensorMat \illvec
\end{equation}
where the column vector
$\recResp$ contains the sensor responses, 
the rows of the matrix $\sensorMat$ are the sensor spectral responsivities,
and $\illvec$ is the illuminant spectral power distribution.

Before the Judd et al. study, one might have thought that three sensor
responses are insufficient to estimate the illumination.  But, from
their data we have learned that we can approximate $\illvec$ with a
three-dimensional linear model, $\illvec \approx \illBasis \illCoef$.
This reduces the equation to
\begin{equation}
\label{e8:dayEst2}
\recResp \approx (\sensorMat \illBasis) \illCoef .
\end{equation}
The matrix $\sensorMat \illBasis$ is $3 \times 3$, and its entries are
all known.  The sensor responses, $\recResp$, are also known.  The
only unknown is $\illCoef$.  Hence, we can estimate, $\illCoef$, and
use these weights to calculate the spectral power distribution,
$\illBasis \illCoef$.

This calculation illustrates two aspects of the role of linear models.
First, linear models represent a priori knowledge about the likely set
of inputs.  Using this information permits us to convert
underdetermined linear equations (Equation \ref{e8:dayEst1}) into
equations we can solve (Equation \ref{e8:dayEst2}).  Linear models are
a blunt but useful tool for representing probabilities.  Using linear
models, it becomes possible to use measurements from only three
color sensors to estimate the full relative spectral power
distribution of daylight illumination.

Second, linear models work smoothly with the imaging equations.  Since
the imaging equations are linear, the estimation methods remain linear
and simple.

\subsection*{Surface Reflectance Models}
The daylights are an important class of signals for vision.  For most
of the history of the earth, daylight was the only important light
source.  There is no similar set of surface reflectance functions.  I
was reminded by this once by the brilliant color scientist,
G. Wyszecki.  When I was just beginning my study of these issues, I
asked him why he had not undertaken a study of surfaces similar to
daylight study.  He shrugged at me and answered, ``How do you sample
the universe?''

Wyszecki was right, of course.  The daylight measurement study could
begin and end in a single paper.  There is no specific set of surfaces
that of equal importance to the daylights, so we have no way to
perform a similar analysis on surfaces.  But, there are two related
questions we can make some progress on.  First, we can ask what the
properties are of certain collections of surfaces that are of specific
interest to us, say for practical applications.  Second, we can ask
what the visual system, with only three types of cones, can infer
about surfaces.

Over the years linear models for special sets of materials can be used
in many applications.  Printer and scanner manufacturers may be
interested in the reflectance functions of inks.  Computer graphics
programmers may be interested in the reflectance factors of geological
materials, or tea pots.  Color scientists have repeatedly measured the
reflectance functions of standard color samples used in industry, such
as the Munsell chips.  These cases can be of practical value and
interest in printing and scanning applications (e.g. Marimont and
Wandell, 1992; Farrell et al., 1993; Vrhel and Trussell, 1994; Drew
and Funt, 1992).
To discover the regularities in surface functions, then, we should
measure the body reflection terms.  From the studies that have taken
place over the last several years, it has become increasingly clear
that in the visible wavelength region the surface reflectance
functions tend to be quite smooth, and thus exhibit a great deal of
regularity.  Hence, linear models serve to describe the reflectance
functions quite well.

For example, Cohen (1970), Maloney (1986) and Parkinnen (1990) studied
the reflectance properties of a variety of surfaces including special
samples and some natural objects.  For each of the sets studied by
these authors the data can be modeled nearly perfectly using a linear
model with less than six dimensions.  Excellent approximations, though
not quite perfect, can be obtained by three-dimensional
approximations.

As an example, I have built a linear model to approximate a small
collection surface reflectance functions for a color target, the {\em
Macbeth ColorChecker}, that is used widely in industrial
applications.  The target consists of 24 square patches laid out in a
4 by 6 array.  The surfaces in this target were selected to have
reflectance functions similar to a range of naturally occurring
surfaces.  They include reflectances similar to human skin, flora, and
other materials (McCamy, 1976).
\begin{figure}
\centerline{
 \psfig{figure=../09col/fig/macbethApprox.ps,clip= ,width=5.5in}
}
\caption[Macbeth approximations]{
{\em A linear model to approximate the surface reflectances in the
Macbeth ColorChecker.}  The panels in each row of this figure show the
surface reflectance functions of six colored surfaces (dashed line)
and the approximation to these functions using a linear model (solid
lines).  The approximations using linear models with three (a), two
(b) and one (c) dimension respectively are shown.  }
\label{f8:macbethApprox}
\end{figure}
To create the linear model, I measured the surface reflectance
functions of these patches with a spectral radiometer in my laboratory.
The original data set, then, consisted of measurements from 370nm to
730nm in 1 nm steps for each of the 24 patches.  Then, using
conventional statistical packages, I calculated a three-dimensional
linear model to fit all of these surface reflectance functions.  The
linear model basis functions, $\surBasisi{i}{\lambda}$, were selected
to minimize the squared error~\footnote{ I calculated the singular
value decomposition of the matrix whose columns consist of the surface
reflectance vectors.  I used the left singular vectors as the basis
functions.}
\begin{equation}
\label{e8:surModel}
\left (\surf{\lambda} - \sum_{i=1}^{N} \surCoefi{i} \surBasisi{i}{\lambda} \right ) ^2 .
\end{equation}

The values $\surCoefi{i}$ are called the {\em surface coefficients},
and we will represent them as a vector, $\surCoef = (\surCoefi{1},
\ldots , \surCoefi{N})$.  There are fewer surface coefficients than
data measurements.  If we create a matrix whose columns are are the
basis functions, $\surBasis$, then we can express the linear model
approximation as $\surBasis \surCoef$.

The dashed lines in Figure~\ref{f8:macbethApprox} show the reflectance
functions of six of the twenty-four surfaces.  The smooth curves
within each row of the figure contain the approximations using linear
models with different dimensionality.  The bottom row shows a
one-dimensional linear model; in this case the approximations are
scaled copies of one another.  As we increase the dimensionality of
the linear model the approximations become very similar to the
originals.  The 3-dimensional model the approximations are quite close
to the true functions.

The low-dimensional linear model approximates these surface
reflectance functions because the functions vary smoothly as a
function of wavelength.  The linear model consists of a few, slowly
varying basis functions shown in Figure~\ref{f8:macbethBasis}.  The
first basis function captures the light-dark variation of the
surfaces.  The second basis function captures a red-green variation,
and the third a blue-yellow variation.

\begin{figure}
\centerline{
 \psfig{figure=../09col/fig/macbethBasis.ps,clip= ,width=5.5in}
}
\caption[Macbeth Basis Functions]{
{\em Basis functions of the linear model for the Macbeth
ColorChecker.}  The surface reflectance functions in the collection
vary smoothly with wavelength, as do the basis functions.  The first
basis function is all positive and explains the most variance in the
surface reflectance functions.  The basis functions are ordered in
terms of their relative significance for reducing the error in the
linear model approximation to the surfaces. }
\label{f8:macbethBasis}
\end{figure}

\begin{figure}
\centerline{
 \psfig{figure=../09col/fig/macbethRender.ps,clip= ,width=5.5in}
}
\caption[Macbeth Color Checker Approximations]{
{\em Color renderings of the linear model approximations to the
Macbeth ColorChecker.}  The linear model approximations are shown
rendered under a blue sky illumination. The dimension of the linear
model approximation is shown above each image.  The one-dimensional
approximation the surfaces appear achromatic, varying only in
lightness.  For this illuminant, and using three or more dimensions in
the linear model, the rendering is visually indistinguishable from a
complete rendering of the surfaces. }
\label{f8:macbethColor}
\end{figure}

Although the approximations are quite good, there are still
differences between the surface reflectance functions and the three
dimensional linear model.  We might ask whether these differences are
visually salient.  This question is answered by the renderings of
these surface approximations in Figure \ref{f8:macbethColor},.  The
one-dimensional model looks like a collection of surfaces in various
shades of gray.  For the blue sky illumination used in the rendering,
linear models with three or more dimensions are visually
indistinguishable from a rendering using the complete set of data.

\subsection*{Sensor-based error measures. }
I have described linear models that minimize the squared error between
the approximation and the original spectral function.  As the last
analysis showed, however, when we choose linear models to minimize the
spectral error we are not always certain whether we have done a good
job in minimizing the visual error.  In some applications, the
spectral error may not be the objective function that we care most
about minimizing.  When the final consumer of the image is a human, we
may only need to capture that part of the reflectance function that is
seen by the sensors.

If we are modeling reflectance functions for a computer graphics
application, for example, there is no point in modeling the
reflectance function at 300nm since the human visual system cannot
sense light in that part of the spectrum anyway.  For these
applications, we should be careful to model accurately those parts of
the function that are most significant for vision.  In these cases,
one should select a linear model of the surface reflectance by
minimizing a different error measure, one that takes into account the
selectivity of the eye.

Marimont and Wandell (1992) describe how to create linear models that
are appropriate for the eye.  They consider how to define linear
models that minimize the root mean squared error in the photopigment
absorption rates, rather than the root mean squared error of the
spectral curves.  Their method is called {\em one-mode analysis}.  For
many applications, the error measure minimized by one-mode analysis is
superior to root mean squared error of the spectral curves.

\subsection*{Surface and Illuminant Estimation Algorithms}
There is much regularity in daylight and surface functions; so, it
makes sense to evaluate how well we estimate spectral functions from
sensor responses.  Estimation algorithms rely on two essential
components.

First, we need a method of representing our knowledge about the likely
surface and illuminant functions.  For example, linear models can be
used encode our a priori knowledge~\footnote{More sophisticated
methods are based on using Bayesian estimation as part of the
calculation.  For example, see Freeman and Brainard (1993) and D'Zmura
and Iverson (1993).}  Second, all modern estimation methods assume
that the illumination varies either slowly or not at all across the
image.  This assumption is important because it means that the
illumination adds very few new parameters to estimate.

Consider an example of an image with p points.  We expect to obtain 3
cone absorption rates at each image point, so there are $3p$
measurements.  If we can use a surface 3 dimensional model for the
surfaces, then there are $3p$ unknown surface coefficients.  And,
there is a linear relationship between the measurements and the
unknown quantities.  If the illuminant is known, then the problem is
straightforward to solve.

The additional unknown illuminant parameters make the problem a
challenge.  If the illuminant can vary from point to point, there will
be 3p unknown parameters and the mismatch between known and unknown
parameters will be very great.  But, if the illuminant is constant
across the image, we only have 3 additional parameters.  In this case,
by making some modest assumptions about the image, we can find ways to
infer these three parameters and then proceed to estimate the surface
parameters.

Modern estimation algorithms work by find a method to overcome the
mismatch between the measurements and the unknowns.  We can divide
existing estimation algorithms into two groups. The majority of
estimation algorithms infer the illumination parameters by making one
additional assumption about the image contents.  For example, suppose
we know the reflectance function of one surface.  Then, we can use the
sensor responses from that surface to measure the illuminant
parameters.  Knowing the reflectance function of one surface in the
image compensates for the three unknown illuminant parameters.
There are several implementations of this principle.  The most
important is the assumption that the average of all the surfaces in
the image is gray, which is called the {\em gray-world} assumption
(Buchsbaum, 1980; Land, 1988).  Other algorithms are based on the
assumption that the brightest surface in the image is a uniform,
perfect reflector (McCann, et al. 1977).  An interesting variant on both of
these assumptions is the idea that we can identify specular or glossy
surfaces in the image.  Since specularities reflect the illumination
directly, often without altering the illuminant's spectral power
distribution, the sensor responses to glossy surfaces provide
information about the illuminant (Lee 1986; D'Zmura and Lennie, 1986;
Tominaga and Wandell, 1989, 1990).

A second group of estimation algorithms compensates for the mismatch
in measurements and parameters by suggesting ways to acquire more
data.  For example, Maloney and I showed that if one adds a fourth
sensor (at three spatial locations), one can also estimate the surface
and illuminants.  D'Zmura and Iverson (1993ab) explored an interesting
variant of this idea.  They asked what information is available if we
observe the same surface under several illuminants.  Changing the
illumination on a surface is conceptually equivalent to seeing the
surfaces with additional sensors.  Pooling information about the same
object seen under different illuminants, is much like acquiring
additional information from extra sensors (e.g., see Wandell, 1987).

\subsection*{Illuminant correction:  An example calculation}
Before returning to experimental measurements of color appearance,
let's perform an example calculation that is of some practical
interest as well as of some interest in understanding how the visual
system might compensate for illumination changes.  By working this
example, we will start to consider what neural operations might permit
the visual pathways to compensate for changes in the ambient
illumination.

First, let's write down the general expression that shows how the
surface and illuminant functions combine to yield the cone absorption
rates.  The three equations for the three cone types, $\Red$,
$\Green$, and $\Blue$, are
\begin{eqnarray}
\label{e8:recSur}
\recRespi{1} & = & 
   \sum_{\lambda} \recSens{1}(\lambda) E(\lambda) S(\lambda) \nonumber \\
\recRespi{2} & = & 
   \sum_{\lambda} \recSens{2}(\lambda) E(\lambda) S(\lambda) \nonumber \\
r_3 & = & 
   \sum_{\lambda} \recSens{3}(\lambda) E(\lambda) S(\lambda) .
\end{eqnarray}
Next, we replace the illuminant and surface functions in
Equation~\ref{e8:recSur} with their linear model approximations. This
yields a new relationship between the coefficients of the surface
reflectance linear model, $\surCoef$, and the three-dimensional vector
of cone absorptions, $\recResp$,
\begin{equation}
\label{e8:est1}
\recResp \approx \illMat{e} \surCoef .
\end{equation}
We call the matrix $\illMat{e}$ that relates these two vectors the
{\em lighting} matrix.  The entries of this matrix depend upon the
illumination, $\illvec$.  The $ij^{th}$ entry of the lighting matrix
is
\begin{equation}
\label{e8:lmatrix}
\sum_{\lambda} 
 \left (
  \sum_k \illCoefi{k} \illBasisi{k}{\lambda} \right )
       \recSens{i}(\lambda) 
       \surBasisi{j}{\lambda} .
\end{equation}

We can compute two lighting matrices (Equation~\ref{e8:est1}) from the
spectral curves we have been using as examples.  For one lighting
matrix I used the mean daylight spectral power distribution, and for
the other I used the spectral power distribution of a tungsten
bulb.  I used the linear model of the Macbeth ColorChecker for the
surface basis functions and the Stockman and MacLeod cone absorption
functions (see the appendix to Chapter~\ref{chapter:wavelength}).  The
lighting matrix for the blue sky illumination is
\begin{equation}
 \left (
 \begin{array}{ccc}
  591.48&  223.05 & -643.05 \\
  477.62 & 376.93 & -564.48 \\
  267.61 & 487.46 &  350.39 \\
 \end{array}
 \right ) \nonumber
\end{equation}
and the lighting matrix for the tungsten bulb is
\begin{equation}
 \left (
 \begin{array}{ccc}
  593.45 & 168.37 & -646.71 \\
  445.79 & 312.79 & -564.33 \\
  152.46 & 278.51 &  185.47 \\
 \end{array}
 \right ) . \nonumber
\end{equation}
Notice that the largest differences between the matrices are in the
third row.  This is the row that describes the effect of each surface
coefficient on the $\Blue$ cone absorptions.  The blue sky lighting
matrix contains much larger values than matrix for the tungsten
bulb.  This makes sense because that is the region of the spectrum
where these two illuminant spectral power distributions differ the
most (see Figure~\ref{f8:colorSig}).

Imagine, now the following way in which the visual system might
compensate for illumination changes.  Suppose that the cortical
analysis of color is based upon the assumption that the illumination
is always that of a blue sky.  When the illumination is different from
the blue sky, the retina must try to provide a neural signal that is
similar to the one that would have been observed under a blue sky.
What computation does the retina need to perform in order to transform
the cone absorptions obtained under the tungsten bulb illuminant
into the cone absorption that would have occurred under the blue sky?  

The cone absorptions from a single surface under the two illuminants
can be written as
\begin{eqnarray}
\recResp & \approx & \illMat{e} \surCoef, \mbox{ and } \nonumber \\
\recResp^\prime & \approx &  \illMat{e'} \surCoef \nonumber .
\end{eqnarray}
By inverting the lighting matrices and recombining terms, we find that
the cone absorptions under the two illuminants should be related
linearly as,
\begin{equation}
\label{e8:est3}
\recResp \approx \illMat{e} \illMatinv{e'} \recResp^\prime .
\end{equation}
Hence, we can transform the cone absorptions from a surface illuminated by
the tungsten bulb into the cone absorptions of the same surface
illuminated by the blue sky by the following:
\begin{equation}
\label{e8:est4}
\recResp  = 
\left (
 \begin{array}{ccc}
    0.8119 &   0.2271  &  0.0550 \\
   -0.0803 &   1.1344  &  0.1282 \\
    0.0429 &  -0.0755  &  1.8091 \\
 \end{array}
\right ) \recResp^\prime.
\end{equation}
The retina can compensate for the illumination change by linearly
transforming the observed cone absorptions, $\recResp^\prime$, into a
new signal, $\recResp$.

Now, what does it mean for the retina to compute a linear
transformation?  
The linear transformation consists of a simple series of
multiplications and additions.
For example, consider the third row of the matrix in
Equation~\ref{e8:est4}.  
This row defines how the new $\Blue$ cone absorptions should be
computed from the observed absorptions.
When we write this transformation as a single linear equation we
see that the observed and transformed signals are related as
\begin{equation}
\Blue = .0429~\Red^\prime + -0.0755~\Green^\prime + 1.8091~\Blue^\prime .
\end{equation}
The transformed $\Blue$ cone absorption is mainly a scaled version of
the observed absorptions.  Because the tungsten bulb emits much
less energy in the short-wavelength part of the spectrum, the scale
factor is larger than one.  In addition, to be absolutely precise, we
should add in a small amount of the observed $\Red$ cone signal and
subtract out a small amount of the observed $\Green$ cone signal.
But, these contributions are relatively small compared to the
contribution from the $\Blue$ cones.  In general, for each of the cone
types, the largest contributions to the transformed signal are scaled
copies of the same signal type.

In this matrix, and in many practical examples, the only additive term
that is not negligible is the contribution of the $\Green$ cone
response to the transformed $\Red$ signal.  As a rough rule, because
the diagonal terms are much larger than the off-diagonal terms, we can
obtain good first order approximation to the proper transformation by
simply scaling the the observed cone absorptions (e.g., Forsythe et al., 1994).

Compensating for the illumination change by a purely diagonal scaling
of the cone absorptions is called {\em von Kries Coefficient Law}.
The correction is not as precise as the best linear correction, but it
frequently provides a good approximation.  And, as we shall see in the
next section, the von Kries Coefficient Law describes certain aspects of
human color appearance measurements as well.

