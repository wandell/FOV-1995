
\chapter{Classification}
\label{chapter:classification}
Many visual judgments are classifications: an edge is present, or not;
a part is defective, or not; a tumor is present, or not.  The
threshold and discrimination performances reviewed in
Chapter~\ref{chapter:space} are classifications as well: I saw it, or
not; the stimulus was this one, or that.  This appendix explains some
of the basic concepts in classification theory and their application
to understanding vision.

We will explore the issues in classification by analyzing some simple
behavioral decisions, the kind that take place in many vision
experiments.  Suppose that during an experimental trial, the observer
must decide which of two visual stimuli, $\stimA$ or $\stimB$, is
present.  Part of the observer's decision will be based on the pattern
of cone absorptions during the experimental trial.  We can list the
cone absorptions in a vector, $\data$, whose values represent the
number of absorptions in each of the cones.  Because there are
statistical fluctuations in the light source, and variability in the
image formation process, the pattern of cone absorptions created by
the same stimulus varies from trial to trial.  As a result, the
pattern of cone absorptions from the two different stimuli may
sometimes be the same, and perfect classification may be impossible.

What response strategy should the subject use to classify the stimuli
correctly as often as possible?  A good way to lead your life is this:
{\em When you are uncertain and must decide, choose the more likely
alternative.}

We can translate this simple principle into a statistical procedure by
the following set of calculations.  Suppose that we know the
probability of the signal being $\stimA$ when we observe $\data$.
This is called the {\em conditional probability}, $P(\stimA \mid
\data)$, which is read as ``the probability of $\stimA$ given
$\data$.''  Suppose we also know the conditional probability that the
signal is $\stimB$ is $P(\stimB \mid \data)$.  The observer should
decide that the signal is the one that is more likely given the data,
namely
\begin{equation}
\label{ea3:ml}
\mbox{If}~ P(\stimA \mid \data) ~ > ~ P(\stimB \mid \data )  
        \mbox{~~choose \stimA,  else \stimB}.
\end{equation}

We call the expression in Equation~\ref{ea3:ml} a {\em decision rule}.
The decision rule in Equation~\ref{ea3:ml} is framed in terms of the
likelihood of the stimuli ($\stimA$ or $\stimB$) given the data
($\data$).  This formulation of the probabilities runs counter to our
normal learning experience.  During training, we know that stimulus
$\stimA$ has been presented, and we experience a collection of cone
absorptions.  Experience informs us about probability of the data
given the stimulus, $P(\data \mid \stimA)$, not the probability of the
stimulus given the data, $P(\stimA \mid \data)$.  Hence, we would like
to reformulate Equation~\ref{ea3:ml} in terms of the way we acquire
our experience.

{\em Bayes Rule} is a formula that converts probabilities derived from
experience, $P(\data \mid \stimA)$, into the form we need for the
decision-rule, $P(\stimA \mid \data)$.  The expression for Bayes
Rule is
\begin{equation}
\label{ea3:BayesRule}
P(\stimA  \mid  \data ) = 
 {P ( \data  \mid  \stimA )}  \frac{ P ( A ) }{ P ( \data ) }
\end{equation}
where $P(A)$ is the probability the experimenter presents signal
$\stimA$ (in this case one-half) and $P(\data)$ is the probability of
observing $\data$ quanta.  As we shall see, this second probability
turns out to be irrelevant to our decision.

The probabilities on the right hand side of Bayes Rule are either
estimated from our experience, $P(\data \mid \stimA)$, or they are a
structural part of the experimental design $P(\stimA)$.  The
probability that stimulus $\stimA$ or $\stimB$ is presented is called
the {\em a priori} probability, or {\em base rate}.  The probability
$P(\data \mid \stimA)$ is called the {\em likelihood} of the
observation given the hypothesis.  The probability, $P(\stimA \mid
\data)$ is called the {\em a posteriori} probability.  Bayes Rule
combines the base rate and the likelihood of the observation to form
the a posteriori probability of the hypothesis.

We can use Bayes Rule to express the decision criterion in
Equation~\ref{ea3:ml} in this more convenient form.
\begin{equation}
\label{ea3:PosteriorOdds}
\mbox{If}~~ 
  P(\data \mid \stimA) \frac { P(\stimA)}{ P(\data) } > 
  P(\data \mid \stimB) \frac{ P(\stimB) }{ P(\data) } 
~~\mbox{choose $\stimA$, else $\stimB$}.
\end{equation}
Since the probability of observing the data, $P(\data)$, divides both
of sides of this inequality, this probability is irrelevant to the
decision.  Therefore, we can re-write Equation \ref{ea3:PosteriorOdds}
as
\begin{equation}
\label{ea3:Likelihood}
\frac{ P(\data \mid \stimA) }{ P(\data \mid \stimB) } > 
  \frac{ P(\stimB) } { P(\stimA) }.
\end{equation}
The term on the left hand side of the Equation~\ref{ea3:Likelihood} is
called the {\em likelihood ratio}.  The quantity on the right is
called the {\em odds ratio}.  The formula tells us that we should
select $\stimA$ when the likelihood ratio, which depends on the data,
exceeds the odds ratio, which depends on the a priori knowledge.  A
system that uses this formula to classify the stimuli is called a {\em
Bayes classifier}.

\subsection*{A Bayes classifier for an Intensity Discrimination Task}
In this section we will devise a Bayes classifier for a simple
experimental decision.  Suppose that we ask an observer to decide
whether we have presented one of two brief visual stimuli; the two
stimuli are identical in all ways but their intensity.  We suppose
that the observer decides which stimulus was presented based on the
total number of cone absorptions during the trial, $\sum_{i=1}^{N} d_i
= d$, and without paying attention to the spatial distribution of the
cone absorptions.

Across experimental trials, the number of absorptions from $\stimA$
will vary.  There are two sources of this variability.  One source of
variability is in the stimulus itself.  Photons emitted by a light
source are the result of a change in the energy level of an electron
within the source material.  Each electron within the material has
some probability of changing states and yielding some energy in the
form of a photon.  This change in excitation level is a purely
statistical event and cannot be precisely controlled.  The variability
in the number of emitted photons, then, is inherent in the physics of
light emission and cannot be eliminated.

A second source of variability is in the observer.  On different
experimental trials, the position of the eye, accommodative state of
the lens, and other optical factors will vary.  As these image
formation parameters vary, the chance that photons are absorbed within
the photopigment will vary.  These statistic fluctuations are
unavoidable as well.

The physical variability is easy to model, while the biological
variability is quite subtle.  For this illustrative calculation, we
ignore the effects of biological variability and consider only the
stimulus variability.  In this case, the number of absorptions will
follow the {\em Poisson distribution}.  The formula for the Poisson
probability distribution describes the probability of emitting $d$
quanta given a mean level of $\mu$,
\begin{equation}
\label{ea3:Poisson}
P(d \mid {\mu}) = ( {\mu}^{d} / d ! ) e^{- \mu} .
\end{equation}
The value $\mu$ is called the {\em rate-parameter} of the Poisson
distribution.  The variance of the Poisson distribution is also $\mu$.

Figure~\ref{fa3:sdt} shows the probability distributions of the number
of cone absorptions from the two stimuli, $\stimA$ and $\stimB$.  For
this illustration, I have assumed that the mean absorptions from the
two stimuli are $\muA = 8$ and $\muB = 12$, respectively.  Since the a
priori stimulus probabilities are equal, the observer will select
stimulus $\stimA$ whenever $P(d \mid A)$ exceeds $P(d \mid \stimB)$.
For this pair of distributions, this occurs whenever the observation
is less than 9.
\begin{figure}
\centerline{
  \psfig{figure=../Appendix/fig/sdt.ps ,clip= ,width=5.5in}
}
\caption[Bayes Classifier for Intensity Discrimination]{
{\em Bayes classifier for an intensity discrimination.}  The Poisson
distributions of stimulus $\stimA$ with rate parameter $\muA = 8$
(x's) and and $\stimB$ with rate parameter $\muB = 12$ (o's) are
shown.  When the a priori probabilities of seeing the stimuli are
equal, the Bayes classifier selects $\stimB$ when the absorptions
exceed the dashed line drawn through the graph.  When the a priori
probabilities are $P(\stimA) = 0.75$ and $P(\stimB) = 0.25$, the Bayes
classifier selects $\stimB$ when the absorptions exceed the solid line
drawn through the graph.  }
\label{fa3:sdt}
\end{figure}

Now, suppose the experimenter presents $\stimA$ with probability
$0.75$ and $\stimB$ with probability $0.25$.  In this case, the
subject can be right three quarters of the time simply by always
guessing $\stimA$.  Given the strong a priori odds for $\stimA$, the
Bayes classifier will only choose $\stimB$ if the likelihood exceeds
three to one.  From the distributions in Figure~\ref{fa3:sdt} we find
this occurs when there are at least 13 absorptions.  The Bayes
classifier uses the data and the a priori probabilities to make a
decision~\footnote{ In a classic paper, Hecht, Schlaer and Pirenne
(1942) measured the smallest number of quanta necessary to reliably
detect a signal.  In the most sensitive region of the retina, under
the optimized viewing conditions, they found that 100 quanta at the
cornea, which corresponds to about 10 absorptions, are sufficient.
The quanta are absorbed in an area covered by several hundred rods;
hence, it is quite unlikely that two quanta will be absorbed by the
same rod.  They concluded, quite correctly, that a single photon of
light can produce a measurable response in a single rod.

To find further support for their observation, Hecht et al. analyzed
how the subject's responses varied across trials.  They assumed that
all of the observed variability was due to the stimulus, and none was
due to the observer.  Their account is repeated in a wonderful
didactic style in Cornsweet's book.  I don't think this assumption is
justified; a complete treatment of the data must take into account
variability intrinsic to the human observer (e.g., Nachmias and
Kocher, 1970). }.

\subsection*{A Bayes classifier for a Pattern Discrimination Task}
To discriminate between different spatial patterns, or stimuli located
at different spatial positions, the observer must use the spatial
pattern of cone absorptions.  Consequently, pattern discrimination
must depend on comparisons of a vector of measurements, $\data$, not
just a single value.  In this section, we develop a
Bayes classifier that can be applied to a pattern discrimination task.

Again, for illustrative purposes we assume that the variability is due
entirely to the stimulus.  Moreover, we will assume that the
variability in light absorption at neighboring receptors is
statistically independent.  Independence means that the probability of
$d_i$ absorptions at the $i^{th}$ receptor and $d_j$ at the $j^{th}$
is
\[
P(d_i ~\&~ d_j \mid \stimA) = P(d_i \mid A)~P(d_j \mid \stimA) .
\]
We can extend this principle across all of the receptors to write the
probability of observing $\data$ given signal $\stimA$, namely
\[
L_A = \prod_{i=1}^{N} P(d_i \mid \stimA) ,
\]
where $L_A$ is the likelihood of observing $\data$ given the stimulus
$\stimA$.

Finally, we must specify the spatial pattern of two stimuli.  The
expected number of absorptions at the $i^{th}$ cone will depend on the
spatial pattern of the stimulus.  We call the mean intensity of the
stimulus at location $i$, $\muAi$ and $\muBi$, respectively.

We are ready to compute the proper Bayes classifier.  The general
form of the decision criterion is
\begin{equation}
\mbox{If}~~P(\data \mid \stimA) / P(\data \mid \stimB) 
  > P( \stimB ) / P( \stimA )~~
\mbox{choose A, else B}.
\end{equation}
If the two stimuli are presented in the experiment equally often, we
can re-write this equation as
\begin{equation}
\mbox{If}~~P( \data \mid \stimA ) > P( \data \mid \stimB )~~
\mbox{choose \stimA, else \stimB}.
\end{equation}
Next, we can use independence to write
\begin{equation}
\mbox{If}~~\prod_{i=1}^N P(d_i \mid \stimA) > 
  \prod_{i=1}^N P(d_i \mid \stimB)~~\mbox{choose A, else B}.
\end{equation}
Now, we substitute the Poisson formula, take the logarithm of both
sides, and regroup terms to obtain
\begin{eqnarray}
\label{ea3:linBayes}
& \mbox{If}~
\sum_{i=1}^{N} d_i \ln (\muAi / \muBi) > 
\sum_{i=1}^{N} \muAi - \sum_{i=1}^{N} \muBi~ & \\
& \mbox{choose A, else B}. \nonumber &
\end{eqnarray}

Equation~\ref{ea3:linBayes} can be interpreted as a very simple
computational procedure.  First, notice that the Equation contains two
terms.  The first term is the weighted sum of the photopigment absorptions,
\begin{equation}
\label{ea3:linClass}
\sum_{i=1}^{N} d_i w_i ,
\end{equation}
where $w_i= \ln ( \muAi / \muBi )$.  This is a very familiar
computation; it is directly analogous to the calculation implemented
by a linear neuron whose receptive field sensitivity at position $i$
is $w_i$ (see Chapter~\ref{chapter:retina}).

The second term is the difference between the total number of
photopigment absorptions expected from each stimulus.
\[
\sum_{i=1}^{N} \muAi - \sum_{i=1}^{N} \muBi .
\]
This term acts as a normalizing criterion to correct for the
overall response to the two stimuli.  The decision rule in
Equation~\ref{ea3:linBayes} compares a weighted sum of cone
absorptions to the normalizing criterion. If the first term exceeds the
second, then choose response $\stimA$, else $\stimB$.

We have learned that a Bayes classifier for pattern discrimination can
be implemented using the results of simple linear calculations, like
the calculations represented in the outputs of some peripheral
neurons.  In fact, making a decision like a Bayes classifier is
equivalent to comparing the response of a linear neuron with a
criterion value.  If response of the linear neuron with receptive
field defined by $w_i$ exceeds the criterion value, then choose
stimulus $\stimA$, otherwise choose $\stimB$.

\begin{figure}
\centerline{
 \psfig{figure=../Appendix/fig/geisler.ps ,clip= ,width=5.5in} }
\caption[Geisler Bayesian classifier]{
{\em Bayes classifier for a spatial discrimination task.}  (a) The
mean retinal image of a line.  The grid lines are separated by 30 sec
of arc, approximately the spacing of the cone inner segments in the
fovea.  (b) The mean retinal image of an image formed by a pair of
lines separated by 30 sec of arc.  The intensity of each line is one
half the intensity of the line in panel (a).  (c) The weights of the
Bayes classifier that should be used to discriminate the stimuli in
(a) and (b), given the assumptions of independent Poisson noise.
(After: Geisler, 1989).  }
\label{fa3:geisler}
\end{figure}
Figure~\ref{fa3:geisler} shows an example of a Bayes classifier
computed for a simple pattern discrimination task.  The two spatial
patterns to be discriminated are shown in panels (a) and (b).  The
image in (a) is the retinal image of a line segment.
Figure~\ref{fa3:geisler}b shows the retinal image of a pair of line
segments, separated by 30 sec of arc.  The grid marks on the image are
spaced by 30 sec of arc, essentially the same spacing as the cone
inner segments. The curves below the images measure the intensity of
the stimuli across a horizontal section.

Figure~\ref{fa3:geisler}c shows the weights of the Bayes classifier
for discriminating the two images; the weights were computing using
Equation~\ref{ea3:linClass}.  In this image the gray level represents
the value of the weight that should be applied to the individual cone
absorptions: a medium gray intensity corresponds to a zero weight,
lighter values are positive weights, and darker values are negative
weights.  The spatial pattern of weights bears an obvious resemblance
to the spatial patterns of receptive fields in the visual cortex.

The Bayes classifier specifies how to obtain the optimal
discrimination performance when we know the signal and the noise.
Apart from the optimality of the Bayes classifier itself, observers in
behavioral experiments can never know the true signal and noise
perfectly.  The performance of the Bayes classifier, therefore, must
be equal or superior to human performance.  In nearly all cases where
a Bayes classifier analysis has been carried out, the Bayesian
classifier performance is vastly better than human observer
performance.  Human observers usually fail to extract a great deal of
information available in the stimulus.  In general, then, the Bayes
classifier does not model human performance (Banks et al., 1987).

Why, then, is the Bayes classifier analysis important?  The Bayes
classifier analysis defines what information is available to the
observer.  Defining the stimulus is an essential part of a good
experimental design.  The Bayes classifier defines what the observer
can potentially do, and this serves as a good standard to use in
evaluating performance.  The Bayes classifier helps us to understand
the task; only if we understand the task, can we understand the
observer's performance.

\begin{quote}  
The real power of this approach is that the ideal discriminator [Bayes
classifier] measures all the information available to the later stages
of the visual system.  I would like to suggest that measuring the
information available in discrimination stimuli with a model of the
sort I have described here should be done as routinely as measuring
the luminances of the stimuli with a photometer.  In other words, we
should not only use a light meter but we should also use the
appropriate information meter.  It is simply a matter of following the
first commandment of psychophysics: ``know thy stimulus.''  (Geisler,
1987, p. 30)
\end{quote}

