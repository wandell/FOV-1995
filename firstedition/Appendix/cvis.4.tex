
\chapter{Signal Estimation:  A Geometric View}
\label{chapter:signalEstimation}
This book is filled with calculations of the general form
\begin{equation}
\label{ea4:dotProduct}
\vect{a}^T \vect{x} = \sum_{i=1}^{i=N} a_i x_i ~~~.
\end{equation}
This formula is called the {\em dot product} of the vectors $\vect{a}$
and $\vect{x}$.  It is so important that it is often given a special
notation, such as $\vect{a} \cdot \vect{x}$.  Every time we multiply a
matrix by a vector, $\matr{A} \vect{x}$, we compute the dot product
$\vect{x}$ with the rows of $\matr{A}$.  In this section, I want to
discuss some geometric intuitions connected with the dot product
operation~\footnote{Physicists often refer to
Equation~\ref{ea4:dotProduct} as the scalar product because the result
is a scalar; mathematicians often refer to the dot product as an inner
product and write it using angle brackets $\langle {\vect{a}} ,
{\vect{x}}
\rangle$.}.

Although we used the dot product calculation many times, I decided not
to use the dot product notation in the main portion of the book
because the notation treats the two vectors symmetrically; but, in
most applications the vectors $\vect{a}$ and $\vect{x}$ had an asymmetrical
role.  The vector $\vect{x}$ was usually a stimulus, say the wavelength
composition of a light or a one-dimensional spatial pattern, and the
vector $\vect{a}$ was part of the sensory apparatus, say a photopigment
wavelength sensitivity or a ganglion cell receptive field.  Because
the physical entities we described were not symmetric, I felt that the
asymmetric notation, $\vect{a}^T \vect{x}$, was more appropriate.

The scalar value of the dot product between two vectors is equal to
\[
\vect{a} \cdot \vect{x} = \| \vect{a} \| \| \vect{x} \| \cos( \theta )
\]
where $\| \cdot \|$ is the vector length and $\theta$ is the angle
between the vectors.  To simplify the discussion in the remainder of
this section, we will assume that the vector $\vect{a}$ has unit length.

\begin{figure}
\centerline{
 \psfig{figure=../Appendix/fig/dotProduct.ps,clip= ,width=5.5in}
}
\caption[Dot Product Geometry in two dimensions]{
{\em A geometric interpretation of the two-dimensional dot product}.
(a) A geometrical view of the dot product of two vectors is shown.
The dashed line is a perpendicular from the tip of vector $\vect{x}$ to
the unit-length vector $\vect{a}$. (b) Any vector whose endpoint falls
along the dashed line yields the same scalar value when we compute the
vector's dot product with $\vect{a}$.  }
\label{fa4:dotProduct}
\end{figure}
Figure~\ref{fa4:dotProduct}a is a geometric representation of the dot
product operation.  The unit vector $\vect{a}$ and the signal vector
$\vect{x}$ are drawn as arrows extending from the origin.  A dashed line
is drawn from the tip of $\vect{x}$ (the signal) at right angles to the
vector $\vect{a}$ (the sensor).  The length of the vector from the origin
to point of intersection between the perpendicular dashed line and
$\vect{a}$ is called the {\em projection} of $\vect{x}$ onto $\vect{a}$.
Because we have assumed $\vect{a}$ has unit length, the length of the
projection, $\| \vect{x} \| \cos ( \theta )$, is equal to the dot
product, $\vect{a} \cdot \vect{x} = \| \vect{a} \| \| \vect{x} \| \cos (\theta)$.

By inspecting Figure~\ref{fa4:dotProduct}b, you can see that there are
many signal vectors, $\vect{x}$, that have the same dot product with
$\vect{a}$. Specifically, all of the vectors whose endpoints fall along a
line that is perpendicular to $\vect{a}$, have the same dot product with
$\vect{a}$.  Hence, any signal represented by a vector whose endpoint is
on this line will cause the same response in the sensor represented by
the vector $\vect{a}$.

\begin{figure}
\centerline{
 \psfig{figure=../Appendix/fig/dotProduct3.ps,clip= ,width=5.5in}
}
\caption[Dot product geometry in three dimensions]{
{\em A geometric representation of the three-dimensional dot product.}
(a) The perpendicular from $\vect{x}$ to $\vect{a}$ is indicated by the
dashed line.  When $\vect{a}$ is a unit vector, the distance from the
origin to the intersection of the dashed line with $\vect{a}$ is the
scalar value of the dot product.  (b) Any vector whose endpoint falls
within the indicated plane yields the same scalar value when we
compute the vector's dot with $\vect{a}$.  }
\label{fa4:dotProduct3}
\end{figure}
The geometric intuition extends smoothly to vectors with more than two
dimensions.  Figure~\ref{fa4:dotProduct3}a shows the dot product
between a pair of three-dimensional vectors.  In three-dimensions, all
of the vectors whose endpoints fall on a plane perpendicular to the
unit-length vector $\vect{a}$ have the same dot product with that vector
(Figure~\ref{fa4:dotProduct3}b).  In four or more dimensions the set
of signals with a common dot product value fall on a {\em hyperplane}.

Figures~\ref{fa4:dotProduct}b and \ref{fa4:dotProduct3}b illustrate
the information that is preserved and that is lost in a dot product
calculation.  When we measure the dot product of a pair of
two-dimensional vectors, we learn that the signal must have fallen
along a particular line; when we measure a three-dimensional
dot-product, we learn that the signal must have fallen within a
certain plane.  Hence, each dot product helps to define the set of
possible input signals.

By combining the results from several dot products, we can
estimate the input signal.  We can use the geometric representation of
the dot product in Figure~\ref{fa4:dotProduct} to develop some
intuitions about how to estimate the signal from a collection of
sensor responses.  This is a {\em signal estimation} problem.

\begin{figure}
\centerline{
 \psfig{figure=../Appendix/fig/estimation.ps,clip= ,width=5.5in}
}
\caption[Estimation of the Signal from the Responses]{
{\em Signal estimation from multiple linear sensors.}  (a) We can
infer the location of a two-dimensional signal vector from the
responses of two linear sensors. (b) When the vectors representing the
sensors are orthogonal, the estimation error is small.  (c) When the
vectors representing the sensors are nearly aligned, the estimation
error tends to be quite large in the direction perpendicular to the
sensor vectors.  }
\label{fa4:estimation}
\end{figure}
First, imagine that the response of each linear neuron is equal to the
dot product of a vector describing the spatial contrast pattern of a
stimulus with a vector describing the neuron's receptive field.
Further, suppose that the input stimuli are drawn from a set of simple
spatial patterns, namely the weighted sums of two cosinusoidal
gratings,
\[
\vecti{w}{1} \cos(2 \pi f_1 x) + \vecti{w}{2} \cos(2 \pi f_2 x)
\]
We can represent each of these spatial contrast patterns using a
two-dimensional vector, $\vect{x} = (w_1, w_2)$, whose entries are the
weights of the cosinusoids.  We can represent the sensitivity of each
linear neuron to these spatial patterns using a two-dimensional
vector, $\vect{a}_i$, whose two entries define the neuron's
sensitivity to each of the cosinusoidal terms.  Because the neurons
are linear, we can compute the $i^{th}$ neuron's response to any
pattern in the set from the linear calculation in the dot product,
namely, ${\vect{a}_i}^T \vect{x}$.

Figure~\ref{fa4:estimation}a shows geometrically how to use two
responses to identify uniquely the two-dimensional input stimulus.
Suppose that the response of the neuron with receptive field
$\vect{a}_1$ is $r_1$.  Then, we can infer that the stimulus must fall
along a line perpendicular to the vector $\vect{a}_1$ and intersecting
$\vect{a}_1$ at a distance $\vecti{r}{1} = \| \vect{x} \| \cos (
\theta_i)$ from the origin\footnote{In general, we would place the
perpendicular line at a distance of $\| \vect{x} \| cos ( \theta_i ) /
\| \vect{a}_i\|$, but we have assumed that $\vect{a}_i$ has unit
length.}.  If the response to the second neuron is $r_2$, we can draw
a second line that describes a second set of possible stimulus
locations.  The true stimulus must be on both lines, so it is located
at the intersection of the two dashed lines.

When the sensor responses have some added noise, which is always the
case in real measurements, some sensor combinations provide more
precise estimates than others.  Figure~\ref{fa4:estimation}b shows a
pair of sensors whose vectors are nearly orthogonal to one another.
Because of the sensor noise, the true identity of the signal is
uncertain.  The lightly shaded bands drawn around the perpendicular
dashed lines show the effect of sensor noise on the estimated region
from the individual measurements.  The darkly shaded region is the
intersection of the bands, showing the likely region containing the
signal.  For these orthogonal sensors, the dark band that is likely to
contain the signal is fairly small.

Figure \ref{fa4:estimation}c shows the same representation but for a
pair of sensors represented nearly parallel vectors.  Such a pair of
sensors is said to be {\em correlated}.  When the sensors are
correlated in this way, the shaded region can be quite large in the
direction perpendicular to the sensor vectors.  In the presence of
noise, correlated sensors provide a poor estimate of the signal.

\subsection*{Matrix equations}
Ordinarily, we use matrix multiplication to represent a collection of
dot products.  Many aspects of the signal estimation problem can
be expressed and solved using methods developed for matrix algebra.

To see the connection between matrix multiplication and the signal
estimation problem, consider the following example.  Write the dot
product of a two-dimensional signal and a sensor in the tableau
format,
\[
r_1 = 
\left ( \vecti{a}{1} ~~~ \vecti{a}{2} \right )
\left (
\begin{array}{c}
 \vecti{x}{1} \\
 \vecti{x}{2} \\
\end{array}
\right ) .
\]
Now, suppose we have two sensors and two responses.  Then, we can
expand the tableau by adding more rows to represent the new measurements,
as in
\begin{equation}
\label{ea4:twobytwo}
\left (
 \begin{array}{c}
  \vecti{r}{1} \\
  \vecti{r}{2}
 \end{array}
\right ) =
\left ( 
 \begin{array}{cc}
   \vectij{a}{1}{1} & \vectij{a}{1}{2} \\
   \vectij{a}{2}{1} & \vectij{a}{2}{2}
 \end{array}
\right )
\left (
\begin{array}{c}
 \vecti{x}{1} \\
 \vecti{x}{2} \\
\end{array}
\right ) ,
\end{equation}
where $\vectij{a}{i}{\cdot}$ is a vector that represents the
sensitivity of the $i^{th}$ sensor.  In the usual signal estimation
problem, we know the properties of the sensors represented in the rows
of the matrix and we know the responses in $\vect{r}$.  We wish to
estimate the signal in $\vect{x}$.  The pair of sensor vectors
represented in the rows of the matrix, call it $\matr{A}$, are the
counterpart of the vectors shown in Figure~\ref{fa4:estimation}a.  Our
ability to estimate the signal from the resposnes depends on the
correlation between the rows of the matrix.

The matrix tableau in Equation~\ref{ea4:twobytwo} shows a special case
in which there are two sensors and two unknown values of the signal.
In general, we might have more sensors or more unknowns in the signal.
Having more sensors would force us to add more rows to the matrix;
having more unknowns in the signal would force us to change the
lengths of the sensor and signal vectors.  Each of these changes might
change the general shape of the matrix tableau.  We can write the
general case, without specifying the number of sensors or unknowns,
using the concise representation
\begin{equation}
\label{ea4:linEstimation}
\vect{r} = \matr{A} \vect{x} .
\end{equation}
Again, in the (linear) signal estimation problem we know the responses
($\vect{r}$) and the sensors in the rows of $\matr{A}$ and we wish to
estimate the signal ($\vect{x}$).

Many topics we have reviewed in the text can be framed as linear
estimation problems within the matrix
Equation~\ref{ea4:linEstimation}.  For example, you might imagine that
the rows of the matrix are retinal ganglion receptive fields, the
responses are neural firing rates, and the input is a spatial contrast
pattern (Chapter~\ref{chapter:retina}).  The brain may then need to
estimate various properties of the contrast pattern from the neural
firing patterns.  Or, you might imagine that the rows of the matrix
represent the spectral responsivities of the three cone types, the
responses are the cone signals, and the signal is a spectral power
distribution (Chapters~\ref{chapter:wavelength} and
\ref{chapter:Color}).  Or, you might imagine that the each row of the
matrix represents the receptive field of a space-time oriented neuron,
the output is the neuron's response, and the signal are the two
velocity components of the local motion flow field
(Chapter~\ref{chapter:Motion}).

There are many matrix algebra tools that are useful for solving matrix
equations like Equation~\ref{ea4:linEstimation}.  To choose the proper
tool for a problem, one must take into account a variety of specific
properties of the measurement conditions.  For example, in some cases
there are more sensors than there are unknown entries in the vector
$\vect{x}$.  We can represent this estimation problem using matrix
tableau as
\begin{equation}
\label{ea5:overconstrained}
\left(
 \begin{array}{c}
  ~ \\
  ~ \\
  \vect{r} \\
  ~ \\
  ~ \\
 \end{array}
\right)
 = 
\left ( 
 \begin{array}{ccc}
  ~& ~ & ~ \\
  ~& ~ & ~ \\
  ~ & \matr{A} & ~ \\
  ~& ~ & ~ \\
  ~& ~ & ~
 \end{array}
\right )
\left (
 \begin{array}{c}
 ~ \\
 \vect{x} \\
 ~ \\
 \end{array}
\right ) .
\end{equation}
The shape of the tableau makes it plain that there are more responses
in the vector $\vect{r}$ than there are unknowns in the vector
$\vect{x}$.  This type of estimation problem is called {\em
over-constrained}.  When there is noise in the measurements, no exact
solution to the over-constrained problem may exist; that is, there may
be no vector $\vect{x}$ such that the equality in
Equation~\ref{ea5:overconstrained} is perfectly satisfied.  Instead,
we must define an error criterion and try to find a ``best'' solution,
that is a solution that minimizes the error criterion.

In general, the best solution will depend on the noise properties and
the error criterion.  In some cases, say when the noise is Gaussian
and the error is the sum of the squared difference between the
observed and predicted responses, there are good closed form solutions
to the linear estimation problem.  That is, one can find an estimate
of the signal without using any search algorithms, but by direct
computation.  If one has other error criteria or noise, a search may
be necessary.

A full discussion of the problems involved in signal estimation and
matrix algebra would take us far beyond the scope of this book, but
there are several excellent textbooks that explain these ideas
(Strang, 1993; Lawson and Hanson, 1974, Vetterling, et al., 1992).
Also, the manuals for several computer packages, such as {\em Matlab}
and {\em Mathematica}, contain useful information about using these
methods and further references. To see how some of these tools have
been applied to vision science, you might consult some of the
following references (Brainard, 1994, Heeger and Jepson, 1992;
Marimont and Wandell,1992; Nielsen and Wandell, 1988; Thomas et al.,
1994; Tomasi and Kanade, 1991, Wandell, 1987)




