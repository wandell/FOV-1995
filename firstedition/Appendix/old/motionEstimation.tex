\subsection*{Signal estimation applied to moton}
We can apply these geometric representations of the dot product to the
velocity estimation problem described in Chapter~\ref{chapter:Motion}.
Following the discussion in that chapter, we might take
Figure~\ref{f9:gradVelocity} to represent a velocity 
diagram.  The horizontal and vertical axes correspond to the two
velocity components, $v_x$ and $v_y$.  Each line in the diagram
represents the set of velocities of a 
one-dimensional spatial stimulus that are consistent with the response
amplitude of a space-time oriented sensor.  Each point on a dashed line
represents a velocoity that satisfies the motion gradient constraint
at a location. 
Specifically, at the $i^{th}$ point the velocity in the $x$ and $y$
directions $(v_x,v_y)$ is related to the partial derivatives of the
image intensities in the $x$, $y$, and $t$ directions by the linear
equation
\begin{equation}
v_{x} \parder{I}{x}(a_i,b_i,t_i) + 
  v_{y} \parder{I}{y}(a_i,b_i,t_i) = - \parder{I}{t})(a_i,b_i,t_i)
\end{equation}
We can express this linear equation as a dot product as follows.
Define a column vector, $\vect{m}_i$ whose entries are
$(\parder{I}{x}(a_i,b_i,t_i), \parder{I}{y}(a_i,b_i,t_i)$, and define
the two-dimensional velocity vector as $\vect{v} = (v_x , v_y)$.
Then at a single point in the image we have the dot product relationship,
\[
- d_i = {\vect{m}_i}^T \vect{v}
\]
where $d_i$ is the partial derivative in the $t$ direction,
$\parder{I}{t}(a_i,b_i,t)$. 

We can group the dot products from a collection of points into a
single matrix equation, as follows.
Define a matrix, $\bf M$, whose $i^{th}$ row is ${\vect{m}_i}^T$.
Define the vector $\bf d$ whose $i^{th}$ entry is $d_i$.
We can group the dot product eqations for each location into a single
matrix equation, as
equation
\begin{equation}
{\bf -d = M v }.
\end{equation}
This equation can be written as a matrix tableau,
\begin{equation}
- \left(
 \begin{array}{c}
  \parder{I}{t}(a_1,b_1,t_1) \\
  \parder{I}{t}(a_2,b_2,t_2) \\
  \vdots
 \end{array}
\right)
 = 
\left ( 
 \begin{array}{cc}
  \parder{I}{x}(a_1,b_1,t_1) & \parder{I}{y}(a_1,b_1,t_1) \\ 
  \parder{I}{x}(a_2,b_2,t_2) & \parder{I}{y}(a_2,b_2,t_2) \\ 
  \vdots  & \vdots
 \end{array}
\right )
\left (
 \begin{array}{c}
 v_x \\
 v_y
 \end{array}
\right ) 
\end{equation}
Each row of this matrix equation defines a dot product equation, and
the velocity vectors that solve that equation fall on a line in the
velocity diagram.

This type of problem arises very commonly, and there are many
algebraic tools available to solve this collection of linear
equations.  Generally, we choose a solution to this equation,
$\hat{\vect{v}}$ that minimizes the error between the the observed
values, $\vect{d}$ and the predicted values, $\bf M \hat{\vect{v}}$.
Were it possible, we would find this vector by using the inverse of
the matrix $\bf M$, namely ${\bf M}^{-1}$.  Since $\bf M$ is
rectangular, not square, it does not have a proper inverse.  The
matrix we use to find the velocity vector that minimizes the error is
called the {\em pseudo-inverse} of $\bf M$.  Assuming that the square
matrix ${\bf M}^T {\bf M}$ has an inverse, the pseudo-inverse of $\bf
M$ is ${\bf M}^{T} {\bf M})^{-1} {\bf M}^{T}$, and the estimated
velocity vector is
\begin{equation}
\label{e9:motionFlow}
\hat{\vect{v}} = - ( {\bf M}^{T} {\bf M})^{-1} {\bf M}^{T} d .
\end{equation}


-------------


\subsection*{Estimation Algorithms}
Equation~\ref{e9A:motionFlow} is a forward calculation.
We begin with the camera motion and the depth map and
we create a motion flow field.
This computation is important for computer graphics
and for designing experimental stimuli.
An important and related question is this:
If we know the motion flow field, $\motFlow{u}{v}$, 
can we estimate the rotation ($\rotv$),
translation ($\transv$), and depth map $z(u,v)$?

We can get a rough sense of the estimation challenge
by counting up the number of unknown parameters and
the number of measurements in the motion flow field.
The motion of the pinhole camera position
is described by six parameters (three translation
and three rotation).
These are global parameters that apply to every point
in the image plane.
In addition, there is one unknown depth parameter
that differs at each image point.
Hence, across $N$ image points we have $6 + N$ unknown parameters.

At each image point
we measure two motion flow field values.
Unsurprisingly it is impossible to recover all
of the unknown parameters from the motion flow field vector
at a single image point;
we must use information
from a collection of image points.
This can succeed for a static scene, in principle,
because we accumulate two additional 
motion flow field measurements
from each image location.
Hence, from $N$ image points
we obtain $2*N$ motion flow field values.

After combining $N=7$ points,
we have more measurements (14) than unknowns (13)
and a unique solution becomes possible.
But, the estimation problem is not linear.
There has been a great deal of work on estimation methods
(e.g.,  Heeger and Jepson, 1991; Poelman and Kanade, 1993;
Tomasi and Kanade, 1992, Faugeras, 1992).
The algorithms, simulations, and demonstrations in these
papers show that it is possible
to derive good estimates of camera position
and the depth map from the motion flow field.
It seems likely
that improvements in the algorithms as regards
the imaging model, occlusion, transparency, 
and the use of robust statistical procedures
will create a powerful and practical technology in 
before too long  (M. Black, 1992).


