\chapter{Shift-Invariant Linear Systems}
\label{chapter:sil}
Many of the ideas in this book rely on properties of shift-invariant
linear systems.  In the text, I introduced these properties without
any indication of how to prove that they are true.  I have two goals
for this section.  First, I will sketch proofs of several important
properties of shift-invariant linear systems.  Second, I will describe
{\em convolution} and the {\em Discrete Fourier Series}, two tools
that help us take advantage of these properties\footnote{The results
in this appendix are expressed using real harmonic functions.  This is
in contrast to the common practice of using complex notation,
specifically Euler's complex exponential, as a shorthand to represent
sums of harmonic functions.  The exposition using complex exponentials
is brief and elegant, but I believe it obscures the connection with
experimental measurements.  For this reason, I have avoided the
development based on complex notation.}.

\subsection*{Shift-Invariant Linear Systems:  Definitions}
Shift-invariance is a system property that can be verified by
experimental measurement.  For example, in
Chapter~\ref{chapter:imageformation} I described how to check whether
the optics of the eye is shift-invariant by the following
measurements.  Image a line through the optics onto the retina, and
measure the linespread function.  Then, shift the line to a new
position, forming a new retinal image.  Compare the new image with the
original.  If the images have the same shape, differing only by a
shift in position, then the optics is shift-invariant over the
measured range.

We can express the empirical property of shift-invariance using the
following mathematical notation.  Choose a stimulus, $\pixveci{i}$ and
measure the system response, $\resveci{i}$.  Now, shift the stimulus
by an amount $j$ and measure again.  If the response is shifted by $j$
as well, then the system may be shift-invariant.  Try this experiment for
many values of the shift parameter, $j$.  If the experiment succeeds
for all shifts, then the system is shift-invariant.

If you think about this definition as an experimentalist, you can see
that there are some technical problems in making the measurements
needed to verify shift-invariance.  Suppose that the original stimulus
and response are represented at $N$ distinct points, $i = 1, \ldots,
N$.  If we shift the stimulus three locations so that now the fourth
location contains the first entry, the fifth the second, and so forth,
how do we fill in the first three locations in the new stimulus?  And,
what do we do with the last three values, $N-2, N-1, N$, which have
nowhere to go?

Theorists avoid this problem by treating the real observations as if
they are part of an infinite, periodic set of observations.  They
assume that the stimuli and data are part of an infinite periodic
series with a period of $N$, equal to the number of original
observations.  If the data are infinite, the first three entries
of the shifted vector are the three values at locations $-3, -2,
\mbox{and} -1$.  If the data are periodic with period $N$, these
three values are the same as the values at $N-3, N-2, \mbox{and} N-1$.

The assumption that the measurements are part of an infinite and periodic
sequence permits the theorist to avoid the experimentalist's
practical problem.  The assumption is also essential for obtaining
several of the simple closed-form results concerning the properties
of shift-invariant systems.  The assumption is not consistent with
real measurements since real measurements cannot be made using
infinite stimuli: there is always a beginning and an end to any real
experiment.  As an experimentalist you must always be aware that many
theoretical calculations using shift-invariant methods are not valid
near the boundaries of data sets, such as near the edge of an image.

Suppose we refer to the finite input as, $\pixvec$, and the measured
output, $\lspread$, is finite.  In the theoretical analysis we extend
both of these functions to be infinite and periodic.  We will use a
hat symbol to denote the extended functions,
\begin{equation}
\label{f1:extend}
\lspreadhati{i+N} = \lspreadi{i}
  ~~~\mbox{and}~~~
\pixvechati{i+N}=\pixveci{i} .
\end{equation}

The extended functions $\lspreadhat{}$ and $\pixvechat$ agree with our
measurements over the measurement range from $1, \ldots, N$.  By the
periodicity assumption, the values outside of the measurement range
are filled in by looking at the values within the measurement range.
For example,
\[
(\ldots, 
 \lspreadhati{-1} = \lspreadhati{N-1},
 ~\lspreadhati{0} = \lspreadhati{N},
 ~\lspreadhati{1},
 \ldots,
 \lspreadhati{N},~
 \lspreadhati{N+1} = \lspreadhati{1}, 
 \ldots ) .
\]

\subsection*{Convolution}
Next, we derive some of the properties of linear shift-invariant
systems.  We begin by describing these properties in terms of the {\em
system matrix} (see Chapter~\ref{chapter:imageformation}).  Then, we
will show how the simple structure of the shift-invariant system
matrix permits us to relate the input and output by a summation
formula called {\em cyclic convolution}.  The convolution formula is
so important that shift-invariant systems are sometimes called {\em
convolution systems}\footnote{Since we use only cyclic convolution
here, I will drop the word ``cyclic'' and refer to the formula simply
as convolution.  This is slightly abusive, but conforms to common
practice in many fields.}.

In Chapter~\ref{chapter:imageformation} I reviewed how to measure the
system matrix of an optical system for one-dimensional input stimuli.
We measure the image resulting from a single line at a series of
uniformly spaced input locations.  If the system is shift-invariant,
then the columns of the system matrix are shifted copies of one
another (except for edge artifacts).  To create the system matrix, we
extend the inputs and outputs to be periodic functions
(Equation~\ref{f1:extend}). Then, we select a central block of size $N
\times N$ to be the system matrix and we use the corresponding entries
of the extended stimulus.  For example, if the input stimulus consists
of six values, $\pixvec = (0,0,0,1,0,0)$, and the response to this
stimulus is the vector, $\lspread = (0.0,0.3,0.6,0.2,0.1,0.0)$, then
the $6 \times 6$ system matrix is
\begin{equation}
\Cyclichat =
 \left (
 \begin{array}{cccccc}
  0.2 & 0.6 & 0.3 & 0.0 & 0.0 & 0.1 \\
  0.1 & 0.2 & 0.6 & 0.3 & 0.0 & 0.0 \\
  0.0 & 0.1 & 0.2 & 0.6 & 0.3 & 0.0 \\
  0.0 & 0.0 & 0.1 & 0.2 & 0.6 & 0.3 \\
  0.3 & 0.0 & 0.0 & 0.1 & 0.2 & 0.6 \\
  0.6 & 0.3 & 0.0 & 0.0 & 0.1 & 0.2 
 \end{array}
\right) .
\end{equation}

For a general linear system, we calculate the output using the
summation formula for matrix multiplication in
Equation~\ref{e1:MatrixMultiplication0},
\begin{equation}
\resvechati{i} = \sumoneNj \Cyclichati{ij} \pixvechati{j} ~~.
\end{equation}
When the linear system is shift-invariant system, this summation
formula simplifies for two reasons.  First, because of the assumed
periodicity, the summation is precisely the same when we sum over any
$N$ consecutive integers.  It is useful to incorporate this
generalization into the summation formula as
\begin{equation}
\label{ea1:ccn}
\resvechati{i} = \sumoverNj \Cyclichati{ij} \pixvechati{j} ~~~,
\end{equation}
where the notation $j = \langle N \rangle$ means that summation can
take place over any $N$ consecutive integers.  Second, notice that for
whichever $N \times N$ block of values we choose, the typical entry of
the system matrix will be
\begin{equation}
\label{e1a:cycconv}
\Cyclichati{ij} = \lspreadhati{i-j} .
\end{equation}
We can use this relationship to simplify the summation further,
\begin{equation}
\label{e1a:cc}
\resvechati{i} = \sumoverNj \lspreadhati{i-j} \pixvechati{j} ~~.
\end{equation}
In this form, we see that the response depends only on the input and
the linespread.  The summation formula in Equation~\ref{e1a:cc} is
called {\em cyclic convolution}.  Hence, we have shown that to compute
the response of a shift-invariant linear system to any stimulus, we
need measure only the linespread function.

\subsection*{Convolution and Harmonic Functions}
Next, we study some of the properties of the convolution formula.
Most important, we will see why harmonic functions have a special role
in the analysis of convolution systems.

Beginning with our analysis of optics in
Chapter~\ref{chapter:imageformation}, we have relied on the fact that
the response of a shift-invariant system to a harmonic function at
frequency $f$ is also a harmonic function at $f$.  In that chapter,
the result was stated in two equivalent ways.

\be

\item
If the input is a harmonic at frequency $f$, the output is a shifted
and scaled copy of the harmonic.

\item
The response to a harmonic at frequency $f$ will be the weighted sum
of a sinusoid and cosinusoid at the same frequency
(Equation~\ref{e1:r1}).

\ee

We can derive this result from the convolution formula.  Define a new
variable, $k=i-j$, and substitute $k$ into Equation~\ref{e1a:cc}.
Remember that the summation can take place over any adjacent $N$
values.  Hence, the substitution yields a modified convolution
formula,
\begin{equation}
\label{e1a:c2}
\resvechati{i} = \sumoverNk \lspreadhati{k} \pixvechati{ {i - k} } .
\end{equation}

Next, we use the convolution formula in Equation~\ref{e1a:c2} to
compute the response to a sinusoidal input $\SfNj$.  From trigonometry
we have that
\begin{equation} 
\label{e1a:sinSum}
\SfNij = \SfNi \CfNj + \SfNj \CfNi ~ .
\end{equation}
Substitute Equation~\ref{e1a:sinSum} into Equation~\ref{e1a:c2},
remembering that $\sin (-k) = - \sin (k)$ and $\cos (-k) = \cos (k)$.
\begin{eqnarray}
\resvechati{i} & = & \sumoverNk \lspreadhati{k} \SfNi \CfNk
 + \sumoverNk \lspreadhati{k} \SfnNk \CfNi \nonumber \\
& = & \SfNi \sumoverNk \lspreadhati{k} \CfNk
 - \CfNi \sumoverNk \lspreadhati{k} \SfNk
\end{eqnarray}
We can simplify this expression to the form
\begin{equation}
\label{e1a:harmonic}
\resvechati{i} = a~\SfNi - b~\CfNi
\end{equation}
where 
\begin{eqnarray}
a & = & \sumoverNk \lspreadhati{k} \CfNk \nonumber \\
b & = & \sumoverNk \lspreadhati{k} \SfNk \nonumber
\end{eqnarray}

We have shown that when the input to the system is a sinusoidal
function at frequency $f$, the output of the system is the weighted
sum of a sinusoid and a cosinusoid, both at frequency $f$.  This is
equivalent to showing that when the input is a sinusoid at frequency
$f$, the output will be a scaled and shifted copy of the input, $s_f
\sin (\pfiN + \phi_f )$ (see Equation~\ref{e1:r1}).  As we shall see
below, it is easy to generalize this result to all harmonic functions.

\subsection*{The Discrete Fourier Series:  Definitions}
In general, when we measure the response of a shift-invariant linear
system we measure $N$ output values.  When the input is a sinusoid, or
more generally a harmonic, we can specify the response using only the
two numbers, $a$ and $b$, in Equation~\ref{e1a:harmonic}.  We would
like to take advantage of this special property of shift-invariant systems.  To
do so, we need a method of representing input stimuli as the weighted
sum of harmonic functions.

The method used to transform a stimulus into the weighted sum of
harmonic functions is called the {\em Discrete Fourier Transform}
(DFT).  The representation of the stimulus as the weighted sum of
harmonic functions is called the {\em Discrete Fourier Series} (DFS).
We use the DFS to represent an extended stimulus, $\pixvechati{}$.
\begin{equation}
\label{e1a:DFS}
\pixvechati{i} = \sumzeroNf a_f \CfNi +  b_f \SfNi ~~.
\end{equation}

We are interested in that part of the extended stimulus that coincides
with our measurements.  We can express the relationship between the
harmonic functions and the original stimulus, $\pixvec{}$, using a
matrix equation
\begin{equation}
\label{e1a:dfs2}
\pixvec{} = \matCos \vect{a} + \matSin \vect{b}
\end{equation}
which has the matrix tableau form
\begin{equation}
\left (  
 \begin{array}{c}    ~ \\   \pixvec{} \\   ~ \end{array}
\right ) =
\left (
 \begin{array}{ccc} ~ & ~ &  ~ \\ ~ & \matCos & ~\\ ~ &  ~ & ~ \end{array}
\right )
\left (
 \begin{array}{c}  ~ \\ \vect{a } \\ ~ \end{array}
\right )
 +
\left (
 \begin{array}{ccc} ~ & ~ &  ~ \\ ~ & \matSin & ~\\ ~ &  ~ & ~ \end{array}
\right ) 
\left(
 \begin{array}{c} ~ \\ \vect{b} \\ ~ \end{array}
\right ) \nonumber
\end{equation}
The vectors ${\vect{a}}$ and ${\vect{b}}$ contain the coefficients
$a_f$ and $b_f$, respectively.  The columns of the matrices $\matCos$
and $\matSin$ contain the relevant portions of the cosinusoidal and
sinusoidal terms, $\CfNi$ and $\SfNi$, that are used in the DFS
representation.

The DFS represents the original stimulus as the weighted sum of a set
of harmonic functions (i.e., sampled sine and cosine functions).  We
call these sampled harmonic functions the {\em basis functions} of the
DFS representation. The vectors $\vect{a}$ and $\vect{b}$ contain
basis functions {\em weights} or {\em coefficients} that specify how
much of each basis function must be added in to recreate the original
stimulus, $\pixvec$.

\subsection*{The Discrete Fourier Series:  Properties}
\begin{figure}
\centerline{
 \psfig{figure=../Appendix/fig/sincos.ps,clip= ,width=5.5in}
}
\caption[Cosinusoidal and Sinusoidal terms in the DFS]{
{\em The basis functions of the Discrete Fourier Series.}  The
cosinusoidal (a) and sinusoidal (b) basis functions of the discrete
Fourier series representation when $N=8$ are shown.  Notice the
redundancy between the functions at symmetrically placed frequencies.}
\label{f1a:sincos}
\end{figure}
Figure~\ref{f1a:sincos} shows the sampled sine and cosine functions
for a period of $N = 8$.  The functions are arrayed in a circle to
show how they relate to one another.  There are a total of 16 basis
functions.  But, as you can see from Figure~\ref{f1a:sincos}, they are
redundant.  The sampled cosinusoids in the columns of $\matCos$ repeat
themselves (in reverse order); for example, when $N=8$ the cosinusoids
for $f = 1,2,3$ are the same as the cosinusoids for $f= 7,6,5$.  The
sampled sinusoids in the columns of $\matSin$ also repeat themselves
except for a sign reversal (multiplication by negative one).  There
are only four independent sampled cosinusoids, and four independent
sampled sinusoids.  As a result of this redundancy, neither the
$\matSin$ nor the matrix $\matCos$ is invertible.

Nevertheless, the properties of these harmonic basis functions make it
simple to calculate the vectors containing the weights of the harmonic
functions from the original stimulus, $\pixvec$.  To compute
$\vect{a}$ and $\vect{b}$, we multiply the input by the basis
functions, as in
\begin{equation}
\label{e1a:dfsCalcCoef}
\vect{a} = \frac{1}{N} \matCos^T \pixvec 
~~~\mbox{and}~~~
\vect{b} = \frac{1}{N} \matSin^T \pixvec .
\end{equation}

We can derive the relationship in Equation~\ref{e1a:dfsCalcCoef} from
two observations.  First, the matrix sum, $\matr{H} = \matSin +
\matCos$ has a simple inverse.  The columns of $\matr{H}$ are
orthogonal to one another, so that the inverse of $\matr{H}$ is
simply\footnote{This observation is the basis of another useful linear
transform method called the {\em Hartley Transform} (Bracewell,
1986).}.
\[
\matr{H}^{-1} = \frac{1}{N} \matr{H}^{T} = \frac{1}{N} (\matSin + \matCos)^T .
\]

Second, the columns of the matrices $\matCos$ and $\matSin$ are
perpendicular to one another: $\matr{0} = \matCos \matSin^{T}$.
This observation should not be surprising since continuous sinusoids
and cosinusoids are also orthogonal to one another.

We can use these two observations to derive
Equation~\ref{e1a:dfsCalcCoef} as follows.  Express the fact that
$\matr{H}$ and $\frac{1}{N} \matr{H}^{T}$ are inverses as follows: 
\begin{eqnarray}
\label{e1a:dfsInv}
{{\bf I}_{N \times N}} & = & \matr{H} (\frac{1}{N} \matr{H}^T ) \\
 & = & \frac{1}{N} (\matCos + \matSin) (\matCos + \matSin)^T \\
 & = & \frac{1}{N} (\matCos \matCos^T + \matSin \matSin^T),
\end{eqnarray}
where ${\matr{I}_{N \times N}}$ is the identity matrix.  Then,
multiply both sides of Equation~\ref{e1a:dfsInv} by $\pixvec$,
%cn cn' stim + sn sn' stim = stim.
\begin{equation}
\label{e1a:dfsCoef}
\pixvec =  
  \frac{1}{N} 
    (\matCos \matCos^T \pixvec + \matSin \matSin^T \pixvec ).
\end{equation}   
Compare Equation~\ref{e1a:dfsCoef} and Equation~\ref{e1a:dfs2}.
Notice that the Equations become identical if we make the assignments
in Equation~\ref{e1a:dfsCalcCoef}.  This completes the sketch of the
proof.

\subsection*{Measuring a Convolution System using Harmonic Functions.}

Finally, we show how to predict the response of a shift-invariant
linear system to any stimulus using only the responses to unit
amplitude cosinusoidal inputs.  This result is the logical basis for
describing system performance from measurements of the response to
harmonic functions.  When the system is linear and shift-invariant,
its responses to harmonic functions is a complete description of the
system; but, this is not true for arbitrary linear systems.

Because cosinusoids and sinusoids are shifted copies of one another,
the response of a shift-invariant linear system to these functions is
the same except for a shift.  From a calculation like the one in
Equation~\ref{e1a:sinSum}, except using a cosinusoidal input, we can
calculate the following result: If the response to a cosinusoid at $f$
is a sum of cosinusoid and sinusoid with weights, $(a_f, b_f)$, then,
the response to a sinusoid at frequency $f$ will have weights $(b_f,
-a_f)$.  Hence, if we know the response to a cosinusoid at $f$, we
also know the response to a sinusoid at $f$.

Next, we can use our knowledge of the response to sinusoids and
cosinusoid at $f$ to predict the response to any harmonic function at
$f$.  Suppose that the input is a harmonic function $a \CfNi + b
\SfNi$, and the output is, say $a^\prime \CfNi + b^\prime
\SfNi$.  If the response to a unit amplitude
cosinusoid is $u_f \CfNi + v_f \SfNi$, then the response to a unit
amplitude sinusoid is $v_f \CfNi - u_f \SfNi$.  Using these two facts
and linearity we calculate the coefficients of the response,
\begin{eqnarray}
\label{e1a:system}
a^\prime & = & a  u_f + b v_f  \nonumber \\
b^\prime & = & a  v_f - b u_f .
\end{eqnarray}

We have shown that if we measure the system response to unit amplitude
cosinusoidal inputs, we can compute the system response to an
arbitrary input stimulus as follows.
\be
\item 
Compute the DFS coefficients of the input stimulus using
Equation~\ref{e1a:dfsCalcCoef}.

\item 
Calculate the DFS coefficients of the output using
Equation~\ref{e1a:system}.

\item 
Reconstruct the output from the coefficients using
Equation~\ref{e1a:DFS}.

\ee
You will find this series of calculations used implicitly at several
points in the text.  For example, we followed this organization when I
described measurements of the optical quality of the lens
(Chapter~\ref{chapter:imageformation}) and when I described
measurements of behavioral sensitivity to spatiotemporal patterns
(Chapter~\ref{chapter:space}).
