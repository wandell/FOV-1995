<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>9&nbsp; Color – Foundations of Vision (1995)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter-10-motion-and-depth.html" rel="next">
<link href="./part-3-image-interpretation.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./part-3-image-interpretation.html">Image Interpretation</a></li><li class="breadcrumb-item"><a href="./chapter-9-color.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Color</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Foundations of Vision (1995)</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">How to study vision</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Image Encoding</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./part-1-image-encoding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction to Image Encoding</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-2-image-formation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Image Formation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-3-the-photoreceptor-mosaic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Photoreceptor Mosaic</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-4-wavelength-encoding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Wavelength Encoding</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Image Representation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./part-2-image-representation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction to Image Representation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-5-the-retinal-representation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Retina</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-6-the-cortical-representation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Cortical Representation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-7-pattern-sensitivity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Pattern Vision</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-8-multiresolution-image-representations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Multiresolution Representations</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Image Interpretation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./part-3-image-interpretation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction to Image Interepretation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-9-color.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Color</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-10-motion-and-depth.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Motion and Depth</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter-11-seeing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Seeing</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendix</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Appendix</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./numbers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Useful numbers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./online-teaching-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Online Teaching Resources</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#color-overview" id="toc-color-overview" class="nav-link active" data-scroll-target="#color-overview"><span class="header-section-number">9.1</span> Color overview</a></li>
  <li><a href="#color-constancy-theory" id="toc-color-constancy-theory" class="nav-link" data-scroll-target="#color-constancy-theory"><span class="header-section-number">9.2</span> Color Constancy: Theory</a></li>
  <li><a href="#spectral-image-formation" id="toc-spectral-image-formation" class="nav-link" data-scroll-target="#spectral-image-formation"><span class="header-section-number">9.3</span> Spectral Image Formation</a>
  <ul class="collapse">
  <li><a href="#surface-reflectance-estimation" id="toc-surface-reflectance-estimation" class="nav-link" data-scroll-target="#surface-reflectance-estimation">Surface reflectance estimation</a></li>
  <li><a href="#linear-models" id="toc-linear-models" class="nav-link" data-scroll-target="#linear-models">Linear Models</a></li>
  <li><a href="#simple-illuminant-estimation" id="toc-simple-illuminant-estimation" class="nav-link" data-scroll-target="#simple-illuminant-estimation">Simple Illuminant Estimation</a></li>
  <li><a href="#surface-reflectance-models" id="toc-surface-reflectance-models" class="nav-link" data-scroll-target="#surface-reflectance-models">Surface Reflectance Models</a></li>
  <li><a href="#sensor-based-error-measures" id="toc-sensor-based-error-measures" class="nav-link" data-scroll-target="#sensor-based-error-measures">Sensor-based error measures</a></li>
  <li><a href="#surface-and-illuminant-estimation-algorithms" id="toc-surface-and-illuminant-estimation-algorithms" class="nav-link" data-scroll-target="#surface-and-illuminant-estimation-algorithms">Surface and Illuminant Estimation Algorithms</a></li>
  <li><a href="#illuminant-correction-an-example-calculation" id="toc-illuminant-correction-an-example-calculation" class="nav-link" data-scroll-target="#illuminant-correction-an-example-calculation">Illuminant correction: An example calculation</a></li>
  </ul></li>
  <li><a href="#color-constancy-experiments" id="toc-color-constancy-experiments" class="nav-link" data-scroll-target="#color-constancy-experiments"><span class="header-section-number">9.4</span> Color Constancy: Experiments</a>
  <ul class="collapse">
  <li><a href="#asymmetric-color-matching-experiments" id="toc-asymmetric-color-matching-experiments" class="nav-link" data-scroll-target="#asymmetric-color-matching-experiments">Asymmetric Color-matching Experiments</a></li>
  <li><a href="#the-linearity-of-asymmetric-color-matches" id="toc-the-linearity-of-asymmetric-color-matches" class="nav-link" data-scroll-target="#the-linearity-of-asymmetric-color-matches">The linearity of asymmetric color-matches</a></li>
  <li><a href="#von-kries-coefficient-law-experiments" id="toc-von-kries-coefficient-law-experiments" class="nav-link" data-scroll-target="#von-kries-coefficient-law-experiments">Von Kries Coefficient Law: Experiments</a></li>
  <li><a href="#how-color-constant-are-we" id="toc-how-color-constant-are-we" class="nav-link" data-scroll-target="#how-color-constant-are-we">How Color Constant Are We?</a></li>
  </ul></li>
  <li><a href="#the-perceptual-organization-of-color" id="toc-the-perceptual-organization-of-color" class="nav-link" data-scroll-target="#the-perceptual-organization-of-color"><span class="header-section-number">9.5</span> The Perceptual Organization of Color</a>
  <ul class="collapse">
  <li><a href="#opponent-colors" id="toc-opponent-colors" class="nav-link" data-scroll-target="#opponent-colors">Opponent-Colors</a></li>
  <li><a href="#hue-cancellation" id="toc-hue-cancellation" class="nav-link" data-scroll-target="#hue-cancellation">Hue Cancellation</a></li>
  <li><a href="#opponent-colors-measurements-at-threshold" id="toc-opponent-colors-measurements-at-threshold" class="nav-link" data-scroll-target="#opponent-colors-measurements-at-threshold">Opponent-colors measurements at threshold</a></li>
  <li><a href="#opponent-signals-in-the-visual-pathways" id="toc-opponent-signals-in-the-visual-pathways" class="nav-link" data-scroll-target="#opponent-signals-in-the-visual-pathways">Opponent Signals in the Visual Pathways</a></li>
  <li><a href="#decorrelation-of-the-cone-absorptions" id="toc-decorrelation-of-the-cone-absorptions" class="nav-link" data-scroll-target="#decorrelation-of-the-cone-absorptions">Decorrelation of the Cone Absorptions</a></li>
  <li><a href="#spatial-pattern-and-color" id="toc-spatial-pattern-and-color" class="nav-link" data-scroll-target="#spatial-pattern-and-color">Spatial Pattern and Color</a></li>
  </ul></li>
  <li><a href="#the-cortical-basis-of-color-appearance" id="toc-the-cortical-basis-of-color-appearance" class="nav-link" data-scroll-target="#the-cortical-basis-of-color-appearance"><span class="header-section-number">9.6</span> The Cortical Basis of Color Appearance</a>
  <ul class="collapse">
  <li><a href="#clinical-studies" id="toc-clinical-studies" class="nav-link" data-scroll-target="#clinical-studies">Clinical studies</a></li>
  <li><a href="#congenital-monochromacy" id="toc-congenital-monochromacy" class="nav-link" data-scroll-target="#congenital-monochromacy">Congenital Monochromacy</a></li>
  <li><a href="#regularities-of-the-cerebral-dyschromatopsia-syndrome" id="toc-regularities-of-the-cerebral-dyschromatopsia-syndrome" class="nav-link" data-scroll-target="#regularities-of-the-cerebral-dyschromatopsia-syndrome">Regularities of the Cerebral Dyschromatopsia Syndrome</a></li>
  <li><a href="#behavioral-studies-of-patients-with-cerebral-dyschromatopsia" id="toc-behavioral-studies-of-patients-with-cerebral-dyschromatopsia" class="nav-link" data-scroll-target="#behavioral-studies-of-patients-with-cerebral-dyschromatopsia">Behavioral studies of patients with cerebral dyschromatopsia</a></li>
  <li><a href="#how-many-cone-types-are-functional" id="toc-how-many-cone-types-are-functional" class="nav-link" data-scroll-target="#how-many-cone-types-are-functional">How many cone types are functional?</a></li>
  <li><a href="#physiological-studies-of-color-appearance" id="toc-physiological-studies-of-color-appearance" class="nav-link" data-scroll-target="#physiological-studies-of-color-appearance">Physiological studies of color appearance</a></li>
  <li><a href="#reasoning-about-cortex-and-perception" id="toc-reasoning-about-cortex-and-perception" class="nav-link" data-scroll-target="#reasoning-about-cortex-and-perception">Reasoning about Cortex and Perception</a></li>
  </ul></li>
  <li><a href="#summary-and-conclusions" id="toc-summary-and-conclusions" class="nav-link" data-scroll-target="#summary-and-conclusions"><span class="header-section-number">9.7</span> Summary and Conclusions</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./part-3-image-interpretation.html">Image Interpretation</a></li><li class="breadcrumb-item"><a href="./chapter-9-color.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Color</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-color" class="quarto-section-identifier"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Color</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="color-overview" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="color-overview"><span class="header-section-number">9.1</span> Color overview</h2>
<p>Edwin Land was one of the great inventors and entrepreneurs in US history; he created instant developing film and founded the Polaroid Corporation. The first instant developing film made black and white reproductions, and after a few years Land decided to create a color version of the film. In order to learn about color appearance, Land returned to his laboratory to experiment with color. He was so surprised by his observations that he decided to write a paper summarizing his observations. In a paper published in the Proceedings of the National Academy of Sciences (USA), Land startled many people by arguing that there are only two, not three, types of cones. He further went on to dismiss the significance of the color-matching experiments. He wrote: “We have come to the conclusion that the classical laws of color mixing conceal great basic laws of color vision (<span class="citation" data-cites="land1959-pnas">Land (<a href="references.html#ref-land1959-pnas" role="doc-biblioref">1959</a>)</span>).” Land’s sharp words, an arrow aimed at the heart of color science, provoked heated rejoinders from two leading scientists, <span class="citation" data-cites="judd1960-land">Judd (<a href="references.html#ref-judd1960-land" role="doc-biblioref">1960</a>)</span> and <span class="citation" data-cites="walls1960-landland">Walls (<a href="references.html#ref-walls1960-landland" role="doc-biblioref">1960</a>)</span>.</p>
<p>What was it that Land, a brilliant man, found so objectionable about color-matching? It seems to me that Land’s reading of the literature led him to believe that the curves we measure in the color-matching experiment can be used to predict color appearance. When he set the textbooks down and began to experiment with color, he was sorely disappointed. He found that the color-matching measurements do not answer many important questions about color appearance.</p>
<p>Land’s observation is consistent with our review of color-matching in <a href="chapter-4-wavelength-encoding.html" class="quarto-xref"><span>Chapter 4</span></a>. The results from the color-matching experiment can be used to predict when two lights will look the same, but they cannot be used to tell us what the two lights look like. As Color Plate 2 (Albers) illustrates, the experimental results in the color-matching experiment can be explained by the matches between the cone photopigment absorptions at a point, while color appearance forces us to think about the pattern of photopigment absorptions spread across the cone mosaics. <a href="#fig-contrast-illusion" class="quarto-xref">Figure&nbsp;<span>9.1</span></a> illustrates this point again. The two squares in the image reflect the same amount of light to your eye. Yet, because the squares are embedded in different surroundings, we interpret the squares very differently, seeing one as light and the other as dark.</p>
<div id="fig-contrast-illusion" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-contrast-illusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./wp-content/uploads/2012/02/contrast1.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-contrast-illusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.1: Color appearance in a region depends on the spatial pattern of cone absorptions, not just the absorptions within the region. The two square regions are physically identical and thus create the same local rate of photopigment absorption. Yet, they appear to have different lightness because of the difference in their relative absorptions compared to nearby areas.
</figcaption>
</figure>
</div>
<p>Land’s paper contained a set of interesting qualitative demonstrations that illustrate these same points. While the limitations of the color-matching experiment were new to Land and the reviewers of his paper, they were not new to most color scientists. For example, <span class="citation" data-cites="judd1940">Judd (<a href="references.html#ref-judd1940" role="doc-biblioref">1940</a>)</span> had worked for years trying to understand these effects. Later in this chapter I will review work at Kodak and in academic laboratories, contemporaneous with that of Land, that was designed to elucidate the mechanisms of color appearance. This episode in the history of color science remains important, however, because it reminds us that the phenomena of color appearance are very significant and very compelling, enough so to motivate Edwin Land to challenge whether the color establishment had answered the right questions. As to Land’s additional and extraordinary claim in those papers, that there are two not three types of photoreceptors, well, we all have off days<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
</section>
<section id="color-constancy-theory" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="color-constancy-theory"><span class="header-section-number">9.2</span> Color Constancy: Theory</h2>
<p>If the absolute rates of photopigment absorptions don’t explain color appearance, what does? The illusions in Color Plate 2 (Albers) and <a href="#fig-contrast-illusion" class="quarto-xref">Figure&nbsp;<span>9.1</span></a> both suggest that color appearance is related to the <em>relative</em> cone absorption rates. Within an image, bright objects generate more cone absorptions than dark objects; red objects create more <span class="math inline">\(L\)</span> cone absorptions and blue objects more <span class="math inline">\(S\)</span> cone absorptions. Hence, one square in <a href="#fig-contrast-illusion" class="quarto-xref">Figure&nbsp;<span>9.1</span></a> appears light because it is associated with more cone absorptions than its neighboring region, while the other appears dark because it is associated with fewer cone absorptions. The relative absorption rate is very closely connected to the idea of the stimulus contrast that has been so important in this book. Color appearance depends more on the local contrast of the cone absorptions than on the absolute level of cone absorptions.</p>
<p>The dependence on relative, rather than absolute, absorption rates is a general phenomenon, not something that is restricted to a few textbook illusions. Consider a simple thought experiment that illustrates the generality of the phenomenon. Suppose you read this book indoors. The white part of the page reflects about 90 percent of the light towards your eye, while the black ink reflects only about 2 percent. Hence, if the ambient illumination inside a reading room is 100 units, the white paper reflects 90 units and the black ink 2 units. When you take the book outside, the illumination level can be 100 times greater, or 10,000 units. Outside the black ink reflects 200 units towards your eye, which far exceeds the level of the white paper when you were indoors. Yet, the ink continues to look black. As we walk about the environment, then, we must constantly be inferring the lightness and color of objects by comparing the spatial pattern of cone absorptions<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p>This thought experiment also illustrates us that the color we perceive informs us mainly about objects. The neural computation of color is structured so that objects retain their color appearance whether we encounter them in shade or sun. When the color appearance of an object changes, we think that the object itself has changed. The defining property of an object is not the absolute amount of light it reflects, but rather how much light it reflects relative to other objects. From our thought experiment, it follows that the color of an object imaged at a point on the retina should be inferred from the relative level of cone absorptions caused by an object. To compute the relative level of cone absorptions, we must take into account the spatial pattern of cone absorptions, not just the cone absorptions at a single point.</p>
<p>On this view, color appearance is a mental explanation of why an object causes relatively more absorptions in one cone type than another object. The physical attribute of an object that describes how well the object reflects light at different wavelengths is called the object’s <em>surface reflectance</em>. Generally, objects that reflect light mainly in the long-wavelength part of the spectrum usually appear red; objects that reflect mainly short-wavelength light usually appear blue. Yet, as we shall explore in the next few pages, interpreting the cone absorption rates in terms of the surface reflectance functions is not trivial. How the nervous system makes this interpretation is an essential question in color appearance. A natural place to begin our analysis of color appearance, then, is with the question: how can the central nervous system can infer an object’s surface reflectance function from the mosaic of cone absorptions?</p>
</section>
<section id="spectral-image-formation" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="spectral-image-formation"><span class="header-section-number">9.3</span> Spectral Image Formation</h2>
<p>To understand the process of inferring surface reflectance from the light incident at our eyes, we must understand a little about how images are formed. The light incident at our corneas and absorbed by our cones depends in part on the properties of the objects that reflect the light and in part on the wavelength composition of the ambient illumination. We must understand each of these components, and how they fit together, to see what information we might extract from the retinal image about the surface reflectance function. A very simple description of the imaging process is shown in <a href="#fig-spectral-image-formation" class="quarto-xref">Figure&nbsp;<span>9.2</span></a> (a).</p>
<div id="fig-spectral-image-formation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-spectral-image-formation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./wp-content/uploads/2012/02/imaging-1024x651.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-spectral-image-formation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.2: A description of spectral image formation. (a) Light from a source arrives at a surface and is reflected towards an observer. The light at the observer is absorbed by the cones and ultimately leads to a perception of color. (b) The functions associated with the imaging process include the spectral power distribution of the light source, the surface reflectance function of the object, the result of multiplying these two functions to create the color signal incident at the eye, and the cone absorptions caused by the incident signal.
</figcaption>
</figure>
</div>
<p>Ordinarily, image formation begins with a light source. We can describe the spectral properties of the light source in terms of the relative amount of energy emitted at each wavelength, namely the <em>spectral power distribution</em> of the light source (see <a href="chapter-4-wavelength-encoding.html" class="quarto-xref"><span>Chapter 4</span></a>). The light from the source is either absorbed by the surface or reflected. The fraction of the light reflected by the surface defines the <em>surface reflectance</em> function. As a first approximation, we can calculate the light reflected towards the eye by multiplying the spectral power distribution and the surface reflectance function together (<a href="#fig-spectral-image-formation" class="quarto-xref">Figure&nbsp;<span>9.2</span></a> (b)). We will call this light the <em>color signal</em> because it serves as the signal that ultimately leads to the experience of color. The color signal leads to different amounts of absorptions in the three cone classes, and the interpretation of these cone absorptions by the nervous system is the basis of our color perception.</p>
<div id="fig-surface-reflectance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-surface-reflectance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./wp-content/uploads/2012/02/surfR1.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-surface-reflectance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.3: The surface reflectance function measures the proportion of light scattered from a surface at each wavelength. The panels show the surface reflectance functions of various colored papers along with the color name associated with the paper. Surface reflectance correlates with the color appearance; as Newton wrote “colors in the object are nothing but a disposition to reflect this or that sort of ray more copiously than the rest.”
</figcaption>
</figure>
</div>
<p>In <a href="chapter-4-wavelength-encoding.html" class="quarto-xref"><span>Chapter 4</span></a> I reviewed some properties of illuminants and sensors. But, this is the first time we have considered the surface reflectance function; it is worth spending time thinking about some of the properties of how surfaces reflect light. <a href="#fig-surface-reflectance" class="quarto-xref">Figure&nbsp;<span>9.3</span></a> shows the reflectance function of four matte papers, that is papers with a smooth even surface free from shine or highlights. Because these curves describe the fraction of light reflected, they range between zero and one. While it is common to refer to an object as having a surface reflectance function, as I have just done, in a certain sense, the notion of a surface reflectance function is a ruse. If you look about the room you are in, you will probably see some surfaces that are shiny or glossy. As you move around these surfaces, changing the geometrical relationship between yourself, the lighting, and the surface, the light reflected to your eye changes considerably. Hence, the tendency of the surface to reflect light towards your eye does not depend only on the surface; the light scattered to your eye also depends on the viewing geometry, too<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>The surface reflectance dependence on viewing geometry because the reflected light arises from several different physical processes that occur when light is incident at the surface. Each of these processes contributes simultaneously to the light scattered from a surface, and each has its own unique properties. The full model describing reflectance appears to be be complex; but, <span class="citation" data-cites="shafer1985">Shafer (<a href="references.html#ref-shafer1985" role="doc-biblioref">1985</a>)</span> has created a simple approximation of the reflection process, called the <em>dichromatic reflection model</em>, that captures several important features of surface reflectance. <a href="#fig-dichromatic-reflection" class="quarto-xref">Figure&nbsp;<span>9.4</span></a> (a) sketches the model, which applies to a broad collection of materials called <em>dielectric</em> surfaces<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> (<span class="citation" data-cites="klinker1988-highlights">Klinker et al. (<a href="references.html#ref-klinker1988-highlights" role="doc-biblioref">1988</a>)</span>; <span class="citation" data-cites="shafer1985">Shafer (<a href="references.html#ref-shafer1985" role="doc-biblioref">1985</a>)</span>; <span class="citation" data-cites="nayar1993-reflectance">Nayar and Bolle (<a href="references.html#ref-nayar1993-reflectance" role="doc-biblioref">1993</a>)</span>, <span class="citation" data-cites="wolff1994-relative">Wolff (<a href="references.html#ref-wolff1994-relative" role="doc-biblioref">1994</a>)</span>).</p>
<p>According to the dichromatic reflection model, dielectric material consists of a clear substrate with embedded colorant particles. One way light is scattered from the surface is by a mirror-like reflection at the interface of the surface. This process is called <em>interface</em> reflections. A second scattering process takes place when the rays enter the material. These rays are reflected randomly between the colorant particles. A fraction of the incident light is absorbed by the material, heating it up, and part of the light emerges. This process is called <em>body</em> reflection.</p>
<p>The spatial distributions of light scattered by these two mechanisms are quite different (<a href="#fig-dichromatic-reflection" class="quarto-xref">Figure&nbsp;<span>9.4</span></a> (b)). Light scattered by interface reflection is quite restricted in angle, much as a mirror reflects incident rays. Conversely, the light scattered by body reflection emerges equally in all directions. When a surface has no interface reflections, but only body reflections, it is called a <em>matte</em> or <em>Lambertian</em> surface. Interface reflection is commonly called <em>specular</em> reflection and is the reason why some objects appear <em>glossy</em>.</p>
<div id="fig-dichromatic-reflection" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dichromatic-reflection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./wp-content/uploads/2012/02/refProcess1-1024x874.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dichromatic-reflection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.4: The dichromatic reflection model of surface reflectance in inhomogeneous materials. (a) Light is scattered from a surface by two different mechanisms. Some incident light is reflected at the interface (interface reflection). Other light enters the material, interacts with the embedded particles, and then emerges as reflected light (body reflection). (b) Rays of light reflected by interface reflections is likely to be concentrated in one direction. Rays of light reflected by body reflection are reflected with nearly equal likelihood in many different directions. Because interface reflections are concentrated in certain directions, light reflected by this process can be much more intense than light reflected by body reflection.
</figcaption>
</figure>
</div>
<p>The different geometrical distribution in how body and interface reflections are reflected is the reason why specular highlights on a surface appear much brighter than diffuse reflection. Nearly all of the specular scattering is confined to a small angle; the body reflection is divided among many directions. The interface reflections provide a strong signal, but because they can only be seen from certain angles they are not a reliable source of information. As the object and observer change their geometric relationship the specular highlight moves along the surface of the object, or it may disappear altogether.</p>
<p>For many types of materials, interface reflection is not selective for wavelength. The spectral power distribution of the light scattered at the interface is the same as the spectral power distribution of the incident light. This is the reason specular highlights take on the color of the illumination source. Body reflection, on the other hand, does not return all wavelengths uniformly. The particles in the medium absorb light selectively, and it is this property of the material that distinguishes objects in terms of their color appearance. Ordinarily, when people refer to the surface reflectance of an object, they mean to refer to the body reflection of the object.</p>
<div id="fig-color-signal" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-color-signal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./wp-content/uploads/2012/02/colorSig.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-color-signal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.5: The light reflected from objects changes as the illuminant changes. (a) The shaded panel on the left shows the spectral power distributions of a light source similar to a tungsten bulb. The three graphs on the right show the light reflected from the red, green and yellow papers in <a href="#fig-surface-reflectance" class="quarto-xref">Figure&nbsp;<span>9.3</span></a> when illuminated by this source. The bar plots above the graphs show the three cone absorption rates caused by the color signal. (b) When the light source is similar to the blue sky, as in the shaded panel on the left, the light reflected from the same papers is quite different. These change considerably, too, and are thus an unreliable cue to the surface reflectance of the object.
</figcaption>
</figure>
</div>
<p>We can describe the reflection of light by a matte surface with a simple mathematical formula. Suppose that the illuminant spectral power distribution is $ e() $. We suppose that the body reflectance is $s() $. Then the color signal, that is the light arriving at the eye, is</p>
<p><span id="eq-color-signal"><span class="math display">\[
c(\lambda) = s(\lambda) e(\lambda) .
\tag{9.1}\]</span></span></p>
<p><a href="#fig-color-signal" class="quarto-xref">Figure&nbsp;<span>9.5</span></a> shows several examples of the light reflected from matte surfaces. The shaded graph in <a href="#fig-color-signal" class="quarto-xref">Figure&nbsp;<span>9.5</span></a> (a) is the spectral power distribution of an illuminant similar to a tungsten bulb. The other panels show the spectral power distribution of light that would be reflected from the red, green and yellow papers in <a href="#fig-surface-reflectance" class="quarto-xref">Figure&nbsp;<span>9.3</span></a>. The shaded graph in <a href="#fig-color-signal" class="quarto-xref">Figure&nbsp;<span>9.5</span></a> (b) shows the spectral power distribution similar to blue sky illumination and the light reflected from the same three papers. Plainly, the spectral composition of the reflected light changes when the illuminant changes.</p>
<p>We can calculate the cone absorption rates caused by each of these color signals (see <a href="chapter-4-wavelength-encoding.html" class="quarto-xref"><span>Chapter 4</span></a>). These rates are shown in the bar plots inset within the individual graphs of reflected light. By comparing the insets in the top and bottom, you can see that the cone absorption rates from each surface changes dramatically when the illumination changes. This observation defines a central problem in understanding color appearance. Color is a property of objects; but, the reflected light, and thus the cone absorptions, varies with the illumination. If color must describe a property of an object, the nervous system must <em>interpret</em> the mosaic of photopigment absorptions and estimate something about the surface reflectance function. This is an estimation problem. How can the nervous system use the information in the cone absorptions to infer the surface reflectance function?</p>
<section id="surface-reflectance-estimation" class="level3">
<h3 class="anchored" data-anchor-id="surface-reflectance-estimation">Surface reflectance estimation</h3>
<p>There are some very strong limitations on what we can achieve when we set out to estimate surface reflectance from cone absorptions. First, notice that the color signal depends on two spectral functions that are continuous functions of wavelength: the spectral power distribution of the ambient illumination and the surface reflectance function. The light incident at the eye is the product of these two functions. So, any illuminant and surface combination that produces this same light will be indistinguishable to the eye.</p>
<p>One easy mathematical way to see why this is so is to consider the color signal. Recall that the color signal is equal to the product of the illuminant spectral power distribution $ e() $ and the surface reflectance function $ s() $,</p>
<p><span id="eq-color-signal-example"><span class="math display">\[
c(\lambda) = s(\lambda) e(\lambda) .
\tag{9.2}\]</span></span></p>
<p>Suppose we replace the illuminant with a new illuminant, $ f() e() $ and all of the surfaces with new functions $ s() / f() $. This change has no effect on the color signal,</p>
<p><span id="eq-color-signal-invariance"><span class="math display">\[
c(\lambda) = \frac{s(\lambda)}{f(\lambda)} f(\lambda) e(\lambda) = s(\lambda) e(\lambda)
\tag{9.3}\]</span></span></p>
<p>and thus no effect on the photopigment absorption rates. Hence, there is no way the visual system can discriminate between these two illuminant and surface pairs.</p>
<p>Now, consider a second limitation to the estimation problem. The visual system does not measure the spectral power distribution directly. Rather, the visual system only encodes the absorption rates of the three different cones. Hence, the nervous system cannot be certain which of the many metameric spectral power distributions is responsible for causing the observed cone absorption rates (see <a href="chapter-4-wavelength-encoding.html" class="quarto-xref"><span>Chapter 4</span></a>) for a definition of metameric). This creates even more uncertainty for the estimation problem.</p>
<p>In the introduction to this part of the book, I quoted Helmholtz’ suggestion: the visual system imagines those objects being present that could give rise to the retinal image. We now find that the difficulty we have in following this advice is not that are no solutions, but rather that there are too many. We encode so little about the color signal that many different objects could all have given rise to the retinal image.</p>
<p>Which of the many possible solutions should we select? The general strategy we should adopt is straightforward: Pick the most likely one. Will this be a helpful estimate, or are there so many likely signals that encoding the most likely one is hardly better than guessing with no information?</p>
<p>Perhaps the most important point that we have learned from color constancy calculations over the last ten years is this: the set of surface and illuminant functions we encounter is not so diverse as to make estimation from the cone catches useless. Some surface reflectance functions and some illuminants are much more likely than others, even with only three types of cones, it is possible to make educated guesses that do more good than harm.</p>
<p>The surface reflectance estimation algorithms we will review are all based on this principle. They differ only in the set of assumptions they make concerning what the observer knows and what we mean by most likely. I review them now because some of the tools are useful and interesting, and some of the summaries of the data are very helpful in practical calculations and experiments.</p>
</section>
<section id="linear-models" class="level3">
<h3 class="anchored" data-anchor-id="linear-models">Linear Models</h3>
<p>To estimate which lights and surfaces are more probable, we need to do two things. First, we need to measure the spectral data from lights and surfaces. Second, we need a way to represent the likelihood of observing different surface and illuminant functions.</p>
<p>Since the early part of the 1980s, <em>linear models</em> of surface and illuminant functions have been used widely to represent our best guess about the most likely surface and illuminant functions. A linear model of a set of spectral functions, such as surface reflectances, is a method of efficiently approximating the measurements. There are several ways to build linear models, including <em>principal components analysis</em>, <em>centroid analysis</em>, or <em>one mode analysis</em>. These methods have much in common, but they differ slightly in their linear model formulation and error measures (<span class="citation" data-cites="cohen1964">Cohen (<a href="references.html#ref-cohen1964" role="doc-biblioref">1964</a>)</span>; <span class="citation" data-cites="judd1964-daylight">Judd et al. (<a href="references.html#ref-judd1964-daylight" role="doc-biblioref">1964</a>)</span>; <span class="citation" data-cites="maloney1986-linearmodels-surface">Maloney (<a href="references.html#ref-maloney1986-linearmodels-surface" role="doc-biblioref">1986</a>)</span>; <span class="citation" data-cites="marimont1992-linearmodelssurface">Marimont and Wandell (<a href="references.html#ref-marimont1992-linearmodelssurface" role="doc-biblioref">1992</a>)</span>).</p>
<p>As an example of how to build a linear model, we will review a classic paper by <span class="citation" data-cites="judd1964-daylight">Judd et al. (<a href="references.html#ref-judd1964-daylight" role="doc-biblioref">1964</a>)</span>. These authors built a linear model of an important source of illumination, daylight spectral power distributions. They collected more than six hundred measurements of the spectral power distribution of daylights at different times of day and under different weather conditions and on different continents. To measure the spectral power distribution of daylight we place an object with a known reflectance outside. It is common to use blocks of pressed magnesium oxide as a standard object because such blocks reflect light of all wavelengths nearly equally. Moreover, the material is essentially a pure diffuser: a quantum of light incident on the surface from any angle is reflected back with equal probability in all other directions above the surface.</p>
<p>Since they were interested in the relative spectral composition, not the absolute level, Judd et al.&nbsp;normalized their measurements so that they were all equal to the value 100 at 560nm. Their main interest was in the wavelength regime visible to the human eye, so they made measurements roughly from 400nm to 700nm. Their measurements were spaced every 10 nm. Hence, they could represent each daylight measurement by a set of thirty-one numbers. Three example daylight spectral power distributions, normalized to coincide at 560nm, are shown in <a href="#fig-daylight-spectra" class="quarto-xref">Figure&nbsp;<span>9.6</span></a>.</p>
<div id="fig-daylight-spectra" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-daylight-spectra-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./wp-content/uploads/2012/02/illExample.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-daylight-spectra-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.6: The relative spectral power distribution of three typical daylights. The curves drawn here are typical daylight measured by <span class="citation" data-cites="judd1964-daylight">Judd et al. (<a href="references.html#ref-judd1964-daylight" role="doc-biblioref">1964</a>)</span>. The curves are normalized to coincide at 560nm (Source: <span class="citation" data-cites="judd1964-daylight">Judd et al. (<a href="references.html#ref-judd1964-daylight" role="doc-biblioref">1964</a>)</span>).
</figcaption>
</figure>
</div>
<p>The data plotted in <a href="#fig-daylight-spectra" class="quarto-xref">Figure&nbsp;<span>9.6</span></a> show that measured daylight relative spectral power distributions can differ depending on the time of day and the weather conditions. But, after examining many daylight functions, Judd et al.&nbsp;found that the curves do not vary wildly and unpredictably; the data are fairly regular. Judd et al.&nbsp;captured the regularities in the data by building a linear model of the observed spectral power distributions. They designed their linear model, a <em>principal components model</em>, using the following logic.</p>
<p>First, they decided that they wanted to approximate their observations in the <em>squared-error sense</em>. That is, suppose <span class="math inline">\(e(\lambda)\)</span> is a measurement, and <span class="math inline">\(\hat{e}(\lambda)\)</span> is the linear model estimate of the measurement. Then, they decided to select the approximation in order to minimize the <em>squared error</em> <span class="math display">\[
\sum_{\lambda} (e(\lambda) - \hat{e}(\lambda) )^2 .
\]</span></p>
<p>When we consider the collection of observations as a whole, the function that approximates the entire data set with the smallest squared error is the mean. The mean observation from Judd et al.’s data set, $ e_{0}() $, is the bold curve in <a href="#fig-linear-model-daylight" class="quarto-xref">Figure&nbsp;<span>9.7</span></a>.</p>
<p>Once we know the mean, we need only to approximate the difference between the mean and each individual measurement. We build the linear model to explain these differences as follows. First, we select a fixed set of <em>basis functions</em>. Basis functions, like the mean, are descriptions of the measurements. We approximate a measurement’s difference from the mean as the weighted sum of the basis functions. For example, suppose <span class="math inline">\(\Delta e_{j}(\lambda)\)</span> is the difference between the <span class="math inline">\(j^{th}\)</span> daylight measurement and the mean daylight. Further, suppose we select a set of <span class="math inline">\(N\)</span> basis functions, <span class="math inline">\(E_{i}(\lambda)\)</span>. Then, we approximate the differences from the mean as the weighted sum of these basis functions, namely</p>
<p><span id="eq-linear-model-diff"><span class="math display">\[
\Delta e_{j}(\lambda) \approx \sum_{i=1}^{i=N} w_{i} E_{i}(\lambda) .
\tag{9.4}\]</span></span></p>
<p>The basis functions are chosen to make the sum of the squared errors between the <em>collection</em> of measurements and their approximations as small as possible. The number of basis functions, <span class="math inline">\(N\)</span>, is called the <em>dimension</em> of the linear model. The values <span class="math inline">\(w_{i}\)</span> are called the linear model <em>weights</em>, or <em>coefficients</em>. They are chosen to make the squared error between an <em>individual</em> measurement and its approximation as small as possible. They serve to describe the properties of the specific measurement.</p>
<p>As the dimension of the linear model increases, the precision of the linear model approximation improves. The dimension one chooses for an application depends on the required precision<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
<p>Judd et al.&nbsp;found an excellent approximation of the daylight measurements using the mean and two basis functions. The linear model representation expresses the measurements efficiently. The mean and two basis functions are fixed, side conditions of the model. Their linear model approximation of each measurement uses only two weights. The empirical results have been confirmed by other investigators and the results have been adopted by the international color standards organization to create a model of daylights (<span class="citation" data-cites="dixon1978-daylight">Dixon (<a href="references.html#ref-dixon1978-daylight" role="doc-biblioref">1978</a>)</span>; <span class="citation" data-cites="sastri1966-spectral">Sastri and Das (<a href="references.html#ref-sastri1966-spectral" role="doc-biblioref">1966</a>)</span>). The mean and two basis terms from the international standard are plotted in <a href="#fig-linear-model-daylight" class="quarto-xref">Figure&nbsp;<span>9.7</span></a>.</p>
<div id="fig-linear-model-daylight" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-linear-model-daylight-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./wp-content/uploads/2012/02/linearModDay1.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-linear-model-daylight-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.7: A linear model for daylight spectral power distributions. The curve labeled mean is the mean spectral power distribution of a set of daylights whose spectral power distributions were normalized to a value of 100 at 560nm. The curves labeled 1st and 2nd show the two basis curves used to define a linear model of daylights. By adding together the mean and weighted sums of the two basis functions, one can generate examples of typical relative spectral power distributions of daylight. (Source: <span class="citation" data-cites="judd1964-daylight">Judd et al. (<a href="references.html#ref-judd1964-daylight" role="doc-biblioref">1964</a>)</span>).
</figcaption>
</figure>
</div>
<p>Because daylights vary in their absolute spectral power distributions, not just their relative distributions, we should extend Judd et al.’s linear model to a three-dimensional linear model that includes absolute intensity. A three dimensional linear model we might use consists of the mean and the two derived curves. In this case the three-dimensional linear model approximation becomes<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<p><span id="eq-illuminant-linear-model"><span class="math display">\[
e(\lambda) = \sum_{i=1}^{3} w_{i} E_{i}(\lambda) .
\tag{9.5}\]</span></span></p>
<p>We can express the linear model in <a href="#eq-illuminant-linear-model" class="quarto-xref">Equation&nbsp;<span>9.5</span></a> as a matrix equation, <span class="math inline">\(\mathbf{e} = \mathbf{B}_{\mathbf{e}} \mathbf{w}\)</span> in which <span class="math inline">\(\mathbf{e}\)</span> is a vector representing the illuminant spectral power distribution, the three columns of <span class="math inline">\(\mathbf{B}_{\mathbf{e}}\)</span> contain the basis functions <span class="math inline">\(E_{i}(\lambda)\)</span>, and <span class="math inline">\(\mathbf{w}\)</span> is a three dimensional vector containing the linear model coefficients <span class="math inline">\(w_{i}\)</span>.</p>
<p>We can see why linear models are efficient by writing <a href="#eq-illuminant-linear-model" class="quarto-xref">Equation&nbsp;<span>9.5</span></a> as a matrix tableau.</p>
<p><span class="math display">\[
\left (
    \begin{array}{ccc}
        &amp; . &amp; \\
        &amp; . &amp; \\
        &amp; . &amp; \\
        &amp; e(\lambda) &amp; \\
        &amp; . &amp; \\
        &amp; . &amp; \\
        &amp; . &amp; \\
    \end{array}
\right )
=
\left (
    \begin{array}{ccc}
        . &amp; . &amp; . \\
        . &amp; . &amp; . \\
        . &amp; . &amp; . \\
        E_{1}(\lambda) &amp; E_{2}(\lambda) &amp; E_{3}(\lambda) \\
        . &amp; . &amp; . \\
        . &amp; . &amp; . \\
        . &amp; . &amp; . \\
    \end{array}
\right )
\left (
    \begin{array}{ccc}
        &amp; w_{1} &amp; \\
        &amp; w_{2} &amp; \\
        &amp; w_{3} &amp; \\
    \end{array}
\right )
\]</span></p>
<p>The single spectral power distribution, the vector on the left, consists of measurements at many different wavelengths. The linear model summarizes each measurement as the weighted sum of the basis functions, which are the same for all measurements, and a few weights, <span class="math inline">\(\mathbf{w}\)</span>, which are unique to each measurement. The linear model is efficient because we represent each additional measurement using only three weights, <span class="math inline">\(\mathbf{w}\)</span>, rather than the full spectral power distribution.</p>
</section>
<section id="simple-illuminant-estimation" class="level3">
<h3 class="anchored" data-anchor-id="simple-illuminant-estimation">Simple Illuminant Estimation</h3>
<p>Efficiency is useful; but, if efficiency were our only objective we could find more efficient algorithms. The linear models are also important because they lead to very simple estimation algorithms. As an example, consider how we might use a device with three color sensors, like the eye, to estimate the spectral power distribution of daylight. Such a device is vastly simpler than the spectroradiometer Judd et al.&nbsp;needed to make many measurements of the light.</p>
<p>Suppose we have a device with three color sensors, whose spectral responsivities are, say, <span class="math inline">\(r_i(\lambda),\ i = 1 \ldots 3\)</span>. The three sensor responses will be</p>
<p><span id="eq-sensor-responses"><span class="math display">\[
\begin{aligned}
r_1 &amp; = &amp; \sum_\lambda R_1(\lambda)\, e(\lambda) \\
r_2 &amp; = &amp; \sum_\lambda R_2(\lambda)\, e(\lambda) \\
r_3 &amp; = &amp; \sum_\lambda R_3(\lambda)\, e(\lambda)
\end{aligned}
\tag{9.6}\]</span></span></p>
<p>We can group these three linear equations into a single matrix equation</p>
<p><span id="eq-sensor-matrix"><span class="math display">\[
r = \mathbf{S}\, e
\tag{9.7}\]</span></span></p>
<p>where the column vector <span class="math inline">\(r\)</span> contains the sensor responses, the rows of the matrix <span class="math inline">\(R\)</span> are the sensor spectral responsivities, and <span class="math inline">\(e\)</span> is the illuminant spectral power distribution.</p>
<p>Before the Judd et al.&nbsp;study, one might have thought that three sensor responses are insufficient to estimate the illumination. But, from their data we have learned that we can approximate <span class="math inline">\(e\)</span> with a three-dimensional linear model, <span class="math inline">\(e \approx E\, \mathbf{w}\)</span>. This reduces the equation to</p>
<p><span id="eq-sensor-linear-model"><span class="math display">\[
r \approx (\mathbf{S} \mathbf{B}_{\mathbf{e}})\, \mathbf{w}
\tag{9.8}\]</span></span></p>
<p>The matrix <span class="math inline">\((\mathbf{S} \mathbf{B}_{\mathbf{e}})\)</span> is <span class="math inline">\(3 \times 3\)</span>, and its entries are all known. The sensor responses, <span class="math inline">\(r\)</span>, are also known. The only unknown is <span class="math inline">\(w\)</span>. Hence, we can estimate <span class="math inline">\(w\)</span>, and use these weights to calculate the spectral power distribution, <span class="math inline">\(\mathbf{B}_\mathbf{e}\, \mathbf{w}\)</span>.</p>
<p>This calculation illustrates two aspects of the role of linear models. First, linear models represent a priori knowledge about the likely set of inputs. Using this information permits us to convert underdetermined linear equations (<a href="#eq-sensor-matrix" class="quarto-xref">Equation&nbsp;<span>9.7</span></a>) into equations we can solve (<a href="#eq-sensor-linear-model" class="quarto-xref">Equation&nbsp;<span>9.8</span></a>). Linear models are a blunt but useful tool for representing probabilities. Using linear models, it becomes possible to use measurements from only three color sensors to estimate the full relative spectral power distribution of daylight illumination.</p>
<p>Second, linear models work smoothly with the imaging equations. Since the imaging equations are linear, the estimation methods remain linear and simple.</p>
</section>
<section id="surface-reflectance-models" class="level3">
<h3 class="anchored" data-anchor-id="surface-reflectance-models">Surface Reflectance Models</h3>
<p>The daylights are an important class of signals for vision. For most of the history of the earth, daylight was the only important light source. There is no similar set of surface reflectance functions. I was reminded by this once by the brilliant color scientist, G. Wyszecki. When I was just beginning my study of these issues, I asked him why he had not undertaken a study of surfaces similar to daylight study. He shrugged at me and answered, “How do you sample the universe?”</p>
<p>Wyszecki was right, of course. The daylight measurement study could begin and end in a single paper. There is no specific set of surfaces that of equal importance to the daylights, so we have no way to perform a similar analysis on surfaces. But, there are two related questions we can make some progress on. First, we can ask what the properties are of certain collections of surfaces that are of specific interest to us, say for practical applications. Second, we can ask what the visual system, with only three types of cones, can infer about surfaces.</p>
<p>Over the years linear models for special sets of materials can be used in many applications. Printer and scanner manufacturers may be interested in the reflectance functions of inks. Computer graphics programmers may be interested in the reflectance factors of geological materials, or tea pots. Color scientists have repeatedly measured the reflectance functions of standard color samples used in industry, such as the Munsell chips. These cases can be of practical value and interest in printing and scanning applications (e.g. <span class="citation" data-cites="marimont1992-linearmodelssurface">Marimont and Wandell (<a href="references.html#ref-marimont1992-linearmodelssurface" role="doc-biblioref">1992</a>)</span>; <span class="citation" data-cites="farrell1992">Farrell et al. (<a href="references.html#ref-farrell1992" role="doc-biblioref">1992</a>)</span>; <span class="citation" data-cites="vrhel-trussell1994">Vrhel and Trussell (<a href="references.html#ref-vrhel-trussell1994" role="doc-biblioref">1994</a>)</span>; <span class="citation" data-cites="drew1992-interreflection">Drew and Funt (<a href="references.html#ref-drew1992-interreflection" role="doc-biblioref">1992</a>)</span>). To discover the regularities in surface functions, then, we should measure the body reflection terms. From the studies that have taken place over the last several years, it has become increasingly clear that in the visible wavelength region the surface reflectance functions tend to be quite smooth, and thus exhibit a great deal of regularity. Hence, linear models serve to describe the reflectance functions quite well.</p>
<p>For example, <span class="citation" data-cites="cohen1964">Cohen (<a href="references.html#ref-cohen1964" role="doc-biblioref">1964</a>)</span>, <span class="citation" data-cites="maloney1986-linearmodels-surface">Maloney (<a href="references.html#ref-maloney1986-linearmodels-surface" role="doc-biblioref">1986</a>)</span> and <span class="citation" data-cites="parkkinen1989-spectra">Parkkinen et al. (<a href="references.html#ref-parkkinen1989-spectra" role="doc-biblioref">1989</a>)</span> studied the reflectance properties of a variety of surfaces including special samples and some natural objects. For each of the sets studied by these authors the data can be modeled nearly perfectly using a linear model with less than six dimensions. Excellent approximations, though not quite perfect, can be obtained by three-dimensional approximations.</p>
<p>As an example, I have built a linear model to approximate a small collection surface reflectance functions for a color target, the <em>Macbeth ColorChecker</em>, that is used widely in industrial applications. The target consists of 24 square patches laid out in a 4 by 6 array. The surfaces in this target were selected to have reflectance functions similar to a range of naturally occurring surfaces. They include reflectances similar to human skin, flora, and other materials (<span class="citation" data-cites="mccamy1976-chart">McCamy et al. (<a href="references.html#ref-mccamy1976-chart" role="doc-biblioref">1976</a>)</span>).</p>
<div id="fig-macbeth-approx" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-macbeth-approx-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./wp-content/uploads/2012/02/macbethApprox.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-macbeth-approx-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.8: A linear model to approximate the surface reflectances in the Macbeth ColorChecker. The panels in each row of this figure show the surface reflectance functions of six colored surfaces (dashed line) and the approximation to these functions using a linear model (solid lines). The approximations using linear models with three (a), two (b) and one (c) dimension respectively are shown.
</figcaption>
</figure>
</div>
<p>To create the linear model, I measured the surface reflectance functions of these patches with a spectral radiometer in my laboratory. The original data set, then, consisted of measurements from 370nm to 730nm in 1 nm steps for each of the 24 patches. Then, using conventional statistical packages, I calculated a three-dimensional linear model to fit all of these surface reflectance functions. The linear model basis functions, <span class="math inline">\(S_{i}(\lambda)\)</span>, were selected to minimize the squared error<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>.</p>
<p><span id="eq-surface-linmodel-error"><span class="math display">\[
\left (s(\lambda) - \sum_{i=1}^{N} \sigma_{i} S_{i}(\lambda) \right ) ^2 .
\tag{9.9}\]</span></span></p>
<p>The values <span class="math inline">\(\sigma_{i}\)</span> are called the <em>surface coefficients</em>, and we will represent them as a vector, <span class="math inline">\(\mathbf{\sigma} = (\sigma_{1}, \ldots , \sigma_{N})\)</span>. There are fewer surface coefficients than data measurements. If we create a matrix whose columns are are the basis functions, <span class="math inline">\({\mathbf{B}_\mathbf{s}}\)</span>, then we can express the linear model approximation as <span class="math inline">\({\mathbf{B}_\mathbf{s}} \mathbf{\sigma}\)</span>.</p>
<p>The dashed lines in <a href="#fig-macbeth-approx" class="quarto-xref">Figure&nbsp;<span>9.8</span></a> show the reflectance functions of six of the twenty-four surfaces. The smooth curves within each row of the figure contain the approximations using linear models with different dimensionality. The bottom row shows a one-dimensional linear model; in this case the approximations are scaled copies of one another. As we increase the dimensionality of the linear model the approximations become very similar to the originals. The 3-dimensional model the approximations are quite close to the true functions.</p>
<p>The low-dimensional linear model approximates these surface reflectance functions because the functions vary smoothly as a function of wavelength. The linear model consists of a few, slowly varying basis functions shown in <a href="#fig-macbeth-basis" class="quarto-xref">Figure&nbsp;<span>9.9</span></a>. The first basis function captures the light-dark variation of the surfaces. The second basis function captures a red-green variation, and the third a blue-yellow variation.</p>
<div id="fig-macbeth-basis" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-macbeth-basis-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./wp-content/uploads/2012/02/macbethBasis.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-macbeth-basis-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.9: Basis functions of the linear model for the Macbeth ColorChecker. The surface reflectance functions in the collection vary smoothly with wavelength, as do the basis functions. The first basis function is all positive and explains the most variance in the surface reflectance functions. The basis functions are ordered in terms of their relative significance for reducing the error in the linear model approximation to the surfaces.
</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./wp-content/uploads/2012/02/macbethRender.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption><strong>Color Plate 4.</strong> Color renderings of the linear model approximations to the Macbeth ColorChecker. The linear model approximations are shown rendered under a blue sky illumination. The dimension of the linear model approximation is shown above each image. The one-dimensional approximation the surfaces appear achromatic, varying only in lightness. For this illuminant, and using three or more dimensions in the linear model, the rendering is visually indistinguishable from a complete rendering of the surfaces.</figcaption>
</figure>
</div>
<p>Although the approximations are quite good, there are still differences between the surface reflectance functions and the three dimensional linear model. We might ask whether these differences are visually salient. This question is answered by the renderings of these surface approximations in Color Plate 4. The one-dimensional model looks like a collection of surfaces in various shades of gray. For the blue sky illumination used in the rendering, linear models with three or more dimensions are visually indistinguishable from a rendering using the complete set of data.</p>
</section>
<section id="sensor-based-error-measures" class="level3">
<h3 class="anchored" data-anchor-id="sensor-based-error-measures">Sensor-based error measures</h3>
<p>I have described linear models that minimize the squared error between the approximation and the original spectral function. As the last analysis showed, however, when we choose linear models to minimize the spectral error we are not always certain whether we have done a good job in minimizing the visual error. In some applications, the spectral error may not be the objective function that we care most about minimizing. When the final consumer of the image is a human, we may only need to capture that part of the reflectance function that is seen by the sensors.</p>
<p>If we are modeling reflectance functions for a computer graphics application, for example, there is no point in modeling the reflectance function at 300nm since the human visual system cannot sense light in that part of the spectrum anyway. For these applications, we should be careful to model accurately those parts of the function that are most significant for vision. In these cases, one should select a linear model of the surface reflectance by minimizing a different error measure, one that takes into account the selectivity of the eye.</p>
<p><span class="citation" data-cites="marimont1992-linearmodelssurface">Marimont and Wandell (<a href="references.html#ref-marimont1992-linearmodelssurface" role="doc-biblioref">1992</a>)</span> describe how to create linear models that are appropriate for the eye. They consider how to define linear models that minimize the root mean squared error in the photopigment absorption rates, rather than the root mean squared error of the spectral curves. Their method is called <em>one-mode analysis</em>. For many applications, the error measure minimized by one-mode analysis is superior to root mean squared error of the spectral curves.</p>
</section>
<section id="surface-and-illuminant-estimation-algorithms" class="level3">
<h3 class="anchored" data-anchor-id="surface-and-illuminant-estimation-algorithms">Surface and Illuminant Estimation Algorithms</h3>
<p>There is much regularity in daylight and surface functions; so, it makes sense to evaluate how well we estimate spectral functions from sensor responses. Estimation algorithms rely on two essential components.</p>
<p>First, we need a method of representing our knowledge about the likely surface and illuminant functions. For example, linear models can be used encode our a priori knowledge<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>. Second, all modern estimation methods assume that the illumination varies either slowly or not at all across the image. This assumption is important because it means that the illumination adds very few new parameters to estimate.</p>
<p>Consider an example of an image with <span class="math inline">\(p\)</span> points. We expect to obtain <span class="math inline">\(3\)</span> cone absorption rates at each image point, so there are <span class="math inline">\(3p\)</span> measurements. If we can use a surface 3 dimensional model for the surfaces, then there are <span class="math inline">\(3p\)</span> unknown surface coefficients. And, there is a linear relationship between the measurements and the unknown quantities. If the illuminant is known, then the problem is straightforward to solve.</p>
<p>The additional unknown illuminant parameters make the problem a challenge. If the illuminant can vary from point to point, there will be <span class="math inline">\(6p\)</span> unknown parameters and the mismatch between known and unknown parameters will be very great. But, if the illuminant is constant across the image, we only have <span class="math inline">\(3\)</span> additional parameters. In this case, by making some modest assumptions about the image, we can find ways to infer these three parameters and then proceed to estimate the surface parameters.</p>
<p>Modern estimation algorithms work by find a method to overcome the mismatch between the measurements and the unknowns. We can divide existing estimation algorithms into two groups. The majority of estimation algorithms infer the illumination parameters by making one additional assumption about the image contents. For example, suppose we know the reflectance function of one surface. Then, we can use the sensor responses from that surface to measure the illuminant parameters. Knowing the reflectance function of one surface in the image compensates for the three unknown illuminant parameters. There are several implementations of this principle. The most important is the assumption that the average of all the surfaces in the image is gray, which is called the <em>gray-world</em> assumption (<span class="citation" data-cites="buchsbaum1980-gray">Buchsbaum (<a href="references.html#ref-buchsbaum1980-gray" role="doc-biblioref">1980</a>)</span>; <span class="citation" data-cites="land1977-retinex">Land (<a href="references.html#ref-land1977-retinex" role="doc-biblioref">1977</a>)</span>). Other algorithms are based on the assumption that the brightest surface in the image is a uniform, perfect reflector (<span class="citation" data-cites="mccann1976-retinex">McCann et al. (<a href="references.html#ref-mccann1976-retinex" role="doc-biblioref">1976</a>)</span>). An interesting variant on both of these assumptions is the idea that we can identify specular or glossy surfaces in the image. Since specularities reflect the illumination directly, often without altering the illuminant’s spectral power distribution, the sensor responses to glossy surfaces provide information about the illuminant (<span class="citation" data-cites="lee1986-highlights">Lee (<a href="references.html#ref-lee1986-highlights" role="doc-biblioref">1986</a>)</span>; <span class="citation" data-cites="dzmura1986-sharedrodcone">D’Zmura and Lennie (<a href="references.html#ref-dzmura1986-sharedrodcone" role="doc-biblioref">1986</a>)</span>; <span class="citation" data-cites="tominaga1989-model">Tominaga and Wandell (<a href="references.html#ref-tominaga1989-model" role="doc-biblioref">1989</a>)</span>, <span class="citation" data-cites="tominaga1990-surfacespectralreflectance">Tominaga and Wandell (<a href="references.html#ref-tominaga1990-surfacespectralreflectance" role="doc-biblioref">1990</a>)</span>).</p>
<p>A second group of estimation algorithms compensates for the mismatch in measurements and parameters by suggesting ways to acquire more data. For example, Maloney and I showed that if one adds a fourth sensor (at three spatial locations), one can also estimate the surface and illuminants. <span class="citation" data-cites="dzmura1993-basic">D’Zmura and Iverson (<a href="references.html#ref-dzmura1993-basic" role="doc-biblioref">1993a</a>)</span> and <span class="citation" data-cites="dzmura1993-results">D’Zmura and Iverson (<a href="references.html#ref-dzmura1993-results" role="doc-biblioref">1993b</a>)</span> explored an interesting variant of this idea. They asked what information is available if we observe the same surface under several illuminants. Changing the illumination on a surface is conceptually equivalent to seeing the surfaces with additional sensors. Pooling information about the same object seen under different illuminants, is much like acquiring additional information from extra sensors (e.g., see <span class="citation" data-cites="wandell1987-synthesisanalysis">Wandell (<a href="references.html#ref-wandell1987-synthesisanalysis" role="doc-biblioref">1987</a>)</span>).</p>
</section>
<section id="illuminant-correction-an-example-calculation" class="level3">
<h3 class="anchored" data-anchor-id="illuminant-correction-an-example-calculation">Illuminant correction: An example calculation</h3>
<p>Before returning to experimental measurements of color appearance, let’s perform an example calculation that is of some practical interest as well as of some interest in understanding how the visual system might compensate for illumination changes. By working this example, we will start to consider what neural operations might permit the visual pathways to compensate for changes in the ambient illumination.</p>
<p>First, let’s write down the general expression that shows how the surface and illuminant functions combine to yield the cone absorption rates. The three equations for the three cone types, <span class="math inline">\(L\)</span>, <span class="math inline">\(M\)</span>, and <span class="math inline">\(S\)</span>, are</p>
<p><span id="eq-cone-absorptions"><span class="math display">\[
\begin{aligned}
r_{1} &amp; = \sum_{\lambda} R_{1}(\lambda) E(\lambda) S(\lambda)  \\
r_{2} &amp; = \sum_{\lambda} R_{2}(\lambda) E(\lambda) S(\lambda)  \\
r_{3} &amp; = \sum_{\lambda} R_{3}(\lambda) E(\lambda) S(\lambda) .
\end{aligned}
\tag{9.10}\]</span></span></p>
<p>Next, we replace the illuminant and surface functions in <a href="#eq-cone-absorptions" class="quarto-xref">Equation&nbsp;<span>9.10</span></a> with their linear model approximations. This yields a new relationship between the coefficients of the surface reflectance linear model, <span class="math inline">\(\mathbf{\sigma}\)</span>, and the three-dimensional vector of cone absorptions, <span class="math inline">\(\mathbf{r}\)</span>,</p>
<p><span id="eq-lighting-matrix"><span class="math display">\[
\mathbf{r} \approx \mathbf{\Lambda}_{e} \mathbf{\sigma} .
\tag{9.11}\]</span></span></p>
<p>We call the matrix <span class="math inline">\(\mathbf{\Lambda}_{e}\)</span> that relates these two vectors the <em>lighting</em> matrix. The entries of this matrix depend upon the illumination, <span class="math inline">\(\mathbf{e}\)</span>. The <span class="math inline">\(ij^{th}\)</span> entry of the lighting matrix is</p>
<p><span id="eq-lighting-matrix-entry"><span class="math display">\[
\sum_{\lambda} \left ( \sum_k w_{k} E_{k}(\lambda) \right ) R_{i}(\lambda) S_{j}(\lambda) .
\tag{9.12}\]</span></span></p>
<p>We can compute two lighting matrices (<a href="#eq-lighting-matrix" class="quarto-xref">Equation&nbsp;<span>9.11</span></a>) from the spectral curves we have been using as examples. For one lighting matrix I used the mean daylight spectral power distribution, and for the other I used the spectral power distribution of a tungsten bulb. I used the linear model of the Macbeth ColorChecker for the surface basis functions and the Stockman and MacLeod cone absorption functions (see the appendix to <a href="chapter-4-wavelength-encoding.html" class="quarto-xref"><span>Chapter 4</span></a>). The lighting matrix for the blue sky illumination is</p>
<p><span class="math display">\[
\left (
\begin{array}{ccc}
591.48&amp; 223.05 &amp; -643.05 \\
477.62 &amp; 376.93 &amp; -564.48 \\
267.61 &amp; 487.46 &amp; 350.39 \\
\end{array}
\right )
\]</span></p>
<p>and the lighting matrix for the tungsten bulb is</p>
<p><span class="math display">\[
\left (
\begin{array}{ccc}
593.45 &amp; 168.37 &amp; -646.71 \\
445.79 &amp; 312.79 &amp; -564.33 \\
152.46 &amp; 278.51 &amp; 185.47 \\
\end{array}
\right ) .
\]</span></p>
<p>Notice that the largest differences between the matrices are in the third row. This is the row that describes the effect of each surface coefficient on the <span class="math inline">\(S\)</span> cone absorptions. The blue sky lighting matrix contains much larger values than matrix for the tungsten bulb. This makes sense because that is the region of the spectrum where these two illuminant spectral power distributions differ the most (see <a href="#fig-color-signal" class="quarto-xref">Figure&nbsp;<span>9.5</span></a>).</p>
<p>Imagine, now the following way in which the visual system might compensate for illumination changes. Suppose that the cortical analysis of color is based upon the assumption that the illumination is always that of a blue sky. When the illumination is different from the blue sky, the retina must try to provide a neural signal that is similar to the one that would have been observed under a blue sky. What computation does the retina need to perform in order to transform the cone absorptions obtained under the tungsten bulb illuminant into the cone absorption that would have occurred under the blue sky?</p>
<p>The cone absorptions from a single surface under the two illuminants can be written as</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{r} &amp; \approx \mathbf{\Lambda}_{e} \mathbf{\sigma}, \text{and} \\
\mathbf{r}^\prime &amp; \approx \mathbf{\Lambda}_{e'} \mathbf{\sigma} .
\end{aligned}
\]</span></p>
<p>By inverting the lighting matrices and recombining terms, we find that the cone absorptions under the two illuminants should be related linearly as,</p>
<p><span id="eq-linear-relation-cone-absorptions"><span class="math display">\[
\mathbf{r} \approx \mathbf{\Lambda}_{e} ~ {\mathbf{\Lambda}^{-1}_{e'}} ~ \mathbf{r}^\prime .
\tag{9.13}\]</span></span></p>
<p>Hence, we can transform the cone absorptions from a surface illuminated by the tungsten bulb into the cone absorptions of the same surface illuminated by the blue sky by the following:</p>
<p><span id="eq-tungsten-to-blue-sky"><span class="math display">\[
\mathbf{r} = \left (
\begin{array}{ccc}
0.8119 &amp; 0.2271 &amp; 0.0550 \\
-0.0803 &amp; 1.1344 &amp; 0.1282 \\
0.0429 &amp; -0.0755 &amp; 1.8091 \\
\end{array}
\right ) \mathbf{r}^\prime.
\tag{9.14}\]</span></span></p>
<p>The retina can compensate for the illumination change by linearly transforming the observed cone absorptions, <span class="math inline">\(\mathbf{r}^\prime\)</span>, into a new signal, <span class="math inline">\(\mathbf{r}\)</span>.</p>
<p>Now, what does it mean for the retina to compute a linear transformation? The linear transformation consists of a simple series of multiplications and additions. For example, consider the third row of the matrix in <a href="#eq-tungsten-to-blue-sky" class="quarto-xref">Equation&nbsp;<span>9.14</span></a>. This row defines how the new <span class="math inline">\(S\)</span> cone absorptions should be computed from the observed absorptions. When we write this transformation as a single linear equation we see that the observed and transformed signals are related as</p>
<p><span id="eq-blue-transform"><span class="math display">\[
S = .0429~L^\prime + -0.0755~M^\prime + 1.8091~S^\prime .
\tag{9.15}\]</span></span></p>
<p>The transformed <span class="math inline">\(S\)</span> cone absorption is mainly a scaled version of the observed absorptions. Because the tungsten bulb emits much less energy in the short-wavelength part of the spectrum, the scale factor is larger than one. In addition, to be absolutely precise, we should add in a small amount of the observed <span class="math inline">\(L\)</span> cone signal and subtract out a small amount of the observed <span class="math inline">\(M\)</span> cone signal. But, these contributions are relatively small compared to the contribution from the <span class="math inline">\(S\)</span> cones. In general, for each of the cone types, the largest contributions to the transformed signal are scaled copies of the same signal type.</p>
<p>In this matrix, and in many practical examples, the only additive term that is not negligible is the contribution of the <span class="math inline">\(M\)</span> cone response to the transformed <span class="math inline">\(L\)</span> signal. As a rough rule, because the diagonal terms are much larger than the off-diagonal terms, we can obtain good first order approximation to the proper transformation by simply scaling the the observed cone absorptions (e.g., <span class="citation" data-cites="foster1994">Foster and Nascimento (<a href="references.html#ref-foster1994" role="doc-biblioref">1994</a>)</span>).</p>
<p>Compensating for the illumination change by a purely diagonal scaling of the cone absorptions is called <em>von Kries Coefficient Law</em>. The correction is not as precise as the best linear correction, but it frequently provides a good approximation. And, as we shall see in the next section, the von Kries Coefficient Law describes certain aspects of human color appearance measurements as well.</p>
</section>
</section>
<section id="color-constancy-experiments" class="level2" data-number="9.4">
<h2 data-number="9.4" class="anchored" data-anchor-id="color-constancy-experiments"><span class="header-section-number">9.4</span> Color Constancy: Experiments</h2>
<blockquote class="blockquote">
<p>In woven and embroidered stuffs the appearance of colors is profoundly affected by their juxtaposition with one another (purple, for instance, appears different on white than on black wool), and also by differences of illumination. Thus embroiderers say that they often make mistakes in their colors when they work by lamplight, and use the wrong ones. [Aristotle, c.&nbsp;350 B.C.E. Meteorologica].</p>
</blockquote>
<section id="asymmetric-color-matching-experiments" class="level3">
<h3 class="anchored" data-anchor-id="asymmetric-color-matching-experiments">Asymmetric Color-matching Experiments</h3>
<p>To formulate some ideas about how the visual pathways compute color appearance, we adopted the view that color appearance is a psychological estimate of the surface reflectance function (body reflectance). By thinking about computational methods of estimating body reflectance, we have discovered how the cone absorptions from an object vary with illuminant changes. Finally, we have seen that it is possible compensate approximately for these changes by a applying linear transformation to the cone absorptions.</p>
<p>From the computational analysis, we have discovered a good principle to examine in experimental studies of color appearance: What is the relationship between the cone absorptions of objects that appear the same under different illuminants? The computational analysis suggests that the cone absorptions of lights with the same color appearance, but seen under different illuminants, are related by a linear transformation.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./wp-content/uploads/2012/02/matchingMethods.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption><strong>Color Plate 5.</strong> Two experimental methods for measuring asymmetric color-matches. (a) In a memory matching method, the observer sees a target under one illumination, remembers it, and then identifies a matching target after adapting to a second illumination. (b) In a haploscopic experiment the observer adapts the two eyes separately and makes a simultaneous appearance match. The basic findings from these two types of experiments are the same.</figcaption>
</figure>
</div>
<p>It is up to the experimentalist, then, to find an experimental method to use for measuring the cone absorptions that correspond to the same color appearance when seen in different viewing contexts. Color Plate 5 illustrates two methods of making such measurements. Panel (a) shows a <em>memory matching</em> method. In this method, the subject studies the color of a target that is presented under one illumination source and then must select a target that looks the same under a second illumination source. These measurements identify the stimuli, and thus the cone absorptions, of targets that appear the same under the two illuminants. The drawback of the method is that making such matches is very time-consuming because the subject must adapt to the two illumination sources completely, a process which can take two minutes or more.</p>
<p>Color Plate 5 (b) shows a second method called <em>dichoptic matching</em>. In this experiment the observer views the two scenes simultaneously in different eyes. One eye is exposed to a large neutral surface illuminated by, say, a daylight lamp. The other eye is exposed to an equivalent large neutral surface illuminated by, say, a tungsten lamp. These surfaces define a background stimulus that is different to each eye. The two backgrounds fuse in appearance, and appear as a single large background. To establish the asymmetric color-matches, the experimenter places a test object on top of the standard background seen by one eye. The observer selects a matching object from an array of choices seen by the second eye. The color appearance mapping is defined by measuring the cone absorptions of the <em>test</em> and <em>matching</em> objects, usually small colored papers, seen under their respective illuminants.</p>
<p>The dichoptic method has the advantage that the matches may be set quickly, avoiding the tedious delays required for visual adaptation in memory matches. The method has the disadvantage of making the assumption that adaptation occurs independently, prior to binocular combination<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>.</p>
<p>The experimental methods illustrated in Color Plate 5 generalize conventional usual color-matching experiment. These methods are called <em>asymmetric</em> color-matching because, unlike conventional color-matching, in these experiments the matches are set between stimuli presented in different contexts. As we have already seen, because color appearance discounts estimated changes of the illumination, matches set in the asymmetric color-matching experiment are <strong>not</strong> cone absorption matches. Rather, the observer is establishing a match at a more central site following the correction for the properties of the scene.</p>
<p>The asymmetric color-matching experiment is directly relevant to the questions raised by our computational analysis of color appearance. Moreover, the experiment has a central place in the study of color appearance simply for practical experimental reasons. There are many general questions we might ask about color appearance. For example, we would like to be able to measure which colors are similar to one another; which colors have a common hue, saturation or brightness, and so forth. If we had to study these questions separately under each illuminant, the task would be overwhelming. By beginning with the asymmetric color-matching experiment, we can divide color appearance measurements into two parts and reduce our experimental burden. The asymmetric matches define a mapping between the cone absorptions of objects with the same color appearance seen under different illuminants. From these experiments, we learn how to convert a target seen under one illuminant into an equivalent target under a standard illuminant. This transformations saves a great deal of experimental effort since we can focus most of our questions about color appearance on studies using just one standard illuminant.</p>
</section>
<section id="the-linearity-of-asymmetric-color-matches" class="level3">
<h3 class="anchored" data-anchor-id="the-linearity-of-asymmetric-color-matches">The linearity of asymmetric color-matches</h3>
<p>We have seen measurements of superposition to test linearity throughout this volume. Tests of linearity in asymmetric color matching appear very early in the color appearance literature. When <span class="citation" data-cites="vonkries1905">von Kries (<a href="references.html#ref-vonkries1905" role="doc-biblioref">1905</a>)</span> introduced the coefficient law, he listed several testable empirical results. Among the predictions of the basic law he listed the basic test of linearity, namely</p>
<blockquote class="blockquote">
<p>… there exist several very simple laws, which also appear to be specially adapted for experimental test. Namely, it must be that if <span class="math inline">\(L_1\)</span> on one retinal region causes the same result as <span class="math inline">\(L_2\)</span> on another, and similarly <span class="math inline">\(M_1\)</span>, working on the first, causes the same effect as <span class="math inline">\(M_2\)</span> on the other, in every case also <span class="math inline">\(L_1 + M_1\)</span> must have here the same effect as <span class="math inline">\(L_2 + M_2\)</span> there.<br>
… The extended studies of Wirth (1900-1903) show that the law can be considered as nearly valid for reacting lights that are not too weak.</p>
</blockquote>
<p>Evidently, von Kries not only raised the question of linearity of asymmetric color-matching, but by 1905 he considered it answered affirmatively.</p>
<p>While von Kries considered the question settled, not everyone was persuaded. Over the years, there have been many separate experimental tests of linearity in the asymmetric color-matching experiment. I am particularly impressed by a series of papers by Elaine Wassef, working first in London and then at the University College for Girls in Cairo. Wassef wrote at roughly the same time E. H. Land was working at Polaroid. In her papers, she reports on new studies and a review of the experimental test of asymmetric color-matching linearity<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>. Like von Kries, Wassef asked whether one could predict asymmetric color-matches using the principle of superposition. And, like von Kries, she concluded that the weight of the experimental evidence supported the linearity hypothesis: When the illumination changes, the cone absorptions of the test and matching lights are related by a linear transformation.</p>
<p>I have replotted some of Wassef’s data to illustrate the nature of the measurements and the size of the effect (<a href="#fig-wassef-shift" class="quarto-xref">Figure&nbsp;<span>9.10</span></a>). The illuminant spectral power distributions she used in her dichoptic matching experiment are plotted in <a href="#fig-wassef-shift" class="quarto-xref">Figure&nbsp;<span>9.10</span></a> (a). To plot her results, I have converted Wassef’s reported measurements into one absorptions. I have plotted the cone absorptions of the surfaces that matched in color appearance when seen under the two illuminants. <a href="#fig-wassef-shift" class="quarto-xref">Figure&nbsp;<span>9.10</span></a> (b) shows the <span class="math inline">\(L\)</span> and <span class="math inline">\(M\)</span> cone absorptions of the surfaces under the two illuminants, and @<a href="#fig-wassef-shift" class="quarto-xref">Figure&nbsp;<span>9.10</span></a> (c) shows the <span class="math inline">\(L\)</span> and <span class="math inline">\(S\)</span> cone absorptions. The cone absorptions for objects seen under a tungsten illuminant are plotted as open circles; the cone absorptions for objects seen under a blue sky illumination are plotted as filled squares. The size of the effect is quite substantial. Two sets of points show the cone absorptions of targets that look identical in their respective contexts. Yet, the cone absorption values from the surfaces under these illuminants don’t even overlap in their values.</p>
<div id="fig-wassef-shift" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-wassef-shift-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./wp-content/uploads/2012/02/wassefShift.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-wassef-shift-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.10: Data from an asymmetric color-matching experiment using the dichoptic method. The test and matching lights are viewed in different contexts and appear identical. But, the two lights have very different cone absorption rates. Hence, appearance matches made across an illuminant change are not cone absorption matches. The spectral power distributions of two illuminants, one approximating mean daylight and the other a tungsten illuminant are shown in panel a. Cone absorptions for targets that appear identical to one another in these two contexts are shown as scatterplots in (b) for the (L, M) cones, and (c) for the (L, S) cones. The points plotted as open circles are cone absorptions for tests seen under the first illuminant; matches seen under the second illuminant are plotted as filled squares. The stimuli represented by the absorptions have the same color appearance, but they correspond to very different cone absorptions (Source: <span class="citation" data-cites="wassef1959">Wassef (<a href="references.html#ref-wassef1959" role="doc-biblioref">1959</a>)</span>).
</figcaption>
</figure>
</div>
<p>Taken together, the color matching and asymmetric color-matching show the following. When the objects are in the same context, equating the cone absorptions equate appearance. But, when the two objects are seen under different illuminants, equating cone absorptions does not equate for appearance. Within each context the observer uses the pattern of cone absorptions to infer color appearance, probably by comparing the relative cone absorption rates. Color appearance is an interpretation of the physical properties of the objects in the image.</p>
</section>
<section id="von-kries-coefficient-law-experiments" class="level3">
<h3 class="anchored" data-anchor-id="von-kries-coefficient-law-experiments">Von Kries Coefficient Law: Experiments</h3>
<p>Through his Coefficient Law, J. Von Kries sought to explain these asymmetric color matches by a simple physiological mechanism. He suggested that the visual pathways adjust to the illumination by scaling the signals from the individual cone classes. This hypothesis has a simple experimental prediction: If we plot, say, the <span class="math inline">\(S\)</span> cone absorptions of the test and match surfaces on a single graph, the data should fall along a straight line through the origin. The slope of the predicted line is the scale factor for the illuminant change.</p>
<p>Neither von Kries or Wassef knew the photopigment spectral curves; hence, they could not create the graph they needed to test the von Kries Coefficient Law directly. But, using an indirect measurement based on estimation of the eigenvectors of the measured linear transformations, <span class="citation" data-cites="burnham1957-predictioncolorappearance">Burnham et al. (<a href="references.html#ref-burnham1957-predictioncolorappearance" role="doc-biblioref">1957</a>)</span> and <span class="citation" data-cites="wassef1959">Wassef (<a href="references.html#ref-wassef1959" role="doc-biblioref">1959</a>)</span> rejected von Kries scaling. Despite this rejection, von Kries’ hypothesis continued to be used widely to explain how color appearance varies with illumination. Among theorists, for example, E. H. Land relied entirely on von Kries scaling as the foundation of his retinex theory (<span class="citation" data-cites="brewer1954">Brewer and Lyle Brewer (<a href="references.html#ref-brewer1954" role="doc-biblioref">1954</a>)</span>; <span class="citation" data-cites="brainard1986-analysisretinextheory">Brainard and Wandell (<a href="references.html#ref-brainard1986-analysisretinextheory" role="doc-biblioref">1986</a>)</span>; <span class="citation" data-cites="land1986-recentretinex">Land (<a href="references.html#ref-land1986-recentretinex" role="doc-biblioref">1986a</a>)</span>, <span class="citation" data-cites="land1986-retinexalt">Land (<a href="references.html#ref-land1986-retinexalt" role="doc-biblioref">1986b</a>)</span>).</p>
<div id="fig-wassef-vk" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-wassef-vk-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./wp-content/uploads/2012/02/wassefVK.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-wassef-vk-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.11: The cone absorptions of the test and match surfaces fall close to a straight line. These appearance matches were made by presenting the test and match objects to different eyes. The illuminant for one eye was similar to a tungsten bulb and the other eye was blue skylight. The Von Kries coefficient law predicts that the line should pass through the origin of the graph; while not precisely correct, the rule is a helpful starting point (Source: <span class="citation" data-cites="wassef1959">Wassef (<a href="references.html#ref-wassef1959" role="doc-biblioref">1959</a>)</span>
</figcaption>
</figure>
</div>
<p>Today, we have good estimates of the spectral sensitivities of the cone photopigments and it is possible convert Wassef’s data into cone absorptions and analyze von Kries coefficient law directly. <a href="#fig-wassef-vk" class="quarto-xref">Figure&nbsp;<span>9.11</span></a> shows a graphical evaluation of von Kries hypothesis for the data in <a href="#fig-wassef-shift" class="quarto-xref">Figure&nbsp;<span>9.10</span></a>. Each panel plots the cone absorptions of corresponding test and match targets for one of the three cone types. As predicted by Von Kries, the cone absorptions of the test and match targets fall along a line. Moreover, the slope of the lines relating the cone absorptions also make sense. The slope is largest for the <span class="math inline">\(S\)</span> cones where illuminant change has its largest effect. The data are not perfectly consistent with von Kries scaling, however, because the lines through the data do not pass through the origin, as required by the theory<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>.</p>
<p>There is an emerging consensus in many branches of color science that the von Kries coefficient law explains much about how color appearance depends on the illumination. J. von Kries simple hypothesis is important partly because of its practical utility, and partly because of its implications for the representation of color appearance within the brain. The hypothesis explains the major adjustments for color constancy to in terms of the photoreceptor signal, and the photoreceptor signals combine within the retina (<a href="chapter-4-wavelength-encoding.html" class="quarto-xref"><span>Chapter 4</span></a>). Hence, von Kries hypothesis implies that either (a) the main adjustment takes place very early, or (b) the photoreceptor signals can be separated in the central representation. This topic will come up again later, when we review some of the phenomena concerning color appearance in the central nervous system.</p>
</section>
<section id="how-color-constant-are-we" class="level3">
<h3 class="anchored" data-anchor-id="how-color-constant-are-we">How Color Constant Are We?</h3>
<p>Finally, let’s consider how well the visual pathways correct for illumination change. On this point there is some consensus: The asymmetric color-matches do not compensate completely for the illumination change. The visual pathways compensate for only part of the illuminant change (<span class="citation" data-cites="helson1938">Helson (<a href="references.html#ref-helson1938" role="doc-biblioref">1938</a>)</span>; <span class="citation" data-cites="judd1940">Judd (<a href="references.html#ref-judd1940" role="doc-biblioref">1940</a>)</span>).</p>
<p>Brainard and Wandell (<span class="citation" data-cites="brainard1991-bilinearmodel">Brainard and Wandell (<a href="references.html#ref-brainard1991-bilinearmodel" role="doc-biblioref">1991</a>)</span>, <span class="citation" data-cites="brainard1992-asymmetriccolormatching">Brainard and Wandell (<a href="references.html#ref-brainard1992-asymmetriccolormatching" role="doc-biblioref">1992</a>)</span>) described this phenomenon using results from a recent experiment. We used an experimental apparatus consisting of simulated surfaces and illuminants and an asymmetric color-matching experiment based on memory-matches. We presented subjects with images of simulated colored papers, rendered under a diffuse daylight illuminant, on a CRT display. The subjects memorized the color appearance of one of the surfaces. Next, we changed the the simulated illuminant, slowly over a period of two minutes, giving subjects a chance to adapt to the new illuminant. Then, the subject adjusted the appearance of a simulated surface to match the color appearance of the surface they had memorized.</p>
<div id="fig-illuminant-change-vs-subjective" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-illuminant-change-vs-subjective-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./wp-content/uploads/2012/02/equivIll.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-illuminant-change-vs-subjective-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.12: A comparison of the illuminant change and the subjective illuminant change, as inferred from an asymmetric matching experiment. The simulated illuminant change and subjective illuminant changes are shown by the filled squares and open squares respectively. Subjects behave as if the illuminant change is about half of the true illuminant change (Source: <span class="citation" data-cites="brainard1991-bilinearmodel">Brainard and Wandell (<a href="references.html#ref-brainard1991-bilinearmodel" role="doc-biblioref">1991</a>)</span>).
</figcaption>
</figure>
</div>
<p>We can represent the difference between the two simulated illuminants by plotting the illuminant change. The filled symbols in <a href="#fig-illuminant-change-vs-subjective" class="quarto-xref">Figure&nbsp;<span>9.12</span></a> show the illuminant changes in two experimental conditions. The top panel shows an illuminant change that increased the short-wavelength light and decreased the long-wavelength light. The bottom panel shows an illuminant change that increased the energy at all wavelengths.</p>
<p>Suppose that subjects equated the perceived surface reflectance, but that the illuminant change they estimated was different from the true illuminant change. In that case, we can use the observed matches to infer the the subjects’ illuminant estimates, which are plotted as the open symbols in two panels of <a href="#fig-illuminant-change-vs-subjective" class="quarto-xref">Figure&nbsp;<span>9.12</span></a>. Subjects are acting as if the illuminant change they are correcting for is similar to the simulated illuminant change but, smaller. Subjects’ performance is conservative, correcting for about half the true illuminant change.</p>
<p>Brainard and Wandell’s experiments were conducted on display monitors, and the images were far less interesting than full natural scenes (<span class="citation" data-cites="brainard1991-bilinearmodel">Brainard and Wandell (<a href="references.html#ref-brainard1991-bilinearmodel" role="doc-biblioref">1991</a>)</span> <span class="citation" data-cites="brainard1992-asymmetriccolormatching">Brainard and Wandell (<a href="references.html#ref-brainard1992-asymmetriccolormatching" role="doc-biblioref">1992</a>)</span>). It is possible that given additional clues, subjects may come closer to true illuminant estimation. But, in most laboratory experiments to date, subjects do not compensate fully for changes in the illumination. When the illumination changes color appearance changes less than it might if color was defined by the cone absorptions; but, it changes more than it would if the nervous system used the best possible computational algorithms. The performance of biological systems often seems to fall in this regime. Very poor behavior is forced to change towards a better solution. But, the evolutionary pressure does not force our nervous system to solve estimation problems perfectly. When the marginal return for additional improvements is not great, pretty well seems to do.</p>
</section>
</section>
<section id="the-perceptual-organization-of-color" class="level2" data-number="9.5">
<h2 data-number="9.5" class="anchored" data-anchor-id="the-perceptual-organization-of-color"><span class="header-section-number">9.5</span> The Perceptual Organization of Color</h2>
<p>In this section, I will review some of the methods for describing the perceptual organization of color appearance. Specifically, we will review the relationship between different colors and some of the systems for describing color appearance. In addition to the implications this organization has for understanding the neural representation of color appearance, there are also many practical needs for descriptive systems of color appearance. Artists and designers need ways to identify and specify the color appearance of a design. Further, they need ways of organizing colors and finding interesting color harmonies. Engineers need to assess the appearance and discriminability of colors used to highway signs and to label parts, packaging, and software icons.</p>
<p>Language provides us with a useful start at organizing color appearance. Spoken English in the U.S. consists of eleven color terms that are widely and consistently used<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>. While the number of terms used differs across cultures, there is a remarkable hierarchical organization to the order in which color names appear. Cultures with a small number of basic color names always include white, black and red. Color terms such as purple and pink enter later (<span class="citation" data-cites="berlinkay1969-basiccolorterms">Berlin and Kay (<a href="references.html#ref-berlinkay1969-basiccolorterms" role="doc-biblioref">1969</a>)</span>; <span class="citation" data-cites="boynton1987-basiccolors">Boynton and Olson (<a href="references.html#ref-boynton1987-basiccolors" role="doc-biblioref">1987</a>)</span>).</p>
<p>Color names are a coarse description of color experience. Moreover, names list, but do not organize, color experience. Thus, they are not helpful when we consider issues such as color similarity or color harmony. A more complete organization of color experience is based on the three perceptual attributes called: <em>hue, saturation</em>, and <em>brightness</em>. Hue is the attribute that permits a color to be classified as red, yellow, green, and so forth. Saturation describes a color’s similarity to a neutral gray or white. A gray object with a small reddish tint has little saturation, while a red object, with little white or gray, is very saturated. An object’s brightness tells us about the relative ordering of the object on the dark to light scale.</p>
<p>Based on psychological studies of the similarity of colored patches with many different hues, saturations and brightnesses, the artist Albert Munsell created a book of colored samples. The appearance of the samples is organized with respect to hue, saturation and brightness. Furthermore, the colored samples are spaced in equal perceptual steps. Munsell organized the samples within his book in terms using a cylindrical organization as shown in <a href="#fig-munsell-book" class="quarto-xref">Figure&nbsp;<span>9.13</span></a>. The <em>Munsell Book of Colors</em> is published and used as a reference in many design and engineering applications.</p>
<div id="fig-munsell-book" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-munsell-book-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./wp-content/uploads/2012/02/munsell.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-munsell-book-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.13: The Munsell Book of Colors is a collection of colored samples organized in terms of three perceptual attributes of color. The samples are arranged using a cylindrical geometry with respect to these attributes. The main axis of the cylinder codes lightness; the distance from the center of the cylinder to the edge codes the Munsell property called value (saturation); the position around the circumference of the cylinder codes the Munsell property called chroma (hue).
</figcaption>
</figure>
</div>
<p>Perceptually, both saturation and brightness can be arranged using a linear ordering from small to large; hue, however, does not follow a linear ordering. So, Munsell organized lightness along the main axis of the cylinder, and saturation as the distance from the center of the cylinder to the edge. The circular hue dimension was mapped around the circumference of the cylinder. The Munsell Book of Colors notation is widely used in industry and science.</p>
<p>Munsell developed a special notation to refer to each of the samples in his book. To distinguish his notation from the colloquial usage, Munsell substituted the word <em>value</em> for lightness and the word <em>chroma</em> for saturation. He retained the word hue, apparently finding no adequate substitute. In the Munsell notation, the words hue, chroma and value have specific and technical meanings. Each colored paper is described using a three-part syntax of hue chroma/value. For example, 3YR 5/3 refers to a colored paper with the hue called 3YR, the chroma level 5, and the value level 3.</p>
<p>The Munsell Book was created before the CIE color standards described in <a href="chapter-4-wavelength-encoding.html" class="quarto-xref"><span>Chapter 4</span></a>. With the advent of the CIE measurement standard, based on the color-matching functions, there was a need for a method to convert the Munsell representation into the CIE standard representation. A committee of the Optical Society, led by Nickerson, Newhall and Evans, measured the CIE values of the Munsell samples in the published book, and the Munsell Corporation agreed to produce the colored samples to measurement standards defined by the Optical Society of America. The new standard for the Munsell Book, based on CIE values rather than pigment formulae, is called the <em>Munsell Renotation System</em>. Calibration tables that describe the color measurements of the Munsell Book samples are tabulated, for example, <span class="citation" data-cites="wyszeckicolorscienceconcepts1982">Wyszecki and Stiles (<a href="references.html#ref-wyszeckicolorscienceconcepts1982" role="doc-biblioref">1982</a>)</span>.</p>
<section id="opponent-colors" class="level3">
<h3 class="anchored" data-anchor-id="opponent-colors">Opponent-Colors</h3>
<p>One of the most remarkable and important insights about color appearance is the concept of <em>opponent-colors</em>, first described by E. Hering (1878). Hering pointed out that there is a powerful psychological relationship between the different hues. While some pairs of hues can coexist in a single color sensation, others cannot. For example, orange is composed of red and yellow while cyan is composed of blue and green. But, we never experience a hue that is simultaneously red and green. Nor do we experience a color sensation that is simultaneously blue and yellow. These two hue pairs, red-green and blue-yellow, are called <em>opponent-colors</em>.</p>
<p>There is no physical reason why these two opponent-colors pairs must exist. That we never perceive red and green, while we easily perceive red and yellow, must be due to the neural representation of colors. Hering argued that opponent-colors exist because the sensations of red and green are encoded in the visual pathways by a single pathway. The excitation of the pathway causes us to perceive one of the opponent-colors; inhibition of the pathway causes us to perceive the other.</p>
<p>Hering made his point forcefully, and extended his theory to explain various other aspects of color appearance, as well. But, his insights were not followed by a set of quantitative studies. Perhaps for this reason, his ideas languished while the colorimetrists used color-matching to set standards for all of modern technology. This is not to say Hering’s work was forgotten. Colorimetrists who thought about color appearance invariably turned to Hering’s insights. In a well-known review article, the eminent scientist D. B. Judd, wrote</p>
<blockquote class="blockquote">
<p>The Hering (1905) theory of opponent colors has come to be fairly well accepted as the most likely description of color processes in the optic nerve and cortex. Thus this theory reappears in the final stage in the stage theories of von Kries-Schrodinger (von Kries (1905); Schrodinger, 1925), Adams (1923, 1942) and Muller (1924, 1930). By far the most completely worked out of these stage theories is that of Muller. … There is slight chance that all of the conjectures are correct, but, even if some of the solutions proposed by Muller prove to be unacceptable, he has nevertheless made a start toward the solution of important problems that will eventually have to be faced by other theorists. [(<span class="citation" data-cites="judd1951a-visualstimulus">Judd (<a href="references.html#ref-judd1951a-visualstimulus" role="doc-biblioref">1951</a>)</span>, pg. 836)].</p>
</blockquote>
</section>
<section id="hue-cancellation" class="level3">
<h3 class="anchored" data-anchor-id="hue-cancellation">Hue Cancellation</h3>
<p>Several experimental observations, beginning in the mid-1950s, catapulted opponent-colors theory from a special-purpose model, known only to to color specialists, to a central idea in vision science.</p>
<p>The first was a behavioral experiment that defined a procedure for measuring opponent-colors, the <em>hue cancellation</em> experiment. The hue cancellation experiment was developed in a series of papers by Jameson and Hurvich (<span class="citation" data-cites="jamesonhurvich1955">Jameson and Hurvich (<a href="references.html#ref-jamesonhurvich1955" role="doc-biblioref">1955</a>)</span>; <span class="citation" data-cites="hurvichjameson1957">Hurvich and Jameson (<a href="references.html#ref-hurvichjameson1957" role="doc-biblioref">1957</a>)</span>). By providing a method of quantifying the opponent-colors insight, Hurvich and Jameson made the idea accessible to other scientists, opening a major line of inquiry.</p>
<p>In the hue cancellation experiment, the observer is asked to judge whether a test light appears to be, say, reddish or greenish. If the test light appears reddish, the subject adds green light in order to cancel precisely the red appearance of the test. If the light appears greenish, then the subject adds red light to cancel the green appearance. The added light is called the <em>canceling</em> light. Once the red or green hue of the test light is canceled, the test plus canceling light appear yellow, blue, or gray. The same experiment can be performed to measure the blue-yellow opponent-colors pairing. In this case the subject is asked whether the test light appears blue or yellow, and the canceling lights also appear blue and yellow.</p>
<p><a href="#fig-hue-cancellation" class="quarto-xref">Figure&nbsp;<span>9.14</span></a> shows a set of hue cancellation measurements obtained by Jameson and Hurvich (<span class="citation" data-cites="jamesonhurvich1955">Jameson and Hurvich (<a href="references.html#ref-jamesonhurvich1955" role="doc-biblioref">1955</a>)</span>; <span class="citation" data-cites="hurvichjameson1957">Hurvich and Jameson (<a href="references.html#ref-hurvichjameson1957" role="doc-biblioref">1957</a>)</span>). Subjects canceled the red-green or blue-yellow color appearance of a series of spectral lights. The vertical axis shows the relative intensity of the canceling lights, scaled so that when equal amounts of these lights are superimposed the result did not appear, say, red or green. The canceling lights always have positive intensity, but the intensity of the green and blue canceling lights are plotted as negative to permit you to distinguish which canceling light was used.</p>
<div id="fig-hue-cancellation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hue-cancellation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./wp-content/uploads/2012/02/hueCancel.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hue-cancellation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.14: Measurements from the hue cancellation experiment. An observer is presented with a monochromatic test light. If the light appears red then some amount of a green canceling light is added to cancel the redness. If the light appears green, then a red canceling light is added to cancel the greenness. The horizontal axis of the graph measures the wavelength of the monochromatic test light, and the vertical axis measures the relative intensity of the canceling light. The entire curve represents the red-green appearance of all monochromatic lights. A similar procedure is used to measure blue-yellow. (Source: <span class="citation" data-cites="hurvichjameson1957">Hurvich and Jameson (<a href="references.html#ref-hurvichjameson1957" role="doc-biblioref">1957</a>)</span>).
</figcaption>
</figure>
</div>
<p>To what extent can we generalize from red-green measurements using monochromatic lights to other lights? The answer to this question we must evaluate the linearity of the hue cancellation experiment. If the experiment is linear, we can use the data in <a href="#fig-hue-cancellation" class="quarto-xref">Figure&nbsp;<span>9.14</span></a> to predict whether any test light will appear red or green (blue-yellow) since all lights are the sum of monochromatic lights. If the experiment is not linear, then the data represent only an interesting collection of observations.</p>
<p>To evaluate the linearity of the hue cancellation experiment, one can perform the following experiment: Suppose the test light <span class="math inline">\(t_{1}\)</span> appears neither red nor green, and the test light <span class="math inline">\(t_{2}\)</span> appears neither red nor green. Does the superposition of these two test lights, <span class="math inline">\(t_{1} + t_{2}\)</span>, also appear neither red nor green? In general, the hue cancellation experiment fails this test of linearity. If we superimpose two lights, neither of which appears red or green, the result can appear red. If we add two lights neither of which appears blue or yellow, the result can appear yellow. Hence, the hue cancellation studies are a useful benchmark. But, we need a more complete (nonlinear) model before we can apply the hue cancellation data in <a href="#fig-lg-opponent-signals" class="quarto-xref">Figure&nbsp;<span>9.16</span></a> to predict the opponent-colors appearance of polychromatic test lights (<span class="citation" data-cites="larimer1975-opponentprocessadditivity">Larimer et al. (<a href="references.html#ref-larimer1975-opponentprocessadditivity" role="doc-biblioref">1975</a>)</span>; <span class="citation" data-cites="burns1984-abneyeffect">Burns et al. (<a href="references.html#ref-burns1984-abneyeffect" role="doc-biblioref">1984</a>)</span>; <span class="citation" data-cites="ayama1989">Ayama and Ikeda (<a href="references.html#ref-ayama1989" role="doc-biblioref">1989</a>)</span>; <span class="citation" data-cites="chichilnisky1995">Chichilnisky (<a href="references.html#ref-chichilnisky1995" role="doc-biblioref">1995</a>)</span>).</p>
</section>
<section id="opponent-colors-measurements-at-threshold" class="level3">
<h3 class="anchored" data-anchor-id="opponent-colors-measurements-at-threshold">Opponent-colors measurements at threshold</h3>
<p>In addition to color appearance judgments, one can also demonstrate the presence of essential opponent-colors signals behaviorally by <em>color test-mixture experiments</em>. These color experiments are direct analogues of the pattern-mixture experiments I reviewed in</p>
<div id="fig-opponent-mixture" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-opponent-mixture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./wp-content/uploads/2012/02/opponentB.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-opponent-mixture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.15: Color test-mixture experiments demonstrate opponent-colors processes. The axes measure percent change in cone absorption rates for the L and M cones. The points show the cone absorptions rates at detection threshold measured using different colored test lights. The smooth curve is an ellipse fit through the data points. The mixture experiment shows that the L and M cone signals cancel one another, so that lights that excite a mixture of L and M cones are harder to see than lights that stimulate just one of these two cone class (Source: <span class="citation" data-cites="wandell1987-synthesisanalysis">Wandell (<a href="references.html#ref-wandell1987-synthesisanalysis" role="doc-biblioref">1987</a>)</span>).
</figcaption>
</figure>
</div>
<p>The intersection of the ellipse with the horizontal axis, shows the relative <span class="math inline">\(L\)</span> cone absorption rate at detection threshold. The intersection of the ellipse with the vertical axis shows the relative <span class="math inline">\(M\)</span> cone absorption rate at detection threshold. The shape of the ellipse shows that signals that stimulate the <span class="math inline">\(L\)</span> and <span class="math inline">\(M\)</span> cones simultaneously are less visible than signals that stimulate only one or the other. At the most extreme points on the ellipse, the cone absorptions of the <span class="math inline">\(L\)</span> and <span class="math inline">\(M\)</span> cones are more than five times the rate required to detect a signal when each cone class is stimulated alone. The poor sensitivity to mixtures of signals from these two cone types shows that the signals must oppose one another. The cancellation of threshold level signals from the <span class="math inline">\(L\)</span> and <span class="math inline">\(M\)</span> cones, as well as between the <span class="math inline">\(S\)</span> cones and the other two classes (not shown), have been observed in many different laboratories and under many different experimental conditions (e.g., <span class="citation" data-cites="boynton1964-huewavelength">Boynton et al. (<a href="references.html#ref-boynton1964-huewavelength" role="doc-biblioref">1964</a>)</span>; <span class="citation" data-cites="mollon1977-shortwave">Mollon and Polden (<a href="references.html#ref-mollon1977-shortwave" role="doc-biblioref">1977</a>)</span>; <span class="citation" data-cites="pugh1976">Pugh (<a href="references.html#ref-pugh1976" role="doc-biblioref">1976</a>)</span>, <span class="citation" data-cites="pugh-mollon1979">Pugh and Mollon (<a href="references.html#ref-pugh-mollon1979" role="doc-biblioref">1979</a>)</span>; <span class="citation" data-cites="stromeyer1985-secondsite">Stromeyer et al. (<a href="references.html#ref-stromeyer1985-secondsite" role="doc-biblioref">1985</a>)</span>; <span class="citation" data-cites="sternheim1979-flicker">Sternheim et al. (<a href="references.html#ref-sternheim1979-flicker" role="doc-biblioref">1979</a>)</span>; <span class="citation" data-cites="wandellpugh1980a">Wandell and Pugh (<a href="references.html#ref-wandellpugh1980a" role="doc-biblioref">1980a</a>)</span>, <span class="citation" data-cites="wandellpugh1980b">Wandell and Pugh (<a href="references.html#ref-wandellpugh1980b" role="doc-biblioref">1980b</a>)</span>).</p>
<p>In addition to demonstrating opponent-colors, these threshold data reveal a second interesting and surprising feature of visual encoding. Two neural signals that are visible when they are seen singly become invisible when they are superimposed. It seems odd that the visual system should be organized so that plainly visible signals can be made invisible. From the figure we can see that this is a powerful effect, suppressing a signal that is more than five times threshold. This observation tells us that in many operating conditions absolute sensitivity is not the dominant criterion. The visual pathways can sacrifice target visibility in order to achieve the goals of the opponent-colors encoding.</p>
</section>
<section id="opponent-signals-in-the-visual-pathways" class="level3">
<h3 class="anchored" data-anchor-id="opponent-signals-in-the-visual-pathways">Opponent Signals in the Visual Pathways</h3>
<p>In addition to these two types of behavioral evidence, there is also considerable physiological evidence that demonstrates the existence of opponent-colors signals in the visual pathway.</p>
<p>In a report that gained widespread attention, <span class="citation" data-cites="svaetichin1956">Svaetichin (<a href="references.html#ref-svaetichin1956" role="doc-biblioref">1956</a>)</span> measured the responses of three types of retinal neurons in a fish. He reported that the electrical responses were qualitatively consistent with Hering’s notion of the opponent-colors representation. In two types of neurons, the electrical response increased to certain wavelengths of light and decreased in response to other wavelengths, paralleling the red-green and blue-yellow opponency in color perception<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>. The electrical response of a third set of neurons increased to all wavelengths of light, as in a black-white representation. Shortly after Svaetichin’s report, De Valois and his colleagues established the existence of opponent-colors neurons in the lateral geniculate nucleus of nonhuman primates. There is now a substantial literature documenting the presence of color opponent-signals in the visual pathways (e.g. <span class="citation" data-cites="devalois1958">De Valois et al. (<a href="references.html#ref-devalois1958" role="doc-biblioref">1958</a>)</span>, <span class="citation" data-cites="devalois1965">De Valois (<a href="references.html#ref-devalois1965" role="doc-biblioref">1965</a>)</span>; <span class="citation" data-cites="devalois1966-lgn">De Valois et al. (<a href="references.html#ref-devalois1966-lgn" role="doc-biblioref">1966</a>)</span>; <span class="citation" data-cites="wiesel-hubel1966">Wiesel and Hubel (<a href="references.html#ref-wiesel-hubel1966" role="doc-biblioref">1966</a>)</span>; <span class="citation" data-cites="gouras1968">Gouras (<a href="references.html#ref-gouras1968" role="doc-biblioref">1968</a>)</span>; <span class="citation" data-cites="derrington1984-chromaticmechanisms">Derrington et al. (<a href="references.html#ref-derrington1984-chromaticmechanisms" role="doc-biblioref">1984</a>)</span>; <span class="citation" data-cites="lennie1990-cortex">Lennie et al. (<a href="references.html#ref-lennie1990-cortex" role="doc-biblioref">1990</a>)</span>).</p>
<p>The resemblance between the psychological organization of opponent-colors measured in the hue cancellation experiment and the neural opponent-signals suggests a link from the neural responses to the perceptual organization. To make a convincing argument for the specific connection between opponent-colors and a specific set of neural opponent-signals, we must identify a linking hypothesis. The hypothesis should tell us how we can predict appearance from the activity of cells, and conversely how we can predict the activity of these cells from appearance.</p>
<p>A natural starting place is to suppose that there is a population of neurons whose members are excited when we perceive red and inhibited when we perceive green. From the linking hypothesis, we predict that neurons in this population will be unresponsive to lights that appear neither red nor green. There are two spectral regions that appear neither red nor green to human observers: one near 570nm and a second near 470nm. To forge a strong link between appearance and neural response, we can ask whether the candidate neural population fails to respond to lights that appear neither red nor green. Then, we might search for a second population that fails to respond to lights that appear neither blue nor yellow.</p>
<p>This question was studied by DeValois and his collaborators in the lateral geniculate nucleus of the monkey. In their studies, DeValois and his colleagues studied the response of neurons to monochromatic stimuli presented on a zero background. They found a weak correspondence between the neutral points of individual neurons and the perceptual neutral points (<span class="citation" data-cites="devalois1966-lgn">De Valois et al. (<a href="references.html#ref-devalois1966-lgn" role="doc-biblioref">1966</a>)</span>). More recently, <span class="citation" data-cites="derrington1984-chromaticmechanisms">Derrington et al. (<a href="references.html#ref-derrington1984-chromaticmechanisms" role="doc-biblioref">1984</a>)</span> measured the responses of lateral geniculate neurons using contrast stimulus presented on a moderate, neutral background. They estimated the input to these neurons from the different cone classes and confirmed the basic observations made by DeValois and his colleagues.</p>
<p>Derrington et al.&nbsp;reported that parvocellular neurons could be classified into two groups of neurons. One population of neurons receives opposing input from the <span class="math inline">\(L\)</span> and <span class="math inline">\(M\)</span> cones. The panel on the left of <a href="#fig-lg-opponent-signals" class="quarto-xref">Figure&nbsp;<span>9.16</span></a> shows my estimate of the spectral sensitivity of this group of parvocellular neurons. For these neurons wavelengths near 570nm are quite ineffective. But, there is a great deal of variation within this cell population making it difficult to be confident in the connection. Moreover, these neurons do not show a second zero-crossing near 470nm that would parallel the human opponent-colors judgments in the hue cancellation experiment.</p>
<p>A second population of lateral geniculate neurons receives input from the <span class="math inline">\(S\)</span> cones and an opposing signal from a combination of the <span class="math inline">\(L\)</span> and <span class="math inline">\(M\)</span> cones. For these neurons, wavelengths near 500nm are quite ineffective. The panel on the right of <a href="#fig-lg-opponent-signals" class="quarto-xref">Figure&nbsp;<span>9.16</span></a> shows my estimate of the spectral sensitivity of this group of parvocellular neurons.</p>
<div id="fig-lg-opponent-signals" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lg-opponent-signals-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./wp-content/uploads/2012/02/dkl.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lg-opponent-signals-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.16: Opponent-signals measured in lateral geniculate nucleus neurons. These spectral response curves are inferred from the measured responses of lateral geniculate neurons to many different colored stimuli presented on a monitor. The vast majority of lateral geniculate neurons in the parvocellular layers can be divided into two groups based on their response to modulations colored lights. One group of neurons receives an opponent contribution from the L and M cones alone (panel a). The second group of neurons receives a signal of like sign from the L and M cones, and an opposing signal from the S cones (panel b) (Source: <span class="citation" data-cites="derrington1984-chromaticmechanisms">Derrington et al. (<a href="references.html#ref-derrington1984-chromaticmechanisms" role="doc-biblioref">1984</a>)</span>).
</figcaption>
</figure>
</div>
<p>There was less order in the opponent-color signals of the magnocellular neurons. Many magnocellular units seemed to be driven by a difference between the <span class="math inline">\(L\)</span> and <span class="math inline">\(M\)</span> cones. A few parvocellular units and a few magnocellular units were driven by a positive sum of the two signals from these two cone types.</p>
<p>The spectral responses of these neural populations suggest that there is only a loose connection between the signals coded by these neurons and the perceptual coding into opponent-hues; it is unlikely that the excitation and inhibition causes our perception of red-green and blue-yellow. One difficulty is the imperfect correspondence between the neural responses and the hue cancellation measurements. The second difficulty is that there is no substantial population of neurons representing a white-black signal. This is a very important perceptual dimension which must be carried in the lateral geniculate nucleus signals. Yet, no clearly identified group of neurons can be assigned this role<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>.</p>
</section>
<section id="decorrelation-of-the-cone-absorptions" class="level3">
<h3 class="anchored" data-anchor-id="decorrelation-of-the-cone-absorptions">Decorrelation of the Cone Absorptions</h3>
<p>opponent-signals measured in the lateral geniculate nucleus probably represent a code used by the visual pathways because of its properties in communicating information from the retina to the brain. The psychological opponent-colors coding may be a consequence of the coding strategy used to communicate information from the retina to the cortex. What reason might there be for using an opponent-signals coding?</p>
<p>One reason to use an opponent-signal representation has to do with the efficiency of the visual encoding. Because of the overlap of the <span class="math inline">\(L\)</span> and <span class="math inline">\(M\)</span> cone spectral sensitivities, the absorption rates of these two cone types are highly correlated. This correlation represents an inefficiency in the visual coding of spectral information. As I described in <a href="chapter-8-multiresolution-image-representations.html" class="quarto-xref"><span>Chapter 8</span></a>, decorrelating the signals can improve the efficiency of the neural representation.</p>
<p>We can illustrate this principle by working an example, parallel to the one in <a href="chapter-8-multiresolution-image-representations.html" class="quarto-xref"><span>Chapter 8</span></a>. Consider the cone absorptions to a set of surfaces. Because of the overlap in spectral sensitivities, the cone absorptions between, say, the <span class="math inline">\(L\)</span> and <span class="math inline">\(M\)</span> cones will be correlated. To remove the correlation, we create a new representation of the signals consisting of the <span class="math inline">\(L\)</span> cone absorptions alone, and a weighted combination of the the <span class="math inline">\(L\)</span>, <span class="math inline">\(M\)</span>, and <span class="math inline">\(S\)</span> cone absorptions. We will choose the weighted combination of signals so that the new signal is independent of the <span class="math inline">\(L\)</span> cone absorptions. As we reviewed in the earlier chapter, by decorrelating the cone absorptions before they travel to the brain, we make effective use of the dynamic range of the neurons transmitting the information (<span class="citation" data-cites="buchsbaum-gottschalk1984">Buchsbaum and Gottschalk (<a href="references.html#ref-buchsbaum-gottschalk1984" role="doc-biblioref">1984</a>)</span>).</p>
<p>The graphs in <a href="#fig-cone-correlogram" class="quarto-xref">Figure&nbsp;<span>9.17</span></a> (a,b) show examples of the correlation of the cone absorptions for a particular set of surfaces and illuminant. These plots represent the cone absorptions from light reflected by a Macbeth ColorChecker viewed under mean daylight illumination. The correlations shown in these two plots are typical of natural images: the <span class="math inline">\(L\)</span> and <span class="math inline">\(M\)</span> cone absorptions are highly correlated (panel a); the <span class="math inline">\(M\)</span> and <span class="math inline">\(S\)</span> cone absorptions are also correlated (panel b).</p>
<div id="fig-cone-correlogram" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cone-correlogram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./wp-content/uploads/2012/02/decor.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cone-correlogram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.17: Absorptions in the three cone classes are highly correlated. The correlation between cone absorptions can be measured using correlograms. In this figure, correlograms are shown of the cone absorptions from the surfaces in the Macbeth ColorChecker illuminated by average daylight. (a) A correlogram of the L and M cone absorptions. (b) A correlogram of the L cone absorptions plotted versus a weighted sum of the cone absorptions that is decorrelated from the L cone absorptions, -.59L + 0.8M- .12S.
</figcaption>
</figure>
</div>
<p>As described in <a href="chapter-8-multiresolution-image-representations.html" class="quarto-xref"><span>Chapter 8</span></a>, we decorrelate the signals derived from the cone absorptions by forming new signals that are weighted combinations of the cone absorptions. There are many linear transforms of the cone absorptions that could serve to decorrelate these absorptions. One such transformation is represented by the following three linear equations[^decor-svd],</p>
<p>% This is the matrix u in the decor sensor analysis in decorrelate.m</p>
<p><span id="eq-decor-opponent-signals"><span class="math display">\[
\begin{aligned}
O_1(\lambda) &amp; = &amp; 1.0L(\lambda) + 0.0M(\lambda) + 0.0S(\lambda) \\
O_2(\lambda) &amp; = &amp; -0.59L(\lambda) + 0.80M(\lambda) + -0.12S(\lambda) \\
O_3(\lambda) &amp; = &amp; -0.34L(\lambda) + -0.11M(\lambda) + 0.93S(\lambda)
\end{aligned}
\tag{9.16}\]</span></span></p>
<p>or, written in matrix form,</p>
<p><span id="eq-decor-opponent-matrix"><span class="math display">\[
\left (
    \begin{array}{ccc}
        &amp; O_1(\lambda) &amp; \\
        &amp; O_2(\lambda) &amp; \\
        &amp; O_3(\lambda) &amp; \\
    \end{array}
\right )
=
\left (
    \begin{array}{rrr}
        1.00 &amp; 0.00 &amp; 0.00 \\
        -0.59 &amp; 0.80 &amp; -0.12 \\
        -0.34 &amp; -0.11 &amp; 0.93 \\
    \end{array}
\right )
\left (
    \begin{array}{ccc}
        &amp; L(\lambda) &amp; \\
        &amp; M(\lambda) &amp; \\
        &amp; S(\lambda) &amp; \\
    \end{array}
\right )
\tag{9.17}\]</span></span></p>
<p>The new signals, <span class="math inline">\(O_i(\lambda)\)</span>, are related to the cone absorptions by a linear transformation. These three signals are decorrelated with respect to this particular collection of surfaces and illuminant.</p>
<p><a name="id1335888271"></a></p>
<p><span class="math display">\[
\begin{aligned}
O_1(\lambda) &amp; = &amp; 1.0L(\lambda) + 0.0M(\lambda) + 0.0S(\lambda) \\
O_2(\lambda) &amp; = &amp; -0.59L(\lambda) + 0.80M(\lambda) + -0.12S(\lambda) \\
O_3(\lambda) &amp; = &amp; -0.34L(\lambda) + -0.11M(\lambda) + 0.93S(\lambda)
\end{aligned}
\]</span> {#eq-decor-opponent-signals}</p>
<p>or, written in matrix form,</p>
<p><span id="eq-decor-opponent-matrix"><span class="math display">\[
\left (
    \begin{array}{c}
        O_1(\lambda) \\
        O_2(\lambda) \\
        O_3(\lambda) \\
    \end{array}
\right )
=
\left (
    \begin{array}{rrr}
        1.00 &amp; 0.00 &amp; 0.00 \\
        -0.59 &amp; 0.80 &amp; -0.12 \\
        -0.34 &amp; -0.11 &amp; 0.93 \\
    \end{array}
\right )
\left (
    \begin{array}{c}
        L(\lambda) \\
        M(\lambda) \\
        S(\lambda) \\
    \end{array}
\right )
\tag{9.18}\]</span></span></p>
<p>The new signals, <span class="math inline">\(O_i(\lambda)\)</span>, are related to the cone absorptions by a linear transformation. These three signals are decorrelated with respect to this particular collection of surfaces and illuminant.</p>
<p>The spectral sensitivity of the three decorrelated signals are shown in <a href="#fig-decor-sensors" class="quarto-xref">Figure&nbsp;<span>9.18</span></a>. The two opponent spectral sensitivities are reminiscent of the hue cancellation measurements and the opponent-signals measured in the lateral geniculate nucleus. One of the sensors has two zero-crossings, near 570nm and 470nm. A second sensor has one zero-crossing near 490nm. The third sensor has no zero-crossings, as required for a white-black pathway. The similarity between the decorrelated signals, the opponent-signals in the lateral geniculate nucleus, and the hue cancellation experiment suggest a purpose for opponent-colors organization. Opponent-colors may exist to decorrelate the cone absorptions and provide an efficient neural representation of color (<span class="citation" data-cites="buchsbaum-gottschalk1984">Buchsbaum and Gottschalk (<a href="references.html#ref-buchsbaum-gottschalk1984" role="doc-biblioref">1984</a>)</span>; <span class="citation" data-cites="derrico1991-compmodel">Derrico and Buchsbaum (<a href="references.html#ref-derrico1991-compmodel" role="doc-biblioref">1991</a>)</span>).</p>
<div id="fig-decor-sensors" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-decor-sensors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./wp-content/uploads/2012/02/decorSensors.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-decor-sensors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.18: The spectral responsivity of a set of color sensors whose responses to the Macbeth ColorChecker under mean daylight are decorrelated. The spectral sensitivities of these sensors resemble the spectral sensitivities of lateral geniculate neurons and the color appearance judgments measured in the hue cancellation experiment.
</figcaption>
</figure>
</div>
<p>The opponent-colors representation is a universal property of human color appearance, just as the need for efficient coding is a simple and universal idea. We should expect to find a precise connection between opponent-colors appearance and neural organization in the central visual pathways. The hue cancellation experiment provides us with a behavioral method of quantifying opponent-colors organization. Hue cancellation measurements establish a standard for neurophysiologists to use when evaluating opponent-signals in the visual pathways as candidates for the opponent-colors representation. Opponent-colors organization is a simple and important idea; pursuing its neural basis will lead us to new ideas about the encoding of visual information.</p>
</section>
<section id="spatial-pattern-and-color" class="level3">
<h3 class="anchored" data-anchor-id="spatial-pattern-and-color">Spatial Pattern and Color</h3>
<p>Color Plate 2 (Albers) and <a href="#fig-contrast-illusion" class="quarto-xref">Figure&nbsp;<span>9.1</span></a> show that the color appearance at a location depends on the local image contrast, that is, the relationship between the local cone absorptions and the mean image absorptions. The targets we used to demonstrate this dependence are very simple spatial patterns, squares or lines, with no internal spatial structure of their own. In this section, we will review how color appearance can also depend on the spatial structure, such as the texture or spatial frequency, of the target itself.</p>
<p>Color Plate 6 shows two squarewaves composed of alternating blue and yellow bars. One squarewave is at a higher spatial frequency than the other. The average signal reflected from the regions containing the squarewaves is the same, that is, these are pure contrast modulations about a common mean field. If you examine the squarewaves from a close distance, you will see that bars in the squarewave patterns are drawn with the same ink. If you place this book a few meters away from you, say across the room, the color of the bars in the high spatial frequency pattern will appear different from the color of the bars in the low spatial frequency pattern. The bars in the high spatial frequency pattern will appear to be light and dark modulations about the green average. The bars in the low spatial frequency pattern will continue to look a distinct blue and yellow<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./wp-content/uploads/2012/02/squarewave.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption><strong>Color Plate 6.</strong> Color appearance covaries with spatial pattern. The bars printed in these two squarewaves are the same. Yet, whether the bars appear the same or not depends on their spatial frequency which you can control by altering the viewing distance. Also, you can influence the color appearance in the two patterns by moving the book rapidly up and down while you look at the patterns. (Source: <span class="citation" data-cites="poirson1993-colorpatterns">Poirson and Wandell (<a href="references.html#ref-poirson1993-colorpatterns" role="doc-biblioref">1993</a>)</span>).</figcaption>
</figure>
</div>
<p><span class="citation" data-cites="poirson1993-colorpatterns">Poirson and Wandell (<a href="references.html#ref-poirson1993-colorpatterns" role="doc-biblioref">1993</a>)</span> used an asymmetric color-matching task to study how color appearance changes with spatial frequency of the squarewave pattern. Subjects viewed squarewave patterns whose bars were colored modulations about a neutral gray background; that is, the average of the two bars comprising the pattern was equal to the mean background level. Subjects adjusted the appearance of a 2 degree square patch to have the same color appearance as each of the bars in the pattern.</p>
<p>Two qualitative observations stood out in this study. First, spatial patterns of moderate and high spatial frequency patterns (above 8 cpd) appear mainly light-dark, with little saturation. Thus, no matter what the relative cone absorptions of a high spatial frequency target, the target appeared to be a light dark variation about the mean level. Second, the spatially asymmetric color appearance matches are not photopigment matches. This can be deduced from the first observation: Because of axial chromatic aberration, moderate frequency squarewave contrast patterns (4 and 8 cpd) cannot stimulate the <span class="math inline">\(S\)</span> cones significantly. Yet, subjects match the bars in these high frequency patterns using a 2 deg patch with considerable <span class="math inline">\(S\)</span> cone contrast. The asymmetric color-matches are established at neural sites central to the photoreceptors.</p>
<p>Poirson and I explained the asymmetric spatial color matches using a pattern-color separable model. In this model, we supposed that the color appearance of the target was determined by the response of three color mechanisms, and that the response of each mechanisms was separable with respect to pattern and color. We derived the spatial and spectral responsivities of these pathways from the observers color-matches; the estimated sensitivities are shown in <a href="#fig-pattern-color-sensitivity" class="quarto-xref">Figure&nbsp;<span>9.19</span></a>.</p>
<p>Interestingly, the three color pathways that we derived from the asymmetric matching experiment correspond quite well to the opponent-color mechanisms derived from the hue cancellation experiment. One pathway is sensitive mainly to light-dark variation; this pathway has the best spatial resolution. The other two pathways are sensitive to red-green and blue-yellow variation. The blue-yellow pathway has the worst spatial resolution. <span class="citation" data-cites="granger1973">Granger and Heurtley (<a href="references.html#ref-granger1973" role="doc-biblioref">1973</a>)</span>, <span class="citation" data-cites="mullen1985">Mullen (<a href="references.html#ref-mullen1985" role="doc-biblioref">1985</a>)</span>, <span class="citation" data-cites="sekiguchi1993a">Sekiguchi et al. (<a href="references.html#ref-sekiguchi1993a" role="doc-biblioref">1993a</a>)</span>, and <span class="citation" data-cites="sekiguchi1993b">Sekiguchi et al. (<a href="references.html#ref-sekiguchi1993b" role="doc-biblioref">1993b</a>)</span> made measurements that presupposed the existence of opponent-color pathways and estimated similar pattern sensitivities of the three mechanisms. Notice that the derivation of the opponent-colors representation in this experiment did not involve asking the observers any questions about the hue or saturation of the targets. The observers simply set color appearance matches; the opponent-colors mechanisms were needed to predict the color matches.</p>
<div id="fig-pattern-color-sensitivity" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pattern-color-sensitivity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./wp-content/uploads/2012/02/colorCsf.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pattern-color-sensitivity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.19: Estimates of the pattern-color separable sensitivity of pathways mediating color appearance. By measuring spatially asymmetric color-matches, it is possible to deduce the pattern and color sensitivity of three visual mechanisms that mediate color appearance judgments. The pattern and wavelength sensitivity of a light-dark, red-green, and blue-yellow mechanism derived from experimental measurements are shown here. (Source: <span class="citation" data-cites="poirson1993-colorpatterns">Poirson and Wandell (<a href="references.html#ref-poirson1993-colorpatterns" role="doc-biblioref">1993</a>)</span>).
</figcaption>
</figure>
</div>
<p>One of the more striking aspects of opponent-colors representations is that the apparent spatial sharpness, or focus, of a color image depends mainly on the sharpness of the light-dark component the image; apparent sharpness depends very little on the spatial structure of the opponent-color image components. This is illustrated in the three images shown in Color Plate 7. These images were created by converting the original image, represented as three spatial patterns of cone absorptions, into three new images corresponding to a light-dark representation and two opponent-colors representations. The image in Color Plate 7 (a) shows the result of spatially blurring the light-dark component and then reconstructing the image; the result appears defocussed. The images in Color Plate 7 (b,c) show the result of applying the same spatial blurring to the red-green and blue-yellow opponent-colors representations and then reconstructing. These images look spatially focused, though their color appearance has been changed somewhat.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./wp-content/uploads/2012/02/compress.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption><strong>Color Plate 7.</strong> The apparent spatial sharpness (focus) of a color image depends mainly on the light-dark component of the image, not the opponent-colors components. A colored image was converted to a light-dark, red-green and blue-yellow representation. To create the three images, the light-dark (a), red-green (b), or blue-yellow (c) components were spatially blurred and then the image was reconstructed. The light-dark image looks defocused, but the same amount of blurring does not make the other two images look defocused. (Source: H. Hel-Or, personal communication).</figcaption>
</figure>
</div>
<p>We can take advantage of the poor spatial resolution of the opponent-colors representations when we code color images for image storage and transmission. We can allocate much less information about the opponent-colors components of color images without changing the apparent spatial sharpness of the image. This property of human perception was important in shaping broadcast television standards and digital image compression algorithms. As a quantitative prediction, we should expect to find that neurons in the central visual pathways that represent light-dark information should be able to represent spatial information at much higher resolution than neurons that code opponent-colors information. Consequently, we should expect that the largest fraction of central neurons encode light-dark, rather than the other two opponent-colors signals.</p>
<p>The differences between the light-dark encoding and the opponent-colors encoding are of great perceptual significance. Consequently, several authors have studied hypotheses based on the idea that opponent-colors signals and light-dark signals are found in separate areas of the brain. In the final section of this chapter, we will consider some of the evidence concerning the representation of color information in the visual cortex.</p>
</section>
</section>
<section id="the-cortical-basis-of-color-appearance" class="level2" data-number="9.6">
<h2 data-number="9.6" class="anchored" data-anchor-id="the-cortical-basis-of-color-appearance"><span class="header-section-number">9.6</span> The Cortical Basis of Color Appearance</h2>
<section id="clinical-studies" class="level3">
<h3 class="anchored" data-anchor-id="clinical-studies">Clinical studies</h3>
<p>In 1974 J.C. Meadows reviewed case studies of fourteen patients who had lost their ability to see colors due to a brain injury. For some patients, the colors of objects appeared wrong. Other patients saw the world entirely in shades of gray. Yet, these patients still had good visual acuity.</p>
<p>The syndrome Meadows reviewed, which I will call <em>cerebral dyschromatopsia</em>, had been described in reports spanning a century<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a> (<span class="citation" data-cites="zeki1990">Zeki (<a href="references.html#ref-zeki1990" role="doc-biblioref">1990</a>)</span>). But, the cases were rare, poor methods were used to study the patients, and the color loss was not well-dissociated from other visual deficits. Consequently, at the time Meadows wrote his review, several well-known investigators had expressed doubt about even the existence of cerebral dyschromatopsia<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a>(e.g. <span class="citation" data-cites="teuber1960">Teuber et al. (<a href="references.html#ref-teuber1960" role="doc-biblioref">1960</a>)</span>). By bringing together a number of new cases and studying them with much better methods, <span class="citation" data-cites="critchley1965">Critchley (<a href="references.html#ref-critchley1965" role="doc-biblioref">1965</a>)</span>, <span class="citation" data-cites="meadows1974">Meadows (<a href="references.html#ref-meadows1974" role="doc-biblioref">1974</a>)</span>, <span class="citation" data-cites="zeki1990">Zeki (<a href="references.html#ref-zeki1990" role="doc-biblioref">1990</a>)</span> and others (e.g. <span class="citation" data-cites="greenlessell1977-cerebraldyschromatopsia">Green and Lessell (<a href="references.html#ref-greenlessell1977-cerebraldyschromatopsia" role="doc-biblioref">1977</a>)</span>; <span class="citation" data-cites="damasio1980-centralachromatopsia">Damasio et al. (<a href="references.html#ref-damasio1980-centralachromatopsia" role="doc-biblioref">1980</a>)</span>; <span class="citation" data-cites="victor1987-centraldyschromatopsia">Victor et al. (<a href="references.html#ref-victor1987-centraldyschromatopsia" role="doc-biblioref">1987</a>)</span>; <span class="citation" data-cites="mollon1980">Mollon et al. (<a href="references.html#ref-mollon1980" role="doc-biblioref">1980</a>)</span>; <span class="citation" data-cites="heywood1987">Heywood et al. (<a href="references.html#ref-heywood1987" role="doc-biblioref">1987</a>)</span>; <span class="citation" data-cites="heywood1992">Heywood et al. (<a href="references.html#ref-heywood1992" role="doc-biblioref">1992</a>)</span>) have removed any doubt about the existence and significance of the syndrome.</p>
</section>
<section id="congenital-monochromacy" class="level3">
<h3 class="anchored" data-anchor-id="congenital-monochromacy">Congenital Monochromacy</h3>
<p>Usually, observers are dichromats or monochromats because they are missing one of the cone photopigments (see e.g. <span class="citation" data-cites="alpern1974-worldwithoutcolor">Alpern (<a href="references.html#ref-alpern1974-worldwithoutcolor" role="doc-biblioref">1974</a>)</span>; <span class="citation" data-cites="smith1972-spectralsensitivitycolorblind">Smith and Pokorny (<a href="references.html#ref-smith1972-spectralsensitivitycolorblind" role="doc-biblioref">1972</a>)</span>). There are also reports of congenital cone monochromacy of central origin. In a thorough and fascinating study, R. A. <span class="citation" data-cites="weale1953">Weale (<a href="references.html#ref-weale1953" role="doc-biblioref">1953</a>)</span> searched England for individuals who (a) could not tell color photographs from black and white, (b) were not photophobic, and (c) had good visual acuity. (Requirements (b) and (c) eliminated rod monochromats). Weale found three cone monochromats, that is individuals who could adjust the intensity of a single primary light to match the appearance of any other test light. Yet, based on direct measurements of the photopigment in the eye of one of the observers, as well as behavioral measurements, some of these cone monochromats were shown to have more than one cone photopigment (<span class="citation" data-cites="weale1959">Weale (<a href="references.html#ref-weale1959" role="doc-biblioref">1959</a>)</span>; <span class="citation" data-cites="gibson1962">Gibson (<a href="references.html#ref-gibson1962" role="doc-biblioref">1962</a>)</span>; see also <span class="citation" data-cites="alpern1974-worldwithoutcolor">Alpern (<a href="references.html#ref-alpern1974-worldwithoutcolor" role="doc-biblioref">1974</a>)</span>). Hence, Weale’s subjects had a congenital dyschromatopsia caused by deficiencies central to the photopigments. At present, we know little more about them.</p>
</section>
<section id="regularities-of-the-cerebral-dyschromatopsia-syndrome" class="level3">
<h3 class="anchored" data-anchor-id="regularities-of-the-cerebral-dyschromatopsia-syndrome">Regularities of the Cerebral Dyschromatopsia Syndrome</h3>
<p>When color loss arises from damage to the brain, the distortion of color appearance can take several forms. In some cases, patients report that colors have completely lost their saturation and hue and the world becomes gray. In other cases, color appearance may become desaturated. Some observer can perform some simple color discrimination tasks, but they report that the colors of familiar objects do not appear right. In many cases the loss is permanent, but there are also reports of transient dyschromatopsia. For example, <span class="citation" data-cites="lawden1993-achromatopsia">Lawden and Cleland (<a href="references.html#ref-lawden1993-achromatopsia" role="doc-biblioref">1993</a>)</span> recently reported on the case of a woman who suffers from migraines. During the migraine attacks, her world becomes transiently colorless.</p>
<p>The variability in the case studies suggest that there are a variety of mechanisms that may disturb color appearance. Across this variability, however, there are also some regularities. First, <span class="citation" data-cites="meadows1974">Meadows (<a href="references.html#ref-meadows1974" role="doc-biblioref">1974</a>)</span> observed that every patient with dyschromatopsia was blind in some portion of the upper visual field.</p>
<p>Second, Meadows examined the reverse correlation: do patients with purely upper visual field losses tend to have cerebral dyschromatopsia? In the literature, he found twelve patients with a purely upper visual field loss, seven had dyschromatopsia. Of sixteen patients with a purely lower visual field loss, none had dyschromatopsia. In humans, the upper visual field is represented along the lower part of the calcarine sulcus (<a href="chapter-6-the-cortical-representation.html" class="quarto-xref"><span>Chapter 6</span></a>). The correlation between field loss and dyschromatopsia suggests that the damage that leads to dyschromatopsia is either near the lower portion of the calcarine or somewhere along the path traced out by the nerve fibers whose signal enters the lower portion of the calcarine cortex.</p>
<p>Third, many of the patients suffer from a syndrome called <em>prosopagnosia</em>, the inability to recognize familiar faces. Twelve of the fourteen patients described by Meadows had this syndrome. The patient with migraines also has transient prosopagnosia (<span class="citation" data-cites="lawden1993-achromatopsia">Lawden and Cleland (<a href="references.html#ref-lawden1993-achromatopsia" role="doc-biblioref">1993</a>)</span>). The co-occurrence of dyschromatopsia and prosopagnosia suggests that the neural mechanisms necessary for recall of familiar faces and color are located close to one another or that they rely on the same visual signal.</p>
<p>Based on his review of the literature, Meadows concluded that</p>
<blockquote class="blockquote">
<p>The evidence on localization in cases of cerebral achromatopsia points to the importance of bilateral, inferiorly placed, posterior lesions of both cerebral hemispheres. (<span class="citation" data-cites="meadows1974">Meadows (<a href="references.html#ref-meadows1974" role="doc-biblioref">1974</a>)</span>, p.&nbsp;622) <a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a></p>
</blockquote>
</section>
<section id="behavioral-studies-of-patients-with-cerebral-dyschromatopsia" class="level3">
<h3 class="anchored" data-anchor-id="behavioral-studies-of-patients-with-cerebral-dyschromatopsia">Behavioral studies of patients with cerebral dyschromatopsia</h3>
<p>Patients with cerebral dyschromatopsia often fail to identify any of the test patterns on the Ishihara plates<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a>. <span class="citation" data-cites="mollon1980">Mollon et al. (<a href="references.html#ref-mollon1980" role="doc-biblioref">1980</a>)</span> reported on a patient who failed to identify the targets on the Ishihara plates (<a href="chapter-4-wavelength-encoding.html" class="quarto-xref"><span>Chapter 4</span></a>) at reading distance, but who could distinguish the targets when the plates were viewed from 2 meters. At the 2 meter viewing distance, the neutral areas separating the target and background are barely visible and the target and background appear contiguous. Twelve years after the original study, <span class="citation" data-cites="heywood1992">Heywood et al. (<a href="references.html#ref-heywood1992" role="doc-biblioref">1992</a>)</span> replicated the finding on the same patient. They also showed that the patient can discriminate contiguous colors, but not colors separated by a gray stripe. Hence, in this patient cerebral dyschromatopsia involves color and pattern together (see also <span class="citation" data-cites="victor1987-centraldyschromatopsia">Victor et al. (<a href="references.html#ref-victor1987-centraldyschromatopsia" role="doc-biblioref">1987</a>)</span>).</p>
<div id="fig-farnsworth-dyschromatopsia" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-farnsworth-dyschromatopsia-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./wp-content/uploads/2012/02/meadows.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-farnsworth-dyschromatopsia-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.20: Results of the Farnsworth test measured on a patient suffering cerebral dyschromatopsia. The patients error scores are high in all hue directions. This pattern of scores is not consistent with any of the usual pattern of errors observed by cone dichromats who are missing one of their cone photopigments (Source: <span class="citation" data-cites="meadows1974">Meadows (<a href="references.html#ref-meadows1974" role="doc-biblioref">1974</a>)</span>).
</figcaption>
</figure>
</div>
<p>Cerebral dyschromatopsics score quite poorly on the Farnsworth-Munsell hue test (see <a href="chapter-4-wavelength-encoding.html" class="quarto-xref"><span>Chapter 4</span></a>). The pattern of errors does not correspond to the errors made by any class of dichromat. The results of the test of one such patient is shown in <a href="#fig-farnsworth-dyschromatopsia" class="quarto-xref">Figure&nbsp;<span>9.20</span></a>. The errors are large in all directions though there is some hint that the errors may be somewhat larger in the blue and yellow portions of the hue circle.</p>
</section>
<section id="how-many-cone-types-are-functional" class="level3">
<h3 class="anchored" data-anchor-id="how-many-cone-types-are-functional">How many cone types are functional?</h3>
<p>The patients’ errors on the Ishihara color plates and the Farnsworth-Munsell hue test are not consistent with a visual pigment loss. Nonetheless, we cannot tell from their performance on these tests whether the separate cone classes are functioning or whether the loss of color perception is due, in part, to cone dysfunction.</p>
<p>Gibson (<span class="citation" data-cites="gibson1962">Gibson (<a href="references.html#ref-gibson1962" role="doc-biblioref">1962</a>)</span>; <span class="citation" data-cites="alpern1974-worldwithoutcolor">Alpern (<a href="references.html#ref-alpern1974-worldwithoutcolor" role="doc-biblioref">1974</a>)</span>; <span class="citation" data-cites="mollon1980">Mollon et al. (<a href="references.html#ref-mollon1980" role="doc-biblioref">1980</a>)</span>) developed a behavioral test to infer whether the patients with cerebral dyschromatopsia had more than a single class of functioning cones. The logic of their behavioral test is based on the fact the cone signals are scaled to correct for changes in the ambient lighting conditions. For example, in the presence of a long-wavelength background, the sensitivity of the <span class="math inline">\(L\)</span> cones is suppressed while the sensitivity of the <span class="math inline">\(S\)</span> cones remains unchanged.</p>
<p>Now, suppose a subject has only a single type of cone. For this observer wavelength sensitivity is determined by the spectral sensitivity a single cone photopigment. Changes of the background illumination will not change the observer’s relative wavelength sensitivity. This is the situation for normal observers under scotopic viewing conditions when we see only through the rods. Under scotopic conditions wavelength sensitivity is determined by the rhodopsin photopigment; changing the background does not change in the relative sensitivity to different test wavelengths<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a>.</p>
<p>If an individual has two functional cone classes, however, changes in the sensitivity of one cone class relative to the other will change the behavioral wavelength sensitivity. Hence, we can detect the presence of two cone classes by measuring wavelength sensitivity on two different backgrounds and noting a change in the observer’s relative wavelength sensitivity.</p>
<p><span class="citation" data-cites="mollon1980">Mollon et al. (<a href="references.html#ref-mollon1980" role="doc-biblioref">1980</a>)</span> measured a cerebral dyschromatopsic’s relative wavelength sensitivity to test wavelengths (510nm and 640nm) on two different backgrounds (510nm and 650nm). I have replotted their data in <a href="#fig-mollon-dyschromatopsia" class="quarto-xref">Figure&nbsp;<span>9.21</span></a>. When the background changes, the relative test wavelength sensitivity changes showing that the subject has at least two functional cone classes, like Weale’s and Alpern’s congenital monochromats.</p>
<div id="fig-mollon-dyschromatopsia" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mollon-dyschromatopsia-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./wp-content/uploads/2012/02/mollon.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mollon-dyschromatopsia-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.21: Experimental demonstration that a patient with cerebral dyschromatopsia has more than a single functioning cone class. (a) The patient’s threshold sensitivity was measured to two monochromatic test lights on two different backgrounds. The change in background illumination changed the patient’s relative wavelength sensitivity. (b) The results of performing the same experiment on a normal observer are shown. The results from the normal observer and the patient are quite similar (Source: <span class="citation" data-cites="mollon1980">Mollon et al. (<a href="references.html#ref-mollon1980" role="doc-biblioref">1980</a>)</span>).
</figcaption>
</figure>
</div>
<p>Clinical studies of cerebral dyschromatopsia shows that central lesions can disturb color vision severely, while sparing many other aspects of visual performance. This clinical syndrome suggests that some of the neural mechanisms essential to the sensation of color appearance may be anatomically separate from the mechanisms required for other visual tasks, such as acuity, motion and depth perception. But, clinical lesions are not neat and orderly, and the syndrome of cerebral dyschromatopsia is quite varied. Alternative hypotheses, for example that neurons carrying color information are more susceptible to stroke damage than other neurons, are also consistent with the clinical observations (<span class="citation" data-cites="mollon1980">Mollon et al. (<a href="references.html#ref-mollon1980" role="doc-biblioref">1980</a>)</span>). To pursue the question of the neural representation of color information, we need to consider other forms of evidence concerning the localization of color appearance.</p>
</section>
<section id="physiological-studies-of-color-appearance" class="level3">
<h3 class="anchored" data-anchor-id="physiological-studies-of-color-appearance">Physiological studies of color appearance</h3>
<p>Much of the agenda for modern research on the cortical representation of color appearance has been set by Zeki via a hypothesis he calls <em>functional segregation</em> (<span class="citation" data-cites="zeki1974">Zeki (<a href="references.html#ref-zeki1974" role="doc-biblioref">1974</a>)</span>, <span class="citation" data-cites="zeki1993">Zeki (<a href="references.html#ref-zeki1993" role="doc-biblioref">1993</a>)</span>; <a href="chapter-6-the-cortical-representation.html" class="quarto-xref"><span>Chapter 6</span></a>.</p>
<p>Zeki argues that there is a direct correlation between the neural responses in cortical areas beyond V1 and various perceptual features, such as color, motion and form. This is not the only hypothesis we might entertain for the relationship between brain structures and perceptual function. An alternative view has been expressed by Livingstone and Hubel (<span class="citation" data-cites="livingstonehubel1984a-anatomy">Livingstone and Hubel (<a href="references.html#ref-livingstonehubel1984a-anatomy" role="doc-biblioref">1984a</a>)</span>, <span class="citation" data-cites="livingstonehubel1984b-specificity">Livingstone and Hubel (<a href="references.html#ref-livingstonehubel1984b-specificity" role="doc-biblioref">1984b</a>)</span>; <span class="citation" data-cites="hubel-livingstone1987-singleneurons">Hubel and Livingstone (<a href="references.html#ref-hubel-livingstone1987-singleneurons" role="doc-biblioref">1987</a>)</span>) who argued that perceptual function can be localized to groups of neurons residing within single visual areas. Specifically, they have argued that differences in the density of the enzyme cytochrome oxidase within cell bodies serves as a clue to the localization of perceptual processing (see <a href="chapter-6-the-cortical-representation.html" class="quarto-xref"><span>Chapter 6</span></a>). This criterion for identifying neural segregation of function seems relevant in areas V1 and V2 since the the anatomical interconnections between these areas appear to respect the differences in cytochrome oxidase density (<span class="citation" data-cites="burkhalter1989">Burkhalter (<a href="references.html#ref-burkhalter1989" role="doc-biblioref">1989</a>)</span>).</p>
<p>Livingstone and Hubel’s hypothesis need not conflict with Zeki’s since information may be interwined within peripheral visual areas only to be segregated later. But, the presence of subdivisions within areas V1 and V2 raise the question of whether more detailed study might not reveal functional subdivisions within areas V4 and MT as well (see e.g. <span class="citation" data-cites="born-tootell1992">Born and Tootell (<a href="references.html#ref-born-tootell1992" role="doc-biblioref">1992</a>)</span>).</p>
<p>The principle line of evidence used to support Zeki’s hypothesis of functional segregation is Barlow’s neuron doctrine: namely, that the receptive field of a neuron corresponds to the perceptual experience the animal will have when the neuron is excited (<a href="chapter-6-the-cortical-representation.html" class="quarto-xref"><span>Chapter 6</span></a>). Based on this doctrine, neurophysiologists frequently assume that neurons with spatially oriented receptive fields are responsible for the perception of form; neurons that are inhibited by some wavelengths and excited by others are responsible for opponent-color percepts; neurons with motion selective receptive fields are responsible for motion perception.</p>
<p>Zeki’s suggestion that monkey area V4 is a color center and area MT is a motion center is based on differences in the receptive field properties of neurons in these two areas. The overwhelming majority of neurons in area MT show motion direction selectivity. Zeki reported that many neurons in area V4 reported an unusual wavelength selectivity (<span class="citation" data-cites="zeki1974">Zeki (<a href="references.html#ref-zeki1974" role="doc-biblioref">1974</a>)</span>, <span class="citation" data-cites="zeki1980">Zeki (<a href="references.html#ref-zeki1980" role="doc-biblioref">1980</a>)</span>, <span class="citation" data-cites="zeki1990">Zeki (<a href="references.html#ref-zeki1990" role="doc-biblioref">1990</a>)</span>).</p>
<p>As we have already seen, qualitative observations concerning receptive neural wavelength selectivity is not a firm basis to establish these neurons as being devoted mainly to color. For example, the vast majority of neurons in the lateral geniculate nucleus respond with opponent-signals, and these neurons have no orientation selectivity. Yet, we know that these neurons surely represent color, form and motion information.</p>
<p>Moreover, the quality of the receptive field measurements in area V4 has not achieved the same level of precision as measurements in the retina or area V1. Because these cells appear to be highly nonlinear, there are no widely agreed upon methods for fully characterizing their responses. And, there have been disputes concerning even the qualitative properties of area V4 receptive fields. For example, <span class="citation" data-cites="desimone1987-v4">Desimone and Schein (<a href="references.html#ref-desimone1987-v4" role="doc-biblioref">1987</a>)</span> report that many cells are selective to orientation, direction of motion, and spatial frequency. Like Zeki, these authors too accept the basic logic of the neuron doctrine. They conclude from the variation of receptive field properties that “V4 is not specialized to analyze one particular attribute of a stimulus; rather, V4 appears to process both spatial and spectral information in parallel.” They then develop an alternative notion of the role of area V4 and later visual areas.</p>
<!-- \\nocite{Desimone1985} -->
</section>
<section id="reasoning-about-cortex-and-perception" class="level3">
<h3 class="anchored" data-anchor-id="reasoning-about-cortex-and-perception">Reasoning about Cortex and Perception</h3>
<p>While hypotheses about the role of different cortical areas in perception are being debated, and experiments have begun, we are at quite an early stage in our understanding of cortical function. This should not be too surprising, after all the scientific investigation of the relationship between cortical responses and perception is a relatively new scientific endeavor, perhaps less than 100 years old. At this point in time we should expect some controversy and uncertainty regarding the status of early hypotheses. Much of the controversy stems from is due to our field’s inexperience in judging which experimental measurements will prove to be a reliable source of information and which will not.</p>
<p>In thinking about what we have learned about cortical function, I find it helpful to consider these two questions:</p>
<ul>
<li>What do we want to know about cortical function?</li>
<li>What are the logical underpinnings of the experimental methods we have available to determine the relationship between cortical responses and perception?</li>
</ul>
<p>When one discovers a new structure in the brain, it is almost impossible to refrain from asking: what does this part of the brain do? Once one poses this question, the answer is naturally formulated in terms of the <em>localization</em> of perceptual function. Our mindset becomes one of asking what happens here, rather than asking what happens. Hypotheses concerning the localization of function are the usual way to begin a research program on brain function. Moreover, I think any fair reading of the historical literature will show that hypotheses about what functions are localized within the brain region serve the useful purpose of organizing early experiments and theory.</p>
<p>On the other hand, in those portions of the visual pathways where our understanding is relatively mature, localization is rarely the central issue. We know that the retina is involved in visual function, and we know that some features of the retinal encoding are important for acuity, adaptation, wavelength encoding, and so forth. Our grasp of retinal function is sufficiently powerful so that we no longer frame questions about retinal function as a problem of localization. Instead, we pose problems in terms of the flow of information; we try to understand how information is represented and transformed within the retina.</p>
<p>For example, we know that information about the stimulus wavelength is represented by the relative absorption rates of the three cone photopigments. The information is not localized in any simple anatomical sense: no single neuron contains all the necessary information, nor are the neurons that represent wavelength information grouped together. Perhaps, one might argue that acuity is localized since acuity is greatest in the fovea. Even so, acuity depends on image formation, proper spacing of the photoreceptors, and appropriate representation of the photoreceptors on the optic tract. Without all of these others components in place, the observer will not have good visual acuity. The important questions about visual acuity are questions about the nature of the information and how the information is encoded and transmitted. That the fovea is the region of highest acuity is important, but not a solution to the question of how we resolve fine detail.</p>
<p>The most important questions about vision are those that Helmholtz posed: What are the principles that govern how the visual pathways make inferences from the visual image? How do we use image information to compute these perceptual inferences? We seek to understand these principles of behavior and neural representations with the same precision as we understand color-matching and the cone photopigments. We begin with spatial localization of brain function so that we can decide where to begin our work, not how to end it.</p>
<p>Thus, as our understanding becomes more refined we no longer formulate hypotheses based on localization of function alone. Instead, we use quantitative methods to compare neural responses and and behavioral measurements. Mature areas of vision science relate perception and neural response by demonstrating correlations between the information in the neural signals and the computations applied to those signals. The information contained in the neural response, and the transformations applied to that information, is the essence of perception.</p>
</section>
</section>
<section id="summary-and-conclusions" class="level2" data-number="9.7">
<h2 data-number="9.7" class="anchored" data-anchor-id="summary-and-conclusions"><span class="header-section-number">9.7</span> Summary and Conclusions</h2>
<p>Color appearance, like so much of vision, is an inference. Mainly, color is a perceptual representation of the surface reflectance of that object. There are two powerful obstacles that make it difficult to infer surface reflectance from the light incident at the eye. First, the reflected light confounds information about the surface and illuminant. Second, the human eye has only three types of cones to encode a spectral signal consisting of many different wavelengths.</p>
<p>We began this chapter by asking what aspects of color imaging might make it feasible to perform this visual inference. Specifically, we studied how surface reflectance might be estimated from the light incident at the eye. We concluded that it is possible to draw accurate inferences about surface reflectance functions when the surface and illuminant spectral curves are regular functions that can be well-approximated by low dimensional linear models. When the input signals are constrained, it is possible to design simple algorithms that use the cone absorptions to estimate accurately surface reflectance.</p>
<p>Next, we considered whether human judgments of color appearance share some of the properties used by algorithms that estimate surface reflectance. As a test of the correspondence between these abstract algorithms and human behavior, we reviewed how judgments of color appearance vary with changes in the illumination. Experimental results using the asymmetric color-matching method show that color appearance judgments of targets seen under different illuminants can be predicted by matches between scaled responses of the human cones. The scale factor depends on the difference in illumination. To a large degree, these results are consistent with the general principle we have observed many times: judgments of color appearance are described mainly by the local contrast of the cone signals, not their absolute level. By basing color appearance judgments on the scaled signal, which approximates the local cone contrast, color appearance correlates more closely with surface reflectance than with the light incident at the eye.</p>
<p>Then, we turned to a more general review of the organizational principles of color appearance. There are two important means of organizing color experience. Many color representations, like the Munsell representation, emphasize the properties of hue, saturation and lightness. A second organizational theme is based on Hering’s observation that red-green and blue-yellow are opponent-colors pairs, and that we never experience these hues together in a single color. The opponent-colors organization has drawn considerable attention with the discovery that many neurons carry opponent-signals, increasing their response to some wavelengths of light and decreasing in response to others.</p>
<p>In recent years, there have been many creative and interesting attempts to study the representation of color information in visual cortex. Most prominent amongst the hypotheses generated by this work is the notion that opponent-colors signals are spatially localized in the cortex. The evidence in support of this view comes from two types of experiments. First, clinical observations show that certain individuals lose their ability to perceive color although they still retina high visual acuity. Second, studies of the receptive fields of individual neurons suggest that opponent-colors signals are represented in spatially localized brain areas. These hypotheses are new and unproven. But, whether they are ultimately right or wrong, these hypotheses are the important opening steps in the modern scientific quest to understand the neural basis of conscious experience.</p>



<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-alpern1974-worldwithoutcolor" class="csl-entry" role="listitem">
Alpern M (1974) What is it that confines in a world without color? Invest Ophthalmol 13:648–674
</div>
<div id="ref-ayama1989" class="csl-entry" role="listitem">
Ayama M, Ikeda M (1989) Dependence of the chromatic valence function on chromatic standards. Vision Res 29:1233–1244
</div>
<div id="ref-berlinkay1969-basiccolorterms" class="csl-entry" role="listitem">
Berlin B, Kay P (1969) Basic color terms: <span>Their</span> universality and evolution
</div>
<div id="ref-born-tootell1992" class="csl-entry" role="listitem">
Born R, Tootell R (1992) Segregation of global and local motion processing in primate middle temporal visual area. Nature 365:279–279
</div>
<div id="ref-boynton1987-basiccolors" class="csl-entry" role="listitem">
Boynton RM, Olson CX (1987) Locating basic colors in the <span>OSA</span> space. Color Res Appl 12:94–105
</div>
<div id="ref-boynton1964-huewavelength" class="csl-entry" role="listitem">
Boynton RM, Schafer W, Neun ME (1964) Hue-wavelength relation measured by color-naming method for three retinal locations. Science 146:666–668
</div>
<div id="ref-brainard1994-bayesian" class="csl-entry" role="listitem">
Brainard DH, Freeman WT (1994) Bayesian method for recovering surface and illuminant properties from photosensor responses. In: Rogowitz BE, Allebach JP (eds) Human vision, visual processing, and digital display v. SPIE
</div>
<div id="ref-brainard1986-analysisretinextheory" class="csl-entry" role="listitem">
Brainard DH, Wandell BA (1986) <a href="https://www.ncbi.nlm.nih.gov/pubmed/3772627">Analysis of the retinex theory of color vision</a>. Journal of the Optical Society of America A, Optics and Image Science 3:1651–1661
</div>
<div id="ref-brainard1991-bilinearmodel" class="csl-entry" role="listitem">
Brainard DH, Wandell BA (1991) <a href="https://psycnet.apa.org/record/1991-98916-013">A bilinear model of the illuminant’s effect on color appearance</a>. In: Landy MS (ed) Computational models of visual processing (pp. MIT Press, xii, pp 171–186
</div>
<div id="ref-brainard1992-asymmetriccolormatching" class="csl-entry" role="listitem">
Brainard DH, Wandell BA (1992) Asymmetric color-matching: <span>How</span> color appearance depends on the illuminant. Journal of the Optical Society of America A 9:1433–1448
</div>
<div id="ref-brewer1954" class="csl-entry" role="listitem">
Brewer WL, Lyle Brewer W (1954) Fundamental response functions and binocular color matching. 44:207
</div>
<div id="ref-buchsbaum1980-gray" class="csl-entry" role="listitem">
Buchsbaum G (1980) A spatial processor model for object colour perception. Journal of The Franklin Institute-engineering and Applied Mathematics 310:1–26
</div>
<div id="ref-buchsbaum-gottschalk1984" class="csl-entry" role="listitem">
Buchsbaum G, Gottschalk A (1984) Chromaticity coordinates of frequency-limited functions. J Opt Soc Am A 1:885–887
</div>
<div id="ref-burkhalter1989" class="csl-entry" role="listitem">
Burkhalter A (1989) Intrinsic connections of rat primary visual cortex: Laminar organization of axonal projections. J Comp Neurol 279:171–186
</div>
<div id="ref-burnham1957-predictioncolorappearance" class="csl-entry" role="listitem">
Burnham RW, Evans RM, Newhall SM (1957) Prediction of color appearance with different adaptation illuminations. Journal of the Optical Society of America 47:35–42
</div>
<div id="ref-burns1984-abneyeffect" class="csl-entry" role="listitem">
Burns S, Elsner A, Pokorny J, Smith V (1984) The abney effect: Chromaticity coordinates of unique and other constant hues. Vision Res 24:479–489
</div>
<div id="ref-chichilnisky1995" class="csl-entry" role="listitem">
Chichilnisky EJ (1995) Perceptual measurements of neural computations in color appearance
</div>
<div id="ref-cohen1964" class="csl-entry" role="listitem">
Cohen J (1964) Dependency of the spectral reflectance curves of the <span>Munsell</span> color chips. Psychonomic Science 1:369–370
</div>
<div id="ref-critchley1965" class="csl-entry" role="listitem">
Critchley M (1965) Neurology’s debt to <span>F</span>. J. Gall (1758-1828). Br Med J 2:775–781
</div>
<div id="ref-dzmura1993-basic" class="csl-entry" role="listitem">
D’Zmura M, Iverson G (1993a) Color constancy. <span>I Basic</span> theory of two-stage linear recovery of spectral descriptions for lights and surfaces. Journal of the Optical Society of America A, Optics, Image Science, and Vision 10:2148–2165
</div>
<div id="ref-dzmura1993-results" class="csl-entry" role="listitem">
D’Zmura M, Iverson G (1993b) Color constancy <span>II Results</span> for two-stage linear recovery of spectral descriptions for lights and surfaces. 10:2166
</div>
<div id="ref-dzmura1994-general" class="csl-entry" role="listitem">
D’Zmura M, Iverson G (1994) Color constancy <span>III General</span> linear recovery of spectral descriptions for lights and surfaces. 11:2389
</div>
<div id="ref-dzmura1986-sharedrodcone" class="csl-entry" role="listitem">
D’Zmura M, Lennie P (1986) Shared pathways for rod and cone vision. Vision Res 26:1273–1280
</div>
<div id="ref-damasio1980-centralachromatopsia" class="csl-entry" role="listitem">
Damasio A, Yamada T, Damasio H, et al (1980) Central achromatopsia: Behavioral, anatomic, and physiologic aspects. Neurology 30:1064–1071
</div>
<div id="ref-devalois1965" class="csl-entry" role="listitem">
De Valois RL (1965) Analysis and coding of color vision in the primate visual system. Cold Spring Harb Symp Quant Biol 30:567–579
</div>
<div id="ref-devalois1966-lgn" class="csl-entry" role="listitem">
De Valois RL, Abramov I, Jacobs GH (1966) Analysis of response patterns of <span>LGN</span> cells. J Opt Soc Am 56:966–977
</div>
<div id="ref-devalois1958" class="csl-entry" role="listitem">
De Valois RL, Smith CJ, Kitai ST, Karoly AJ (1958) Response of single cells in monkey lateral geniculate nucleus to monochromatic light. Science 127:238–239
</div>
<div id="ref-derrico1991-compmodel" class="csl-entry" role="listitem">
Derrico JB, Buchsbaum G (1991) A computational model of spatiochromatic image coding in early vision. J Vis Commun Image Represent 2:31–38
</div>
<div id="ref-derrington1984-chromaticmechanisms" class="csl-entry" role="listitem">
Derrington AM, Krauskopf J, Lennie P (1984) Chromatic mechanisms in lateral geniculate nucleus of macaque. J Physiol 357:241–265
</div>
<div id="ref-desimone1987-v4" class="csl-entry" role="listitem">
Desimone R, Schein SJ (1987) Visual properties of neurons in area <span>V4</span> of the macaque: Sensitivity to stimulus form. J Neurophysiol 57:835–868
</div>
<div id="ref-dixon1978-daylight" class="csl-entry" role="listitem">
Dixon ER (1978) Spectral distribution of <span>Australian</span> daylight. Journal of The Optical Society of America 68:437
</div>
<div id="ref-drew1992-interreflection" class="csl-entry" role="listitem">
Drew MS, Funt B (1992) Variational approach to interreflection in color images. Journal of The Optical Society of America A-optics Image Science and Vision 9:1255–1265
</div>
<div id="ref-farrell1992" class="csl-entry" role="listitem">
Farrell TJ, Patterson MS, Wilson B (1992) A diffusion theory model of spatially resolved, steady-state diffuse reflectance for the noninvasive determination of tissue optical properties in vivo. Medical Physics 19:879–888
</div>
<div id="ref-foster1994" class="csl-entry" role="listitem">
Foster DH, Nascimento SM (1994) Relational colour constancy from invariant cone-excitation ratios. Proc Biol Sci 257:115–121
</div>
<div id="ref-gibson1962" class="csl-entry" role="listitem">
Gibson IM (1962) <span>VISUAL</span> <span>MECHANISMS</span> <span>IN</span> <span>CONE</span>-<span>MONOCHROMAT</span>. In: JOURNAL OF PHYSIOLOGY. p 10P
</div>
<div id="ref-gouras1968" class="csl-entry" role="listitem">
Gouras P (1968) Identification of cone mechanisms in monkey ganglion cells. J Physiol 199:533–547
</div>
<div id="ref-granger1973" class="csl-entry" role="listitem">
Granger EM, Heurtley JC (1973) Letters to the editor: Visual chromaticity-modulation transfer function. J Opt Soc Am 63:1173–1174
</div>
<div id="ref-greenlessell1977-cerebraldyschromatopsia" class="csl-entry" role="listitem">
Green GJ, Lessell S (1977) Acquired cerebral dyschromatopsia. Arch Ophthalmol 95:121–128
</div>
<div id="ref-helson1938" class="csl-entry" role="listitem">
Helson H (1938) Fundamental problems in color vision. <span>I</span>. The principle governing changes in hue, saturation, and lightness of non-selective samples in chromatic illumination. J Exp Psychol 23:439–476
</div>
<div id="ref-hering1964-outlinestheorylight" class="csl-entry" role="listitem">
Hering E (1964) Outlines of a theory of the light sense. Harvard University Press, Cambridge, MA, US
</div>
<div id="ref-heywood1987" class="csl-entry" role="listitem">
Heywood CA, Wilson B, Cowey A (1987) A case study of cortical colour <span>“blindness”</span> with relatively intact achromatic discrimination. J Neurol Neurosurg Psychiatry 50:22–29
</div>
<div id="ref-heywood1992" class="csl-entry" role="listitem">
Heywood C, Gadotti A, Cowey A (1992) Cortical area <span>V4</span> and its role in the perception of color. J Neurosci 12:4056–4065
</div>
<div id="ref-hubel-livingstone1987-singleneurons" class="csl-entry" role="listitem">
Hubel DH, Livingstone MS (1987) Segregation of form, color, and stereopsis in primate area 18. J Neurosci 7:3378–3415
</div>
<div id="ref-hurvichjameson1957" class="csl-entry" role="listitem">
Hurvich LM, Jameson D (1957) An opponent-process theory of color vision. Psychol Rev 64, Part 1:384–404
</div>
<div id="ref-jamesonhurvich1955" class="csl-entry" role="listitem">
Jameson D, Hurvich L (1955) Some quantitative aspects of an opponent-colors theory. <span>I</span>. Chromatic responses and spectral saturation. Journal of the Optical Society of America 45:546–552
</div>
<div id="ref-judd1951a-visualstimulus" class="csl-entry" role="listitem">
Judd DB (1951) Basic correlates of the visual stimulus. In: Stevens SS (ed) Handbook of experimental psychology. Wiley, pp 811–867
</div>
<div id="ref-judd1960-land" class="csl-entry" role="listitem">
Judd DB (1960) Appraisal of <span>Land</span>’s work on two-primary color projections. Journal of The Optical Society of America 50:254–268
</div>
<div id="ref-judd1940" class="csl-entry" role="listitem">
Judd DB (1940) Hue saturation and lightness of surface colors with chromatic illumination. J Opt Soc Am, JOSA 30:2–32
</div>
<div id="ref-judd1964-daylight" class="csl-entry" role="listitem">
Judd DB, MacAdam DL, Wyszecki G, et al (1964) Spectral distribution of typical daylight as a function of correlated color temperature. Journal of The Optical Society of America 54:1031
</div>
<div id="ref-klinker1988-highlights" class="csl-entry" role="listitem">
Klinker GJ, Shafer SA, Kanade T (1988) The measurement of highlights in color images. International Journal of Computer Vision
</div>
<div id="ref-land1977-retinex" class="csl-entry" role="listitem">
Land E (1977) The retinex theory of color vision. Sci Am 237:108–128
</div>
<div id="ref-land1986-recentretinex" class="csl-entry" role="listitem">
Land EH (1986a) Recent advances in <span>Retinex</span> theory. Vision Research 26:7–21
</div>
<div id="ref-land1986-retinexalt" class="csl-entry" role="listitem">
Land EH (1986b) An alternative technique for the computation of the designator in the retinex theory of color vision. Proceedings of the National Academy of Sciences of the United States of America 83:3078–3080
</div>
<div id="ref-land1959-pnas" class="csl-entry" role="listitem">
Land EH (1959) <a href="http://dx.doi.org/10.1073/pnas.45.1.115">Color vision and the natural image. Part i</a>. Proc Natl Acad Sci U S A 45:115–129
</div>
<div id="ref-larimer1975-opponentprocessadditivity" class="csl-entry" role="listitem">
Larimer J, Krantz D, Cicerone CM (1975) Opponent process additivity—II. Yellow/blue equilibria and nonlinear models. Vision Res 15:723–731
</div>
<div id="ref-lawden1993-achromatopsia" class="csl-entry" role="listitem">
Lawden MC, Cleland PG (1993) Achromatopsia in the aura of migraine. J Neurol Neurosurg Psychiatry 56:708–709
</div>
<div id="ref-lee1986-highlights" class="csl-entry" role="listitem">
Lee H-C (1986) Method for computing the scene-illuminant chromaticity from specular highlights. J Opt Soc Am A 3:1694–1699
</div>
<div id="ref-lennie1990-cortex" class="csl-entry" role="listitem">
Lennie P, Krauskopf J, Sclar G (1990) Chromatic mechanisms in striate cortex of macaque. J Neurosci 10:649–669
</div>
<div id="ref-livingstonehubel1984a-anatomy" class="csl-entry" role="listitem">
Livingstone M, Hubel D (1984a) Anatomy and physiology of a color system in the primate visual cortex. J Neurosci 4:309–356
</div>
<div id="ref-livingstonehubel1984b-specificity" class="csl-entry" role="listitem">
Livingstone M, Hubel D (1984b) Specificity of intrinsic connections in primate primary visual cortex. J Neurosci 4:2830–2835
</div>
<div id="ref-maloney1986-linearmodels-surface" class="csl-entry" role="listitem">
Maloney LT (1986) Evaluation of linear models of surface spectral reflectance with small numbers of parameters. Journal of The Optical Society of America A 3:1673–1683
</div>
<div id="ref-marimont1992-linearmodelssurface" class="csl-entry" role="listitem">
Marimont DH, Wandell BA (1992) <a href="https://www.ncbi.nlm.nih.gov/pubmed/1432341">Linear models of surface and illuminant spectra</a>. Journal of the Optical Society of America A, Optics and Image Science 9:1905–1913
</div>
<div id="ref-mccamy1976-chart" class="csl-entry" role="listitem">
McCamy CS, Marcus H, Davidson JG (1976) A color-rendition chart. J App Photog Eng 2:95–99
</div>
<div id="ref-mccann1976-retinex" class="csl-entry" role="listitem">
McCann JJ, McKee SP, Taylor TH (1976) Quantitative studies in retinex theroy. A comparison between theoretical predictions and observer responses to the <span>“color mondrian”</span> experiments. Vision Res 16:445–458
</div>
<div id="ref-meadows1974" class="csl-entry" role="listitem">
Meadows JC (1974) Disturbed perception of colours associated with localized cerebral lesions. Brain : a journal of neurology 97:615–632
</div>
<div id="ref-mollon1977-shortwave" class="csl-entry" role="listitem">
Mollon JD, Polden PG (1977) An anomaly in the response of the eye to light of short wavelengths. Philos Trans R Soc Lond B Biol Sci 278:207–240
</div>
<div id="ref-mollon1980" class="csl-entry" role="listitem">
Mollon J, Newcombe F, Polden P, Ratcliff G (1980) On the presence of three cone mechanisms in a case of total achromatopsia. In: Verriest G (ed) Colour vision deficiencies v: Chapter 3. Hilger, Bristol
</div>
<div id="ref-mullen1985" class="csl-entry" role="listitem">
Mullen KT (1985) <a href="http://dx.doi.org/10.1113/jphysiol.1985.sp015591">The contrast sensitivity of human colour vision to red-green and blue-yellow chromatic gratings</a>. J Physiol 359:381–400
</div>
<div id="ref-nayar1993-reflectance" class="csl-entry" role="listitem">
Nayar SK, Bolle RM (1993) Computing reflectance ratios from an image. Pattern Recognit 26:1529–1542
</div>
<div id="ref-parkkinen1989-spectra" class="csl-entry" role="listitem">
Parkkinen JPS, Hallikainen J, Jaaskelainen T (1989) Characteristic spectra of munsell colors. J Opt Soc Am A, JOSAA 6:318–322
</div>
<div id="ref-poirson1993-colorpatterns" class="csl-entry" role="listitem">
Poirson A, Wandell BA (1993) <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;citation_for_view=w2KlBM4AAAAJ:LkGwnXOMwfcC">Appearance of colored patterns: Pattern-color separability</a>. J Opt Soc Am A-optics Image Sci Vis 10:2458–2470
</div>
<div id="ref-pugh1976" class="csl-entry" role="listitem">
Pugh EN Jr (1976) The nature of the <span class="nocase">pi1</span> colour mechanism of <span>W</span>.<span>S</span>. stiles. J Physiol 257:713–747
</div>
<div id="ref-pugh-mollon1979" class="csl-entry" role="listitem">
Pugh EN Jr, Mollon JD (1979) A theory of the <span class="nocase">pi1</span> and <span class="nocase">pi3</span> color mechanisms of stiles. Vision Res 19:293–312
</div>
<div id="ref-sastri1966-spectral" class="csl-entry" role="listitem">
Sastri VDP, Das SR (1966) Spectral distribution and color of north sky at <span>Delhi</span>. Journal of The Optical Society of America 56:829
</div>
<div id="ref-sekiguchi1993a" class="csl-entry" role="listitem">
Sekiguchi N, Williams DR, Brainard DH (1993a) Aberration-free measurements of the visibility of isoluminant gratings. J Opt Soc Am A Opt Image Sci Vis 10:2105–2117
</div>
<div id="ref-sekiguchi1993b" class="csl-entry" role="listitem">
Sekiguchi N, Williams DR, Brainard DH (1993b) Efficiency in detection of isoluminant and isochromatic interference changes. J Opt Soc Am (A) 10:2118–2133
</div>
<div id="ref-shafer1985" class="csl-entry" role="listitem">
Shafer SA (1985) Using color to separate reflection components. Color Res Appl 10:210–218
</div>
<div id="ref-smith1972-spectralsensitivitycolorblind" class="csl-entry" role="listitem">
Smith VC, Pokorny J (1972) Spectral sensitivity of color-blind observers and the cone photopigments. Vision Research 12:2059–2071
</div>
<div id="ref-sternheim1979-flicker" class="csl-entry" role="listitem">
Sternheim CE, Stromeyer CF, K. KMC (1979) Visibility of chromatic flicker upon spectrally mixed adapting fields. Vision Research 19:175–183
</div>
<div id="ref-stiles1959-colorvision" class="csl-entry" role="listitem">
Stiles WS (1959) Color vision: The approach through increment-threshold sensitivity. Proc Natl Acad Sci U S A 45:100–114
</div>
<div id="ref-stiles1978-mechanismscolorvision" class="csl-entry" role="listitem">
Stiles WS (1978) Mechanisms of colour vision. Academic Press, London, England
</div>
<div id="ref-stiles1939-directionalsensitivityretina" class="csl-entry" role="listitem">
Stiles WS (1939) The directional sensitivity of the retina and the spectral sensitivity of the rods and cones. Proceedings of the Royal Society of London Series B 127:64–105
</div>
<div id="ref-stromeyer1985-secondsite" class="csl-entry" role="listitem">
Stromeyer CF 3rd, Cole GR, Kronauer RE (1985) Second-site adaptation in the red-green chromatic pathways. Vision Res 25:219–237
</div>
<div id="ref-svaetichin1956" class="csl-entry" role="listitem">
Svaetichin G (1956) Spectral response curves from single cones. Acta Physiol Scand Suppl 39:17–46
</div>
<div id="ref-svaetichin1958" class="csl-entry" role="listitem">
Svaetichin G, Macnichol EF (1958) Retinal mechanisms for chromatic and achromatic vision. Ann N Y Acad Sci 74:385–404
</div>
<div id="ref-teuber1960" class="csl-entry" role="listitem">
Teuber HL, Battersby WS, Bender MB (1960) Functioning versus plotted fields. In: Visual field defects after penetrating missile wounds of the brain. Harvard University Press, Cambridge, MA; London, England, pp 83–112
</div>
<div id="ref-tominaga1989-model" class="csl-entry" role="listitem">
Tominaga S, Wandell B (1989) Standard surface-reflectance model and illuminant estimation. Journal of The Optical Society of America A-optics Image Science and Vision 6:576–584
</div>
<div id="ref-tominaga1990-surfacespectralreflectance" class="csl-entry" role="listitem">
Tominaga S, Wandell B (1990) Component estimation of surface spectral reflectance. Journal of The Optical Society of America A-optics Image Science and Vision 7:312–317
</div>
<div id="ref-victor1987-centraldyschromatopsia" class="csl-entry" role="listitem">
Victor JD, Maiese K, Shapley R, et al (1987) Acquired central dyschromatopsia: Analysis of a case with preservation of color discrimination. Clinical Vision Sciences 4:183–196
</div>
<div id="ref-vonkries1905" class="csl-entry" role="listitem">
von Kries J (1905) Influence of adaptation on the effects produced by luminous stimuli. handbuch der Physiologie des Menschen 3:109–282
</div>
<div id="ref-vrhel-trussell1994" class="csl-entry" role="listitem">
Vrhel MJ, Trussell HJ (1994) Filter considerations in color correction. IEEE Transactions on Image Processing 3:147–161. <a href="https://doi.org/10.1109/83.277897">https://doi.org/10.1109/83.277897</a>
</div>
<div id="ref-walls1960-landland" class="csl-entry" role="listitem">
Walls GL (1960) “<span>Land</span>! <span>Land</span>! Psychological Bulletin 57:29–48
</div>
<div id="ref-wandell1987-synthesisanalysis" class="csl-entry" role="listitem">
Wandell BA (1987) <a href="https://www.ncbi.nlm.nih.gov/pubmed/21869373">The synthesis and analysis of color images</a>. IEEE Transactions on Pattern Analysis and Machine Intelligence 9:2–13
</div>
<div id="ref-wandellpugh1980a" class="csl-entry" role="listitem">
Wandell BA, Pugh E (1980a) A field-additive pathway detects brief-duration, long-wavelength incremental flashes. Vision Res 20:613–624
</div>
<div id="ref-wandellpugh1980b" class="csl-entry" role="listitem">
Wandell BA, Pugh E (1980b) Detection of long-duration, long-wavelength incremental flashes by a chromatically coded pathway. Vision Res 20:625–636
</div>
<div id="ref-wassef1958" class="csl-entry" role="listitem">
Wassef EGT (1958) Investigation into the theory of prediction of the appearance of colours and its bearing on the theory of colour vision. Opt Acta (Lond) 5:101–108
</div>
<div id="ref-wassef1952-binocularmatching" class="csl-entry" role="listitem">
Wassef EGT (1952) Application of the binocular matching method to the study of the subjective appearance of surface colours. Optica Acta 2:144–150
</div>
<div id="ref-wassef1959" class="csl-entry" role="listitem">
Wassef EGT (1959) Linearity of the relationship between the tristimulus values of corresponding colours seen under different conditions of chromatic adaptation. Opt Acta (Lond) 6:378–386
</div>
<div id="ref-weale1953" class="csl-entry" role="listitem">
Weale RA (1953) Spectral sensitivity and wave-length discrimination of the peripheral retina. J Physiol 119:170–190
</div>
<div id="ref-weale1959" class="csl-entry" role="listitem">
Weale RA (1959) Photo-sensitive reactions in foveae of normal and cone-monochromatic observers. Opt Acta (Lond) 6:158–174
</div>
<div id="ref-wiesel-hubel1966" class="csl-entry" role="listitem">
Wiesel TN, Hubel DH (1966) Spatial and chromatic interactions in the lateral geniculate body of the rhesus monkey. J Neurophysiol 29:1115–1156
</div>
<div id="ref-wolff1994-relative" class="csl-entry" role="listitem">
Wolff LB (1994) Relative brightness of specular and diffuse reflection. Opt Eng 33:285
</div>
<div id="ref-wyszeckicolorscienceconcepts1982" class="csl-entry" role="listitem">
Wyszecki G, Stiles WS (1982) Color <span>Science</span>: <span>Concepts</span> and <span>Methods</span>, <span>Quantitative Data</span> and <span>Formulae</span>, 2nd edn. John Wiley &amp; Sons, New York
</div>
<div id="ref-zeki1974" class="csl-entry" role="listitem">
Zeki S (1974) Functional organisation of a visual area in the posterior bank of the superior temporal sulcus of the rhesus monkey. J Physiol 236:549–573
</div>
<div id="ref-zeki1990" class="csl-entry" role="listitem">
Zeki S (1990) Parallelism and functional specialization in human visual cortex. Cold Spring Harbor Symposia on Quantitative Biology 55:651–661
</div>
<div id="ref-zeki1980" class="csl-entry" role="listitem">
Zeki S (1980) The representawtion of colours in the cerebral cortex of the monkey. Nature 284:412–418
</div>
<div id="ref-zeki1993" class="csl-entry" role="listitem">
Zeki S (1993) A vision of the brain: <span>The</span> visible world and the cortex. Blackwell Science, Philadelphia, PA
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Of course, even on his off days, Land was worth a billion dollars.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>This example was used by <span class="citation" data-cites="hering1964-outlinestheorylight">Hering (<a href="references.html#ref-hering1964-outlinestheorylight" role="doc-biblioref">1964</a>)</span>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Also, some types of materials fluoresce, which is to say they absorb light at one wavelength and emit light at another (longer) wavelength. This is also a linear process, but too complex to consider in this discussion.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Dielectrics are non-conducting materials.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>The basis functions that minimize the squared error can be found in several ways, most of which are explained in widely available statistical packages. If the data are in the columns of a matrix, one can apply the singular value decomposition to the data matrix and use the left singular vectors. Equivalently, one can find the eigenvectors of the covariance matrix of the data.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>It is possible to improve on this model slightly, but as a practical matter these three curves do quite well as basis functions.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>I calculated the singular value decomposition of the matrix whose columns consist of the surface reflectance vectors. I used the left singular vectors as the basis functions.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>More sophisticated methods are based on using Bayesian estimation as part of the calculation. For example, see <span class="citation" data-cites="brainard1994-bayesian">Brainard and Freeman (<a href="references.html#ref-brainard1994-bayesian" role="doc-biblioref">1994</a>)</span> and <span class="citation" data-cites="dzmura1993-basic">D’Zmura and Iverson (<a href="references.html#ref-dzmura1993-basic" role="doc-biblioref">1993a</a>)</span>; <span class="citation" data-cites="dzmura1993-results">D’Zmura and Iverson (<a href="references.html#ref-dzmura1993-results" role="doc-biblioref">1993b</a>)</span>; <span class="citation" data-cites="dzmura1994-general">D’Zmura and Iverson (<a href="references.html#ref-dzmura1994-general" role="doc-biblioref">1994</a>)</span>.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>This binocular method makes sense if one accepts the view that the adjustment for the illumination is mediated primarily before the signals from the two eyes are combined, in the superficial layers of area V1. The coherence of the experimental method can be tested psychophysically by examining transitivity. The observer matches a test on backgrounds L and <span class="math inline">\(R_1\)</span>, and then on backgrounds L and <span class="math inline">\(R_2\)</span>. The experimenter then places <span class="math inline">\(R_1\)</span> in the left eye and <span class="math inline">\(R_2\)</span> in the right eye and verifies that the matching lights match one another. There is no guarantee, of course, that these measurements are governed by precisely the same visual mechanisms that govern adaptation under normal viewing conditions.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Interestingly, one of the largest sets of data she reviewed was a series of experiments performed at the Kodak research laboratories, Polaroid’s competitor. (<span class="citation" data-cites="wassef1952-binocularmatching">Wassef (<a href="references.html#ref-wassef1952-binocularmatching" role="doc-biblioref">1952</a>)</span>, <span class="citation" data-cites="wassef1958">Wassef (<a href="references.html#ref-wassef1958" role="doc-biblioref">1958</a>)</span>, <span class="citation" data-cites="wassef1959">Wassef (<a href="references.html#ref-wassef1959" role="doc-biblioref">1959</a>)</span>)<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>Indeed, this is equally a failure of the simple linearity that Wassef uses to summarize the data, and more in line with some of the conclusions that <span class="citation" data-cites="burnham1957-predictioncolorappearance">Burnham et al. (<a href="references.html#ref-burnham1957-predictioncolorappearance" role="doc-biblioref">1957</a>)</span> drew about their data.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>White, black, red, green, yellow, blue, brown, purple, pink, orange, gray.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>At first, it was thought that these responses reflected the activity of the cones. Subsequent investigations showed that the responses were from horizontal cells (<span class="citation" data-cites="svaetichin1958">Svaetichin and Macnichol (<a href="references.html#ref-svaetichin1958" role="doc-biblioref">1958</a>)</span>).<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>Some authors have suggested that a single group of lateral geniculate neurons codes a white-black sensation for high spatial frequency patterns and a red-green sensation for low spatial frequency patterns. While this is an interesting hypothesis, notice that the authors have abandoned the idea that there is a specific color sensation associated with the response of lateral geniculate neurons. Instead, they suppose that the perceived hue depends on the pattern of neural activation (Ingling and Martinez, 1984; <span class="citation" data-cites="derrington1984-chromaticmechanisms">Derrington et al. (<a href="references.html#ref-derrington1984-chromaticmechanisms" role="doc-biblioref">1984</a>)</span>).<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>You can also alter the relative color appearance of the patterns by moving the book rapidly up and down. You will see that the low frequency squarewave retains its appearance while the high frequency squarewave becomes a green blur.<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>Some of the terms used to describe color loss vary between authors. The terms trichromacy, dichromacy and monochromacy are precise, referring to the number of primary lights necessary to complete the color matching experiment. Some authors use the phrase <em>cerebral achromatopsia</em>, meaning “without color vision”, to describe a loss of color vision while others use cerebral dyschromatopsia. I prefer the second term because in these cases insensitivity to hue is often not complete and because these patients still distinguish the colors white and black. When the behavioral evidence warrants it, one might append a modifier, such as <em>monochromatic</em> dyschromatopsia, to describe the the color loss more precisely.<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>In his book and in a long article, Zeki argued that the skepticism concerning cerebral dyschromatopsia was caused by their acceptance of a profoundly misguided theory concerning the significance of visual area V1. I agree with Meadows’ gentler assessment; the early evidence in support of cerebral dyschromatopsia is spotty and poorly argued. There was room for some skepticism.<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p>As S. Zeki points out, Meadows’ conclusion echoes a disputed suggestion made a century earlier. While studying a patient who reported a loss of color vision, the French physician, Verrey concluded, &gt; Le centre du sense chromatique se trouverait dans la partie la plus inferieure du lobe occipital, probablement dans la partie posterieure des plis lingual et fusiforme. (Verry, 1888, cited in <span class="citation" data-cites="zeki1990">Zeki (<a href="references.html#ref-zeki1990" role="doc-biblioref">1990</a>)</span>, p.&nbsp;1722) <em>Translation: The center of the chromatic sense will be found in the inferior part of the occipital lobe, probably in the posterior part of the lingual and fusiform gyrus</em>.<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p>But, <span class="citation" data-cites="meadows1974">Meadows (<a href="references.html#ref-meadows1974" role="doc-biblioref">1974</a>)</span> and <span class="citation" data-cites="victor1987-centraldyschromatopsia">Victor et al. (<a href="references.html#ref-victor1987-centraldyschromatopsia" role="doc-biblioref">1987</a>)</span> describe patients who could read all of the plates.<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p>In a beautiful series of experiments, W.S. Stiles (<span class="citation" data-cites="stiles1939-directionalsensitivityretina">Stiles (<a href="references.html#ref-stiles1939-directionalsensitivityretina" role="doc-biblioref">1939</a>)</span>; <span class="citation" data-cites="stiles1959-colorvision">Stiles (<a href="references.html#ref-stiles1959-colorvision" role="doc-biblioref">1959</a>)</span>; <span class="citation" data-cites="stiles1978-mechanismscolorvision">Stiles (<a href="references.html#ref-stiles1978-mechanismscolorvision" role="doc-biblioref">1978</a>)</span>) studied how sensitivity varies as one changes the wavelength and intensity of a test and background lights. He developed a penetrating analysis of this experimental paradigm and identified candidate processes which he believed might describe photoreceptor adaptation. He referred to these processes as <span class="math inline">\(\pi\)</span>-mechanisms, “p” for process and <span class="math inline">\(\pi\)</span> for p.<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./part-3-image-interpretation.html" class="pagination-link" aria-label="Introduction to Image Interepretation">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Introduction to Image Interepretation</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter-10-motion-and-depth.html" class="pagination-link" aria-label="Motion and Depth">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Motion and Depth</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>